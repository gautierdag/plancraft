{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download outputs from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "project = \"plancraft\"\n",
    "entity = \"itl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(f\"{entity}/{project}\")\n",
    "\n",
    "# download all \n",
    "for run in tqdm(runs):\n",
    "    if \"test_small\" not in run.name:\n",
    "        continue\n",
    "    for file in run.files():\n",
    "        if (\n",
    "            file.name.startswith(\"outputs/\")\n",
    "            and file.name.endswith(\".json\")\n",
    "            and \"/test.small/\" in file.name\n",
    "        ):\n",
    "            file.download(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate outputs into single results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# load from local\n",
    "task_results = glob.glob(\"outputs/*/test.small/*/*.json\")\n",
    "results = []\n",
    "\n",
    "for task_result in task_results:\n",
    "    example_id = task_result.split(\"/\")[-1]\n",
    "    seed = task_result.split(\"/\")[-2]\n",
    "    split = task_result.split(\"/\")[-3]\n",
    "    run_name = task_result.split(\"/\")[-4]\n",
    "    try:\n",
    "        with open(task_result) as f:\n",
    "            result = json.load(f)\n",
    "    except:\n",
    "        print(f\"Failed to load {task_result}\")\n",
    "        # remove the file if it failed to load\n",
    "        os.remove(task_result)\n",
    "        continue\n",
    "    result[\"name\"] = run_name\n",
    "    result[\"split\"] = split\n",
    "    result[\"seed\"] = seed\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test.small.json\", \"r\") as f:\n",
    "    test_small = json.load(f)\n",
    "    test_id_set = set([x[\"id\"] for x in test_small])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id_to_complexity_bin = {v[\"id\"]: int(v[\"complexity_bin\"]) for v in test_small}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "# df.model_trace.apply(pd.Series)\n",
    "df = pd.concat([df, df.model_trace.apply(pd.Series)], axis=1).drop(\n",
    "    \"model_trace\", axis=1\n",
    ")\n",
    "# ensure only ids in the test.small are included\n",
    "df = df[df[\"example_id\"].isin(test_id_set)]\n",
    "\n",
    "df[\"name\"] = df[\"name\"].str.replace(\"Meta-Llama-3.1-8B-Instruct\", \"Llama_8B\")\n",
    "df[\"name\"] = df[\"name\"].str.replace(\"Llama-3.3-70B-Instruct\", \"Llama_70B\")\n",
    "df[\"name\"] = df[\"name\"].str.replace(\"oa-llama3-r64-a32\", \"Llama_8B_FT\")\n",
    "df[\"tools\"] = df[\"name\"].str.split(\"_\").str[-1]\n",
    "df[\"mode\"] = df[\"name\"].str.split(\"_\").str[0]\n",
    "df[\"modality\"] = df[\"name\"].str.split(\"_\").str[1]\n",
    "df[\"use_fasterrcnn\"] = df[\"name\"].str.contains(\"fasterrcnn\")\n",
    "df[\"model\"] = df[\"name\"].str.split(\"_\").str[2:-1].str.join(\"_\").str.replace(\"fasterrcnn_\", \"\")\n",
    "# if oracle in name then it is an oracle model\n",
    "df.loc[df[\"name\"].str.contains(\"oracle\"), \"model\"] = \"oracle\"\n",
    "df.loc[df[\"name\"].str.contains(\"oracle\"), \"tools\"] = \"m|s\"\n",
    "\n",
    "print(df[\"tools\"].unique())\n",
    "print(df[\"mode\"].unique())\n",
    "print(df[\"modality\"].unique())\n",
    "print(df[\"model\"].unique())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[(df.name == \"act_text_Llama_8B_m|s|t|se|i\") & (df.seed == \"1\")].tokens_used.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the full runs\n",
    "full_impossible = df.groupby([\"name\", \"seed\"]).filter(lambda x: len(x) == len(test_small))\n",
    "full_non_impossible = df.groupby([\"name\", \"seed\"]).filter(lambda x: len(x) == len([x for x in test_small if not x[\"impossible\"]]))\n",
    "df = pd.concat([full_impossible, full_non_impossible])\n",
    "print(len(df))\n",
    "\n",
    "# df[df[\"name\"] == \"act_text_gpt-4o-mini_m|s|t|se|i\"].groupby([\"seed\"]).apply(\n",
    "#     lambda x: len(x)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impossible(x):\n",
    "    # Whether or not the last element in the dialogue history is impossible\n",
    "    if len(x) == 0:\n",
    "        return False\n",
    "    last_element = x[-1][\"content\"]\n",
    "    if isinstance(last_element, list):\n",
    "        last_element = last_element[-1][\"text\"]\n",
    "    return last_element.startswith(\"impossible:\")\n",
    "\n",
    "def multimodal_dialogue(x):\n",
    "    # Multimodal dialogue wraps content response in a list [{type: \"text\", text: \"response\"}]\n",
    "    if len(x) == 0:\n",
    "        return False\n",
    "    if isinstance(x[-1][\"content\"], list):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "df[\"multimodal_dialogue\"] = df[\"dialogue_history\"].apply(multimodal_dialogue)\n",
    "df[\"impossible_emitted\"] = df[\"dialogue_history\"].apply(get_impossible)\n",
    "df[\"complexity_bin\"] = df[\"example_id\"].apply(lambda x: example_id_to_complexity_bin[x])\n",
    "\n",
    "complexity_bins_names = {\n",
    "    0: \"easy\",\n",
    "    1: \"easy\",\n",
    "    2: \"medium\",\n",
    "    3: \"hard\",\n",
    "    4: \"hard\",\n",
    "    5: \"other\",\n",
    "}\n",
    "df[\"original_complexity_bin\"] = df[\"complexity_bin\"]\n",
    "df[\"complexity_bin\"] = df[\"complexity_bin\"].map(complexity_bins_names)\n",
    "\n",
    "# Count actions\n",
    "def count_actions(dialogue):\n",
    "    actions = {\n",
    "        \"move\": 0,\n",
    "        \"smelt\": 0,\n",
    "        \"think\": 0,\n",
    "        \"impossible\": 0,\n",
    "        \"search\": 0,\n",
    "        \"invalid\": 0,\n",
    "    }\n",
    "    for message in dialogue:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            if isinstance(message[\"content\"], list):\n",
    "                for content in message[\"content\"]:\n",
    "                    if content[\"type\"] != \"text\":\n",
    "                        continue\n",
    "                    action = content[\"text\"].split(\":\")[0]\n",
    "                    if action not in actions:\n",
    "                        actions[\"invalid\"] += 1\n",
    "                    else:\n",
    "                        actions[action] += 1\n",
    "                    break\n",
    "            else:\n",
    "                action = message[\"content\"].split(\":\")[0]\n",
    "                if action not in actions:\n",
    "                    actions[\"invalid\"] += 1\n",
    "                else:\n",
    "                    actions[action] += 1\n",
    "    # rename actions to action_counts\n",
    "    out_dict = {f\"{k}_count\": v for k, v in actions.items()}\n",
    "    out_dict[\"total_actions\"] = sum(actions.values())\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "# count number of actions for oracle\n",
    "def oracle_count(action_history):\n",
    "    actions_count = {\n",
    "        \"move_count\": 0,\n",
    "        \"smelt_count\": 0,\n",
    "        \"impossible_count\": 0,\n",
    "        \"search_count\": 0,\n",
    "        \"think_count\": 0,\n",
    "    }\n",
    "    if len(action_history) == 0:\n",
    "        actions_count[\"impossible_count\"] = 1\n",
    "        actions_count[\"total_actions\"] = sum(actions_count.values())\n",
    "        return actions_count\n",
    "    for action in action_history:\n",
    "        action_type = action[\"action_type\"]\n",
    "        actions_count[f\"{action_type}_count\"] += 1\n",
    "\n",
    "    actions_count[\"total_actions\"] = sum(actions_count.values())\n",
    "\n",
    "    return actions_count\n",
    "\n",
    "\n",
    "df[\"actions_dict\"] = df[\"dialogue_history\"].apply(count_actions)\n",
    "df.loc[df[\"mode\"] == \"oracle\", \"actions_dict\"] = df[df[\"mode\"] == \"oracle\"][\n",
    "    \"action_history\"\n",
    "].apply(oracle_count)\n",
    "\n",
    "df = pd.concat([df, df.actions_dict.apply(pd.Series)], axis=1).drop(\n",
    "    \"actions_dict\", axis=1\n",
    ")\n",
    "\n",
    "# df[\"impossible_accuracy\"] = df[\"impossible_emitted\"] == df[\"impossible\"]\n",
    "\n",
    "# get dict mapping betwwen example_id and the oracle's total_actions\n",
    "oracle_actions = (\n",
    "    df[df[\"mode\"] == \"oracle\"].groupby(\"example_id\")[\"total_actions\"].first().to_dict()\n",
    ")\n",
    "df[\"oracle_length\"] = df[\"example_id\"].map(oracle_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby([\"name\"]).seed.nunique())\n",
    "drop_names = [\"oracle_text_fasterrcnn\", \n",
    "            \"act_images_fasterrcnn_gpt-4o-mini_m|s\"]\n",
    "\n",
    "df = df[~df[\"name\"].isin(drop_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table for Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"example_id\")\n",
    "# succ_df = df[df[\"success\"]]\n",
    "\n",
    "# succ_df[\"total_actions\"](succ_df[\"total_actions\"] - succ_df[\"oracle_length\"]).describe()\n",
    "# df[\"total_actions\"] = df[\"move_count\"] + df[\"smelt_count\"] + df[\"impossible_count\"] + df[\"search_count\"] + df[\"think_count\"]\n",
    "df.loc[(df[\"mode\"] != \"oracle\")&df[\"success\"], \"actions_over_oracle\"] = df.loc[(df[\"mode\"] != \"oracle\")&df[\"success\"], \"total_actions\"] - df.loc[(df[\"mode\"] != \"oracle\")&df[\"success\"], \"oracle_length\"]\n",
    "\n",
    "df.actions_over_oracle.describe()\n",
    "# df[df[\"actions_over_oracle\"] < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_success_df = (\n",
    "    df[\n",
    "        (df[\"mode\"] != \"oracle\")\n",
    "        & (df[\"complexity_bin\"] != \"other\")\n",
    "        & (df[\"mode\"] == \"act\")\n",
    "        & (df[\"tools\"] == \"m|s|t|se|i\")\n",
    "    ]\n",
    "    .groupby([\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"])\n",
    "    .success.mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "overall_success_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# overall_success_df\n",
    "\n",
    "# (df[\"impossible\"] == (df[\"complexity_bin\"] == \"other\"))\n",
    "\n",
    "tools = \"m|s|t|se|i\"\n",
    "model = \"Llama_70B\"\n",
    "\n",
    "number_of_incorrect_impossible = len(\n",
    "    df[\n",
    "        (df[\"tools\"] == tools)\n",
    "        & (df[\"impossible_count\"] > 0)\n",
    "        & (df[\"complexity_bin\"] != \"other\")\n",
    "        & (df[\"model\"] == model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "number_of_total_incorrect = len(\n",
    "    df[\n",
    "        (df[\"tools\"] == tools)\n",
    "        & (~df[\"success\"])\n",
    "        & (df[\"complexity_bin\"] != \"other\")\n",
    "        & (df[\"model\"] == model)\n",
    "    ]\n",
    ")\n",
    "number_of_total_correct = len(\n",
    "    df[\n",
    "        (df[\"tools\"] == tools)\n",
    "        & (df[\"success\"])\n",
    "        & (df[\"complexity_bin\"] != \"other\")\n",
    "        & (df[\"model\"] == model)\n",
    "    ]\n",
    ")\n",
    "print(f\"Number of incorrect impossible: {number_of_incorrect_impossible}\")\n",
    "print(f\"Number of total incorrect: {number_of_total_incorrect}\")\n",
    "print(f\"Number of total correct: {number_of_total_correct}\")\n",
    "print(f\"Accuracy: {number_of_total_correct / (number_of_total_incorrect+number_of_total_correct)}\")\n",
    "\n",
    "\n",
    "df[\n",
    "    (df[\"tools\"] == tools)\n",
    "    & (df[\"impossible_count\"] > 0)\n",
    "    & (df[\"complexity_bin\"] != \"other\")\n",
    "    & (df[\"model\"] == model)\n",
    "].dialogue_history.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f1 score for impossible\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df[\"use_fasterrcnn\"] = df[\"use_fasterrcnn\"].astype(str)\n",
    "\n",
    "df_pivot = (\n",
    "    df[df[\"mode\"] != \"oracle\"]\n",
    "    # .groupby([\"modality\", \"model\", \"mode\", \"tools\", \"recipe_type\"])\n",
    "    .groupby(\n",
    "        [\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\", \"complexity_bin\"]\n",
    "    )\n",
    "    .success.mean()\n",
    "    .unstack(level=-1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# concat with grouped by counts of actions\n",
    "df_pivot = pd.merge(\n",
    "    df_pivot,\n",
    "    df[\n",
    "        (df[\"mode\"] != \"oracle\")\n",
    "        & (df[\"complexity_bin\"] != \"other\")\n",
    "        & (df[\"mode\"] == \"act\")\n",
    "        # & (df[\"modality\"] == \"symb\")\n",
    "    ]\n",
    "    .groupby([\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"total_actions\": \"mean\",\n",
    "            \"move_count\": \"mean\",\n",
    "            \"smelt_count\": \"mean\",\n",
    "            \"think_count\": \"mean\",\n",
    "            \"impossible_count\": \"mean\",\n",
    "            \"search_count\": \"mean\",\n",
    "            \"invalid_count\": \"mean\",\n",
    "            \"actions_over_oracle\": \"mean\",\n",
    "            \"tokens_used\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index(),\n",
    "    on=[\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# add f1 score for impossible\n",
    "df_pivot = pd.merge(\n",
    "    df_pivot,\n",
    "    df[df[\"tools\"].str.contains(\"i\")]\n",
    "    .groupby([\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"])\n",
    "    .apply(\n",
    "        lambda x: f1_score(x[\"impossible\"], x[\"impossible_emitted\"]),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index(),\n",
    "    on=[\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "df_pivot.rename(columns={0: \"impossible f1\"}, inplace=True)\n",
    "\n",
    "\n",
    "# overall success\n",
    "overall_success_df = (\n",
    "    df[\n",
    "        (df[\"mode\"] != \"oracle\")\n",
    "        & (df[\"complexity_bin\"] != \"other\")\n",
    "        & (df[\"mode\"] == \"act\")\n",
    "        # & (df[\"modality\"] == \"symb\")\n",
    "    ]\n",
    "    .groupby([\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"])\n",
    "    .success.mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# # add into the pivot table\n",
    "df_pivot = pd.merge(\n",
    "    df_pivot,\n",
    "    overall_success_df,\n",
    "    on=[\"modality\", \"model\", \"mode\", \"tools\", \"use_fasterrcnn\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# rename success to overall\n",
    "df_pivot.rename(columns={\"success\": \"overall\"}, inplace=True)\n",
    "# rename models to be more human readable\n",
    "df_pivot[\"model\"] = df_pivot[\"model\"].str.replace(\"_\", \" \")\n",
    "\n",
    "# group by modality then tools then model and sort by overall\n",
    "df_pivot = df_pivot.sort_values(\n",
    "    [\"modality\", \"tools\", \"model\", \"overall\"], ascending=[True, True, False, False]\n",
    ")\n",
    "\n",
    "# Replace NaN with \"-\"\n",
    "df_pivot[\"impossible f1\"] = df_pivot[\"impossible f1\"].fillna(\"-\")\n",
    "\n",
    "# Find the maximum value in the \"overall\" column\n",
    "max_overall = df_pivot[\"overall\"].max()\n",
    "\n",
    "# Apply the bolding to the 'overall' column\n",
    "def bold_max(s):\n",
    "    return \"\\\\textbf{%s}\" % s if s == f\"{max_overall:.2f}\" else s\n",
    "\n",
    "# Apply the function to the 'overall' column\n",
    "df_pivot[\"overall\"] = df_pivot[\"overall\"].apply(lambda x: bold_max(f\"{x:.2f}\"))\n",
    "\n",
    "df_pivot = df_pivot[df_pivot[\"mode\"] == \"act\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_to_acronym = {\n",
    "    \"m|s\": \"M S\",\n",
    "    \"m|s|t\": \"M S T\",\n",
    "    \"m|s|t|se\": \"M S T SE\",\n",
    "    \"m|s|t|se|i\": \"M S T SE I\",\n",
    "}\n",
    "\n",
    "df_pivot[\"tools\"] = df_pivot[\"tools\"].map(tools_to_acronym)\n",
    "\n",
    "# title case column names\n",
    "df_pivot.columns = [x.title().replace(\"_\", \" \") for x in df_pivot.columns]\n",
    "\n",
    "# Columns to underline the max values\n",
    "columns_to_format = [\"Easy\", \"Medium\", \"Hard\"]\n",
    "# Group by 'Tools' and format max values\n",
    "for col in columns_to_format:\n",
    "    max_indices = df_pivot.groupby(\"Tools\")[col].transform(\"max\") == df_pivot[col]\n",
    "    df_pivot.loc[max_indices, col] = df_pivot.loc[max_indices, col].apply(\n",
    "        lambda x: f\"\\\\underline{{{x:.2f}}}\"\n",
    "    )\n",
    "df_pivot[\"Think Count\"] = df_pivot[\"Think Count\"].replace(0, \"-\")\n",
    "df_pivot[\"Search Count\"] = df_pivot[\"Search Count\"].replace(0, \"-\")\n",
    "df_pivot[\"Impossible Count\"] = df_pivot[\"Impossible Count\"].replace(0, \"-\")\n",
    "\n",
    "def format_thousands(x):\n",
    "    if x >= 1_000_000:\n",
    "        return f\"{x/1_000_000:.1f}M\"\n",
    "    elif x >= 1_000:\n",
    "        return f\"{x/1_000:.1f}k\"\n",
    "    else:\n",
    "        return f\"{x}\"\n",
    "\n",
    "\n",
    "df_pivot[\"Tokens Used\"] = df_pivot[\"Tokens Used\"].apply(format_thousands)\n",
    "\n",
    "# rename columns to remove \"Count\"\n",
    "df_pivot = df_pivot.rename(\n",
    "    columns={\n",
    "        \"Move Count\": \"Move\",\n",
    "        \"Smelt Count\": \"Smelt\",\n",
    "        \"Think Count\": \"Think\",\n",
    "        \"Impossible Count\": \"Impossible\",\n",
    "        \"Search Count\": \"Search\",\n",
    "        \"Invalid Count\": \"Invalid\",\n",
    "        \"Actions Over Oracle\": \"AE\",  # Action efficiency\n",
    "        \"Total Actions\": \"Avg. Plan Length\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(df_pivot[\"Model\"].unique())\n",
    "model_order = [\n",
    "    \"Llama 8B\",\n",
    "    \"Llama 70B\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"Llama 8B FT\",\n",
    "    \"oam-llama3-r64-a32\",\n",
    "    \"mrcnn gpt-4o-mini\",\n",
    "]\n",
    "\n",
    "# Convert the Model column to a categorical type with the specified order\n",
    "df_pivot[\"Model\"] = pd.Categorical(\n",
    "    df_pivot[\"Model\"], categories=model_order, ordered=True\n",
    ")\n",
    "\n",
    "# Sort by Tools (ascending) and Model (following the categorical order)\n",
    "df_pivot = df_pivot.sort_values(\n",
    "    [\"Tools\", \"Model\"],\n",
    "    ascending=[True, True],  # True for Tools, categorical handles Model order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pivot[\"Use Fasterrcnn\"] = df_pivot[\"Use Fasterrcnn\"] == \"True\"\n",
    "df_pivot[(df_pivot[\"Modality\"] == \"text\") & ~(df_pivot[\"Use Fasterrcnn\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_table = df_pivot[(df_pivot[\"Modality\"] == \"text\") & ~(df_pivot[\"Use Fasterrcnn\"])]\n",
    "print(\n",
    "    text_table[\n",
    "        [\n",
    "            # \"modality\",\n",
    "            \"Tools\",\n",
    "            \"Model\",\n",
    "            # \"mode\",\n",
    "            # \"mixed\",\n",
    "            # \"shaped\",\n",
    "            # \"shapeless\",\n",
    "            # \"smelting\",\n",
    "            # \"very easy\",\n",
    "            \"Easy\",\n",
    "            \"Medium\",\n",
    "            \"Hard\",\n",
    "            # \"very hard\",\n",
    "            \"Overall\",\n",
    "            \"Think\",\n",
    "            \"Search\",\n",
    "            \"Impossible\",\n",
    "            \"Impossible F1\",\n",
    "            \"Avg. Plan Length\",\n",
    "            \"AE\",\n",
    "            \"Tokens Used\",\n",
    "            # \"Invalid Count\",\n",
    "        ]\n",
    "    ].to_latex(index=False, float_format=\"%.2f\", escape=False)\n",
    ")\n",
    "# text_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_table = df_pivot[\n",
    "    ((df_pivot[\"Modality\"] != \"text\") | df_pivot[\"Use Fasterrcnn\"])\n",
    "    & (df_pivot[\"Tools\"] == \"M S\")\n",
    "]\n",
    "\n",
    "print(\n",
    "    real_table[\n",
    "        [\n",
    "            \"Modality\",\n",
    "            # \"Tools\",\n",
    "            \"Model\",\n",
    "            # \"mode\",\n",
    "            # \"mixed\",\n",
    "            # \"shaped\",\n",
    "            # \"shapeless\",\n",
    "            # \"smelting\",\n",
    "            # \"very easy\",\n",
    "            # \"Easy\",\n",
    "            # \"Medium\",\n",
    "            # \"Hard\",\n",
    "            # \"very hard\",\n",
    "            \"Overall\",\n",
    "            # \"Think\",\n",
    "            # \"Search\",\n",
    "            # \"Impossible\",\n",
    "            # \"Impossible F1\",\n",
    "            \"Avg. Plan Length\",\n",
    "            \"AE\",\n",
    "            \"Tokens Used\",\n",
    "            # \"Invalid Count\",\n",
    "        ]\n",
    "    ].to_latex(index=False, float_format=\"%.2f\", escape=False)\n",
    ")\n",
    "\n",
    "# df_pivot[\"Modality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table for Plan Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_steps = df[df[\"mode\"] == \"oracle\"][[\"example_id\", \"number_of_steps\"]].set_index(\"example_id\").to_dict()[\"number_of_steps\"]\n",
    "# calculate steps diff between oracle and model\n",
    "df[\"oracle_steps\"] = df[\"example_id\"].map(id_to_steps)\n",
    "df[\"steps_diff\"] = df[\"number_of_steps\"] - df[\"oracle_steps\"]\n",
    "df.loc[~df[\"success\"], \"steps_diff\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = (\n",
    "    df[df[\"mode\"] != \"oracle\"]\n",
    "    .groupby([\"modality\", \"model\", \"mode\", \"tools\", \"recipe_type\"])\n",
    "    .steps_diff.mean()\n",
    "    .unstack(level=-1)\n",
    "    .reset_index()\n",
    ")\n",
    "df_pivot[\"overall\"] = df_pivot[[\"mixed\", \"shaped\", \"shapeless\", \"smelting\"]].mean(axis=1)\n",
    "df_pivot[\"model\"] = df_pivot[\"model\"].str.replace(\"_\", \" \")\n",
    "\n",
    "df_pivot = df_pivot.sort_values(\n",
    "    [\"modality\", \"tools\", \"model\", \"overall\"], ascending=[True, True, False, False]\n",
    ")\n",
    "\n",
    "# bolden min values for overall column\n",
    "min_overall = df_pivot[\"overall\"].min()\n",
    "\n",
    "def bold_min(s):\n",
    "    return \"\\\\textbf{%s}\" % s if s == f\"{min_overall:.2f}\" else s\n",
    "\n",
    "df_pivot[\"overall\"] = df_pivot[\"overall\"].apply(\n",
    "    lambda x: bold_min(f\"{x:.2f}\")\n",
    ")\n",
    "\n",
    "# in impossible column, if 10 then replace with \"-\"\n",
    "df_pivot[\"impossible\"] = df_pivot[\"impossible\"].apply(lambda x: \"-\" if x == 10 else x)\n",
    "\n",
    "print(\n",
    "    df_pivot[\n",
    "        [\n",
    "            \"mode\",\n",
    "            \"tools\",\n",
    "            \"model\",\n",
    "            \"mixed\",\n",
    "            \"shaped\",\n",
    "            \"shapeless\",\n",
    "            \"smelting\",\n",
    "            \"overall\",\n",
    "            \"impossible\",\n",
    "        ]\n",
    "    ].to_latex(index=False, float_format=\"%.2f\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token used table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_thousands(x):\n",
    "    if x >= 1_000_000:\n",
    "        return f\"{x/1_000_000:.1f}M\"\n",
    "    elif x >= 1_000:\n",
    "        return f\"{x/1_000:.1f}k\"\n",
    "    else:\n",
    "        return f\"{x}\"\n",
    "\n",
    "df_pivot = (\n",
    "    df.groupby([\"modality\", \"model\", \"mode\", \"tools\", \"recipe_type\"])\n",
    "    .tokens_used.mean()\n",
    "    .unstack(level=-1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_pivot[\"overall\"] = df_pivot[[\"mixed\", \"shaped\", \"shapeless\", \"smelting\"]].mean(axis=1)\n",
    "\n",
    "# Apply the formatting to the DataFrame before exporting to LaTeX\n",
    "formatted_df = df_pivot[\n",
    "    [\n",
    "        \"tools\",\n",
    "        \"mode\",\n",
    "        \"model\",\n",
    "        \"mixed\",\n",
    "        \"shaped\",\n",
    "        \"shapeless\",\n",
    "        \"smelting\",\n",
    "        \"overall\",\n",
    "        \"impossible\",\n",
    "    ]\n",
    "].copy()\n",
    "for col in [\"mixed\", \"shaped\", \"shapeless\", \"smelting\", \"overall\", \"impossible\"]:\n",
    "    formatted_df[col] = formatted_df[col].apply(format_thousands)\n",
    "\n",
    "# Export to LaTeX\n",
    "print(formatted_df.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.barplot(data=df, x=\"mode\", y=\"success\", hue=\"tools\")\n",
    "# rotate x labels\n",
    "# plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
