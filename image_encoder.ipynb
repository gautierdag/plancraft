{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from plancraft.environments.env_real import RealPlancraft\n",
    "\n",
    "env = RealPlancraft(\n",
    "    inventory=[{\"type\": \"beetroot_soup\", \"quantity\": 1, \"slot\": 45}],\n",
    "    symbolic_action_space=True,\n",
    "    symbolic_observation_space=True,\n",
    "    resolution=[512, 512],\n",
    "    crop=True,\n",
    ")\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train_fast_rcnn import M1, M2, prepare_m2_batch, sample_environment\n",
    "\n",
    "# m1_path = \"m1.pth\"\n",
    "# m2_path = \"m2.pth\"\n",
    "# M1_model = M1(weights_path = m1_path)\n",
    "# M1_model = M1_model.cuda()\n",
    "M2_model = M2()\n",
    "M2_model = M2_model.cuda()\n",
    "print(\"Loaded model\")\n",
    "\n",
    "N = 1000\n",
    "lr = 0.001\n",
    "batch_size = 4\n",
    "save_every = 100\n",
    "count = 0\n",
    "# m1_optimizer = torch.optim.AdamW(M1_model.parameters(), lr=lr)\n",
    "m2_optimizer = torch.optim.AdamW(M2_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env loaded\n"
     ]
    }
   ],
   "source": [
    "for images, targets, raw_images, inv in sample_environment(N=N, batch_size=batch_size):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# train M2\n",
    "for img, inventory in zip(raw_images, inv):\n",
    "    i = Image.fromarray(img.copy())\n",
    "    m2_batch = prepare_m2_batch(i, inventory)\n",
    "    m2_batch[\"bbox_tensors\"] = torch.stack(m2_batch[\"bbox_tensors\"]).cuda()\n",
    "    m2_batch[\"item_quantity\"] = m2_batch[\"item_quantity\"].unsqueeze(1).cuda()\n",
    "    m2_batch[\"item_locations\"] = m2_batch[\"item_locations\"].cuda()\n",
    "    m2_batch[\"item_types\"] = m2_batch[\"item_types\"].cuda()\n",
    "    break\n",
    "    # M2_model.train()\n",
    "    # M2_model(m2_batch)\n",
    "    # losses = sum(loss for loss in loss_dict.values())\n",
    "    # m2_optimizer.zero_grad()\n",
    "    # losses.backward()\n",
    "    # m2_optimizer.step()\n",
    "\n",
    "# N = len(m2_batch[\"item_types\"])\n",
    "# encoded_img = M2_model.img_encoder(m2_batch[\"bbox_tensors\"]).reshape(N, -1)\n",
    "\n",
    "# M2_model.quantity_encoder(m2_batch[\"item_quantity\"].unsqueeze(1))\n",
    "\n",
    "# m2_batch[\"item_quantity\"].shape\n",
    "# \n",
    "# m2_batch[\"item_quantity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_paths = glob.glob(\"data/oracle/train/imgs/*.png\")\n",
    "# load images\n",
    "image_arrays = [Image.open(p) for p in image_paths]\n",
    "# to tensor\n",
    "image_tensors = [np.array(i)/255 for i in image_arrays]\n",
    "\n",
    "# stack\n",
    "image_tensors = np.stack(image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63818245 0.63606814 0.63203097] [0.20783292 0.21012318 0.21597961]\n"
     ]
    }
   ],
   "source": [
    "image_tensors_mean = image_tensors.mean(axis=(0, 1, 2))\n",
    "image_tensors_std = image_tensors.std(axis=(0, 1, 2))\n",
    "print(image_tensors_mean, image_tensors_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minerl.herobraine.hero.mc import ALL_ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.roi_heads import (\n",
    "    RoIHeads,\n",
    "    fastrcnn_loss,\n",
    "    maskrcnn_loss,\n",
    "    maskrcnn_inference,\n",
    "    keypointrcnn_loss,\n",
    "    keypointrcnn_inference,\n",
    ")\n",
    "\n",
    "\n",
    "class OverrideRoIHeads(RoIHeads):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.quantity_predictor = nn.Sequential(nn.Linear(1024, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features,  # type: Dict[str, Tensor]\n",
    "        proposals,  # type: List[Tensor]\n",
    "        image_shapes,  # type: List[Tuple[int, int]]\n",
    "        targets=None,  # type: Optional[List[Dict[str, Tensor]]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (List[Tensor])\n",
    "            proposals (List[Tensor[N, 4]])\n",
    "            image_shapes (List[Tuple[H, W]])\n",
    "            targets (List[Dict])\n",
    "        \"\"\"\n",
    "        if targets is not None:\n",
    "            for t in targets:\n",
    "                # TODO: https://github.com/pytorch/pytorch/issues/26731\n",
    "                floating_point_types = (torch.float, torch.double, torch.half)\n",
    "                if not t[\"boxes\"].dtype in floating_point_types:\n",
    "                    raise TypeError(\n",
    "                        f\"target boxes must of float type, instead got {t['boxes'].dtype}\"\n",
    "                    )\n",
    "                if not t[\"labels\"].dtype == torch.int64:\n",
    "                    raise TypeError(\n",
    "                        f\"target labels must of int64 type, instead got {t['labels'].dtype}\"\n",
    "                    )\n",
    "                if self.has_keypoint():\n",
    "                    if not t[\"keypoints\"].dtype == torch.float32:\n",
    "                        raise TypeError(\n",
    "                            f\"target keypoints must of float type, instead got {t['keypoints'].dtype}\"\n",
    "                        )\n",
    "\n",
    "        if self.training:\n",
    "            proposals, matched_idxs, labels, regression_targets = (\n",
    "                self.select_training_samples(proposals, targets)\n",
    "            )\n",
    "        else:\n",
    "            labels = None\n",
    "            regression_targets = None\n",
    "            matched_idxs = None\n",
    "\n",
    "        box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
    "        box_features = self.box_head(box_features)\n",
    "        class_logits, box_regression = self.box_predictor(box_features)\n",
    "        quantity_logits = self.quantity_predictor(box_features)\n",
    "\n",
    "        result: List[Dict[str, torch.Tensor]] = []\n",
    "        losses = {}\n",
    "        if self.training:\n",
    "            if labels is None:\n",
    "                raise ValueError(\"labels cannot be None\")\n",
    "            if regression_targets is None:\n",
    "                raise ValueError(\"regression_targets cannot be None\")\n",
    "            loss_classifier, loss_box_reg = fastrcnn_loss(\n",
    "                class_logits, box_regression, labels, regression_targets\n",
    "            )\n",
    "\n",
    "\n",
    "            losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "        else:\n",
    "            boxes, scores, labels = self.postprocess_detections(\n",
    "                class_logits, box_regression, proposals, image_shapes\n",
    "            )\n",
    "            num_images = len(boxes)\n",
    "            for i in range(num_images):\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"boxes\": boxes[i],\n",
    "                        \"labels\": labels[i],\n",
    "                        \"scores\": scores[i],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if self.has_mask():\n",
    "            mask_proposals = [p[\"boxes\"] for p in result]\n",
    "            if self.training:\n",
    "                if matched_idxs is None:\n",
    "                    raise ValueError(\"if in training, matched_idxs should not be None\")\n",
    "\n",
    "                # during training, only focus on positive boxes\n",
    "                num_images = len(proposals)\n",
    "                mask_proposals = []\n",
    "                pos_matched_idxs = []\n",
    "                for img_id in range(num_images):\n",
    "                    pos = torch.where(labels[img_id] > 0)[0]\n",
    "                    mask_proposals.append(proposals[img_id][pos])\n",
    "                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n",
    "            else:\n",
    "                pos_matched_idxs = None\n",
    "\n",
    "            if self.mask_roi_pool is not None:\n",
    "                mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)\n",
    "                mask_features = self.mask_head(mask_features)\n",
    "                mask_logits = self.mask_predictor(mask_features)\n",
    "            else:\n",
    "                raise Exception(\"Expected mask_roi_pool to be not None\")\n",
    "\n",
    "            loss_mask = {}\n",
    "            if self.training:\n",
    "                if targets is None or pos_matched_idxs is None or mask_logits is None:\n",
    "                    raise ValueError(\n",
    "                        \"targets, pos_matched_idxs, mask_logits cannot be None when training\"\n",
    "                    )\n",
    "\n",
    "                gt_masks = [t[\"masks\"] for t in targets]\n",
    "                gt_labels = [t[\"labels\"] for t in targets]\n",
    "                rcnn_loss_mask = maskrcnn_loss(\n",
    "                    mask_logits, mask_proposals, gt_masks, gt_labels, pos_matched_idxs\n",
    "                )\n",
    "                loss_mask = {\"loss_mask\": rcnn_loss_mask}\n",
    "            else:\n",
    "                labels = [r[\"labels\"] for r in result]\n",
    "                masks_probs = maskrcnn_inference(mask_logits, labels)\n",
    "                for mask_prob, r in zip(masks_probs, result):\n",
    "                    r[\"masks\"] = mask_prob\n",
    "\n",
    "            losses.update(loss_mask)\n",
    "\n",
    "        # keep none checks in if conditional so torchscript will conditionally\n",
    "        # compile each branch\n",
    "        if (\n",
    "            self.keypoint_roi_pool is not None\n",
    "            and self.keypoint_head is not None\n",
    "            and self.keypoint_predictor is not None\n",
    "        ):\n",
    "            keypoint_proposals = [p[\"boxes\"] for p in result]\n",
    "            if self.training:\n",
    "                # during training, only focus on positive boxes\n",
    "                num_images = len(proposals)\n",
    "                keypoint_proposals = []\n",
    "                pos_matched_idxs = []\n",
    "                if matched_idxs is None:\n",
    "                    raise ValueError(\"if in trainning, matched_idxs should not be None\")\n",
    "\n",
    "                for img_id in range(num_images):\n",
    "                    pos = torch.where(labels[img_id] > 0)[0]\n",
    "                    keypoint_proposals.append(proposals[img_id][pos])\n",
    "                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n",
    "            else:\n",
    "                pos_matched_idxs = None\n",
    "\n",
    "            keypoint_features = self.keypoint_roi_pool(\n",
    "                features, keypoint_proposals, image_shapes\n",
    "            )\n",
    "            keypoint_features = self.keypoint_head(keypoint_features)\n",
    "            keypoint_logits = self.keypoint_predictor(keypoint_features)\n",
    "\n",
    "            loss_keypoint = {}\n",
    "            if self.training:\n",
    "                if targets is None or pos_matched_idxs is None:\n",
    "                    raise ValueError(\n",
    "                        \"both targets and pos_matched_idxs should not be None when in training mode\"\n",
    "                    )\n",
    "\n",
    "                gt_keypoints = [t[\"keypoints\"] for t in targets]\n",
    "                rcnn_loss_keypoint = keypointrcnn_loss(\n",
    "                    keypoint_logits, keypoint_proposals, gt_keypoints, pos_matched_idxs\n",
    "                )\n",
    "                loss_keypoint = {\"loss_keypoint\": rcnn_loss_keypoint}\n",
    "            else:\n",
    "                if keypoint_logits is None or keypoint_proposals is None:\n",
    "                    raise ValueError(\n",
    "                        \"both keypoint_logits and keypoint_proposals should not be None when not in training mode\"\n",
    "                    )\n",
    "\n",
    "                keypoints_probs, kp_scores = keypointrcnn_inference(\n",
    "                    keypoint_logits, keypoint_proposals\n",
    "                )\n",
    "                for keypoint_prob, kps, r in zip(keypoints_probs, kp_scores, result):\n",
    "                    r[\"keypoints\"] = keypoint_prob\n",
    "                    r[\"keypoints_scores\"] = kps\n",
    "            losses.update(loss_keypoint)\n",
    "\n",
    "        return result, losses\n",
    "    \n",
    "\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "model.roi_heads = OverrideRoIHeads(model.roi_heads.box_roi_pool, model.roi_heads.box_head, model.roi_heads.box_predictor, model.roi_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "LABELS\n",
      "torch.Size([238])\n",
      "self.proposals torch.Size([238, 4])\n",
      "box_feats torch.Size([470, 1024])\n",
      "class_logits torch.Size([470, 976])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 91\u001b[0m\n\u001b[1;32m     79\u001b[0m fake_inputs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda(), torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()]\n\u001b[1;32m     80\u001b[0m fake_targets \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     {\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m], [\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m40\u001b[39m]])\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     },\n\u001b[1;32m     89\u001b[0m ]\n\u001b[0;32m---> 91\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_targets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mM2.forward\u001b[0;34m(self, x, targets, quantity_targets)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_feats\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# custom_classification_loss = F.cross_entropy(\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#     self.class_logits, targets[\"labels\"]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# predicted_quantities = self.quantity_prediction(self.feats)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# loss_dict[\"quantity_loss\"] = F.mse_loss(predicted_quantities, quantity_targets)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_dict\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2\n",
    "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
    "\n",
    "class M2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M2, self).__init__()\n",
    "        self.box_feats = None\n",
    "        self.proposals = None\n",
    "        self.image_shapes = None\n",
    "\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            pretrained=False,\n",
    "            image_mean=[0.63, 0.63, 0.63],\n",
    "            image_std=[0.21, 0.21, 0.21],\n",
    "            min_size=64,\n",
    "            max_size=64,\n",
    "            num_classes=len(ALL_ITEMS),\n",
    "        )\n",
    "\n",
    "        # need to save the intermediate features\n",
    "        # self.model.roi_heads.box_predictor.cls_score.register_pre_forward_hook(\n",
    "        self.model.roi_heads.box_roi_pool.register_forward_pre_hook(\n",
    "            self.save_prop_img_shape\n",
    "        )\n",
    "        self.model.roi_heads.box_predictor.register_forward_hook(self.save_box_feats)\n",
    "        # self.model.roi_heads.box_predictor.bbox_pred.register_forward_pre_hook(\n",
    "        #     self.save_feats\n",
    "        # )\n",
    "        self.quantity_prediction = nn.Sequential(nn.Linear(1024, 1), nn.Sigmoid())\n",
    "        \n",
    "    def save_prop_img_shape(self, _module, input):\n",
    "        # Save the input features to be used later\n",
    "        self.proposals = input[1]\n",
    "        print(\"self.proposals\", self.proposals[0].shape)\n",
    "        self.image_shapes = input[2]\n",
    "        # print(\"self.proposals\", self.proposals.shape)\n",
    "        # print(\"self.image_shapes\", self.image_shapes)\n",
    "    \n",
    "    def save_box_feats(self, _module, input, output):\n",
    "        self.box_feats = input[0]\n",
    "        self.class_logits = output[0]\n",
    "\n",
    "    def forward(self, x, targets=None, quantity_targets=None):\n",
    "        if self.training:\n",
    "            # assert targets is not None and quantity_targets is not None\n",
    "            print(self.box_feats)\n",
    "            loss_dict = self.model(x, targets)\n",
    "            print(\"box_feats\", self.box_feats.shape)\n",
    "            print(\"class_logits\", self.class_logits.shape)\n",
    "            print(\"targets\", targets[\"labels\"])\n",
    "\n",
    "\n",
    "            # custom_classification_loss = F.cross_entropy(\n",
    "            #     self.class_logits, targets[\"labels\"]\n",
    "            # )\n",
    "            # print(custom_classification_loss)\n",
    "\n",
    "            # loss_classifier \n",
    "            # Compute the loss for the quantity classifier\n",
    "            # predicted_quantities = self.quantity_prediction(self.feats)\n",
    "            # loss_dict[\"quantity_loss\"] = F.mse_loss(predicted_quantities, quantity_targets)\n",
    "            return loss_dict\n",
    "        else:\n",
    "            print(self.box_feats)\n",
    "            preds = self.model(x)\n",
    "            print(self.box_feats.shape)\n",
    "            # preds[\"predicted_quantities\"] = self.quantity_prediction(\n",
    "            #     self.feats\n",
    "            # )\n",
    "            return preds\n",
    "\n",
    "m = M2()\n",
    "m.train()\n",
    "m.cuda()\n",
    "\n",
    "fake_inputs = [torch.rand(3, 64, 64).cuda(), torch.rand(3, 64, 64).cuda()]\n",
    "fake_targets = [\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[10, 10, 20, 20], [30, 30, 40, 40]]).cuda(),\n",
    "        \"labels\": torch.tensor([1, 2]).cuda(),\n",
    "    },\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[10, 10, 20, 20], [30, 30, 40, 40]]).cuda(),\n",
    "        \"labels\": torch.tensor([1, 2]).cuda(),\n",
    "    },\n",
    "]\n",
    "\n",
    "m(fake_inputs, targets=fake_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "  (box_head): FastRCNNConvFCHead(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=976, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=3904, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model.roi_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5241,  0.0000,  5.3955,  ...,  6.8653,  0.0000,  4.6057],\n",
      "        [ 6.0374,  0.0000,  8.8156,  ..., 13.1884,  0.0000,  1.1720],\n",
      "        [ 3.8993,  0.0000,  5.2919,  ...,  8.5901,  0.0000,  4.3840],\n",
      "        ...,\n",
      "        [ 5.4248,  0.0000, 14.0726,  ..., 11.6614,  0.0000,  4.3428],\n",
      "        [ 5.7381,  0.0000, 10.8562,  ...,  6.8517,  0.0000,  4.7301],\n",
      "        [ 1.7051,  0.0000,  3.7437,  ...,  7.6001,  0.0000,  6.3838]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "aaaa\n",
      "1\n",
      "torch.Size([627, 1024])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[0.0000e+00, 3.3344e+01, 4.9228e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 5.2162e+01, 5.5332e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 5.6636e+01, 3.4755e+01, 6.2779e+01],\n",
       "          [1.5261e+01, 1.5714e+01, 3.9311e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 4.1206e+01, 5.0870e+01, 5.6290e+01],\n",
       "          [0.0000e+00, 2.4415e+01, 2.8698e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 0.0000e+00, 5.0454e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 3.5726e+01, 2.4476e+01, 5.8015e+01],\n",
       "          [0.0000e+00, 6.0148e+01, 2.7772e+01, 6.3108e+01],\n",
       "          [0.0000e+00, 1.4634e+01, 4.3838e+01, 4.8110e+01],\n",
       "          [1.6675e+01, 3.8937e+01, 6.4000e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 3.1659e+01, 3.3259e+01, 4.8656e+01],\n",
       "          [6.8226e+00, 5.7123e+01, 6.0326e+01, 6.4000e+01],\n",
       "          [3.8750e+01, 0.0000e+00, 5.2835e+01, 5.7352e+01],\n",
       "          [4.1431e+00, 4.3653e+01, 1.5592e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 0.0000e+00, 2.1032e+01, 5.7359e+01],\n",
       "          [2.1397e-01, 2.5872e+01, 2.6626e+00, 6.4000e+01],\n",
       "          [2.4388e+01, 0.0000e+00, 4.1319e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 1.5366e+01, 6.6652e+00, 6.4000e+01],\n",
       "          [1.0324e+00, 3.8675e+01, 1.0645e+01, 6.4000e+01],\n",
       "          [3.5008e-01, 5.4817e+01, 2.2739e+00, 6.4000e+01],\n",
       "          [0.0000e+00, 2.5701e+01, 2.4256e-01, 4.4049e+01],\n",
       "          [2.1896e+00, 2.1055e+01, 1.2965e+01, 6.4000e+01],\n",
       "          [5.6305e+01, 4.3734e+01, 6.4000e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 2.4823e+01, 1.8287e+00, 4.4413e+01],\n",
       "          [4.0600e+01, 2.6693e+01, 6.4000e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 1.8120e+01, 1.1916e+01, 4.6181e+01],\n",
       "          [0.0000e+00, 2.7904e+01, 3.6064e+01, 4.1921e+01],\n",
       "          [0.0000e+00, 0.0000e+00, 8.9159e+00, 6.4000e+01],\n",
       "          [4.0035e+01, 0.0000e+00, 6.4000e+01, 5.8237e+01],\n",
       "          [1.1449e+01, 0.0000e+00, 2.6789e+01, 6.4000e+01],\n",
       "          [3.1015e+00, 4.2163e+01, 1.4721e+01, 5.5482e+01],\n",
       "          [0.0000e+00, 6.1395e+01, 3.2463e+01, 6.4000e+01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.4790e+00, 6.4000e+01],\n",
       "          [7.8991e-01, 1.7235e+01, 1.7872e+00, 1.8033e+01],\n",
       "          [1.6871e+01, 5.4923e+01, 4.7964e+01, 6.4000e+01],\n",
       "          [4.9122e+01, 0.0000e+00, 6.4000e+01, 2.1198e+01],\n",
       "          [2.7573e+00, 1.2094e+01, 6.4749e+00, 1.3148e+01],\n",
       "          [1.7799e+00, 0.0000e+00, 1.3272e+01, 4.8719e+01],\n",
       "          [0.0000e+00, 4.5167e+01, 2.2805e+01, 5.5822e+01],\n",
       "          [4.8903e+01, 0.0000e+00, 5.6669e+01, 6.3773e+01],\n",
       "          [4.0322e+00, 1.7509e+00, 3.5686e+01, 4.1126e+01],\n",
       "          [2.8109e+01, 3.7182e+00, 5.2735e+01, 3.8708e+00],\n",
       "          [1.8483e+01, 0.0000e+00, 3.2150e+01, 6.4000e+01],\n",
       "          [3.2700e+01, 4.6241e+01, 6.4000e+01, 6.4000e+01],\n",
       "          [8.1904e-01, 1.4787e+00, 1.6731e+00, 3.3865e+00],\n",
       "          [5.4527e+01, 1.1259e+01, 6.0038e+01, 1.1611e+01],\n",
       "          [4.2243e+00, 0.0000e+00, 9.6961e+00, 6.4000e+01],\n",
       "          [2.0090e+01, 4.1723e+00, 6.4000e+01, 6.4000e+01],\n",
       "          [2.2850e+00, 6.1736e+01, 1.6226e+01, 6.3713e+01],\n",
       "          [5.4177e+01, 0.0000e+00, 6.4000e+01, 8.0752e+00],\n",
       "          [5.6504e+01, 0.0000e+00, 6.4000e+01, 2.6030e+01],\n",
       "          [5.3194e+00, 4.5560e+00, 1.4704e+01, 4.7599e+00],\n",
       "          [4.3264e+01, 3.7086e-02, 6.4000e+01, 2.6147e-01],\n",
       "          [1.6999e+00, 0.0000e+00, 2.5406e+01, 1.8281e+01],\n",
       "          [5.2416e-01, 3.2162e+00, 1.1623e+00, 3.8389e+00],\n",
       "          [3.4250e+01, 8.2162e+00, 5.4465e+01, 8.3957e+00],\n",
       "          [6.1150e+01, 8.3476e+00, 6.4000e+01, 8.8173e+00],\n",
       "          [2.6562e+00, 1.1778e+01, 6.5095e+00, 1.2152e+01],\n",
       "          [2.5255e+00, 9.0096e+00, 5.8746e+00, 9.3638e+00],\n",
       "          [5.9225e+01, 4.2317e+01, 6.4000e+01, 4.3269e+01],\n",
       "          [6.9900e+00, 1.9012e+00, 1.9174e+01, 2.0914e+00],\n",
       "          [2.1101e+01, 0.0000e+00, 6.4000e+01, 2.5969e+01],\n",
       "          [1.3329e+01, 4.4249e+00, 2.7624e+01, 4.5716e+00],\n",
       "          [6.0690e+01, 2.4006e+01, 6.4000e+01, 2.4248e+01],\n",
       "          [1.1197e+01, 4.3447e+00, 2.4371e+01, 4.5878e+00],\n",
       "          [3.6137e+01, 0.0000e+00, 4.7137e+01, 6.4000e+01],\n",
       "          [2.0716e+00, 3.5386e+00, 4.4622e+00, 3.9589e+00],\n",
       "          [5.7837e+00, 2.7960e+00, 1.5623e+01, 2.9411e+00],\n",
       "          [2.5776e+00, 7.0084e+00, 5.3618e+00, 7.3299e+00],\n",
       "          [1.1579e+00, 5.6925e+00, 2.4069e+00, 6.0507e+00],\n",
       "          [2.0833e+01, 2.0478e+00, 4.8487e+01, 2.2407e+00],\n",
       "          [2.8634e+00, 0.0000e+00, 6.4640e+00, 3.4449e-01],\n",
       "          [4.8572e+01, 0.0000e+00, 6.4000e+01, 8.7939e-01],\n",
       "          [1.3336e+01, 6.8102e-02, 2.7035e+01, 2.4250e-01],\n",
       "          [3.0598e+00, 6.2847e+01, 4.6457e+01, 6.3838e+01],\n",
       "          [7.8191e+00, 4.0863e+00, 2.0582e+01, 4.2205e+00],\n",
       "          [1.0600e+01, 2.3588e+00, 2.4092e+01, 2.5313e+00],\n",
       "          [6.2378e+01, 4.2280e+00, 6.4000e+01, 4.4107e+00],\n",
       "          [6.6572e+00, 6.3559e+00, 1.4561e+01, 7.0119e+00],\n",
       "          [3.8930e+00, 7.1481e-01, 7.6903e+00, 2.5847e+00],\n",
       "          [4.3244e+01, 0.0000e+00, 5.1231e+01, 6.4000e+01],\n",
       "          [1.4646e+00, 1.6603e-01, 2.8971e+00, 2.2550e+00],\n",
       "          [7.2289e+00, 6.2228e+01, 3.4125e+01, 6.3652e+01],\n",
       "          [5.5749e+01, 3.0296e+01, 6.4000e+01, 3.1154e+01],\n",
       "          [4.8395e+01, 0.0000e+00, 6.4000e+01, 3.3317e+00],\n",
       "          [2.5441e+01, 5.6143e+01, 6.4000e+01, 6.3254e+01],\n",
       "          [7.9757e+00, 0.0000e+00, 4.9795e+01, 2.8478e+01],\n",
       "          [5.1939e+01, 1.1199e+01, 5.6994e+01, 1.1399e+01],\n",
       "          [2.2109e-01, 0.0000e+00, 3.4962e+00, 6.4000e+01],\n",
       "          [5.1439e+01, 0.0000e+00, 6.4000e+01, 6.4000e+01],\n",
       "          [2.9274e+01, 9.0661e+00, 5.0881e+01, 9.2844e+00],\n",
       "          [2.2129e-01, 0.0000e+00, 6.3549e+00, 3.4693e+01],\n",
       "          [3.5641e+01, 0.0000e+00, 5.0305e+01, 1.1339e+01],\n",
       "          [7.7345e-01, 0.0000e+00, 1.1757e+01, 1.6453e+01],\n",
       "          [2.4599e-02, 0.0000e+00, 6.3451e+00, 1.6934e+01],\n",
       "          [1.3135e+01, 2.4488e+01, 2.3006e+01, 2.4855e+01],\n",
       "          [5.5448e+01, 1.2810e+01, 6.3449e+01, 1.3263e+01],\n",
       "          [1.3926e+01, 0.0000e+00, 1.8209e+01, 6.4000e+01],\n",
       "          [4.3542e+01, 1.1681e+01, 4.6440e+01, 1.2161e+01]], device='cuda:0',\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "          42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 32, 42, 42, 42, 42, 32, 50, 42,\n",
       "          42, 50, 42, 42, 32, 42, 50, 42, 42, 50, 50, 32, 42, 42, 42, 42, 50, 50,\n",
       "          42, 50, 50, 50, 50, 50, 50, 50, 42, 50, 50, 50, 42, 50, 50, 50, 50, 50,\n",
       "          50, 42, 50,  4, 50, 50, 50, 50, 50, 32, 50, 42, 50, 42, 42, 42, 50, 42,\n",
       "          42, 50, 42, 42, 42, 42, 50, 50, 32, 50], device='cuda:0'),\n",
       "  'scores': tensor([0.9999, 0.9998, 0.9994, 0.9986, 0.9983, 0.9973, 0.9973, 0.9961, 0.9955,\n",
       "          0.9947, 0.9942, 0.9934, 0.9932, 0.9931, 0.9920, 0.9911, 0.9885, 0.9849,\n",
       "          0.9833, 0.9830, 0.9820, 0.9817, 0.9814, 0.9800, 0.9792, 0.9776, 0.9766,\n",
       "          0.9737, 0.9719, 0.9635, 0.9632, 0.9485, 0.9455, 0.9104, 0.9060, 0.9052,\n",
       "          0.9031, 0.8976, 0.8943, 0.8896, 0.8807, 0.8727, 0.8717, 0.8691, 0.8562,\n",
       "          0.8472, 0.8408, 0.8377, 0.8310, 0.8237, 0.8119, 0.8064, 0.7746, 0.7723,\n",
       "          0.7700, 0.7507, 0.7433, 0.7330, 0.7310, 0.7276, 0.7129, 0.7123, 0.7077,\n",
       "          0.7020, 0.6919, 0.6716, 0.6645, 0.6622, 0.6590, 0.6418, 0.6343, 0.5992,\n",
       "          0.5883, 0.5668, 0.5634, 0.5622, 0.5537, 0.5528, 0.5525, 0.5483, 0.5456,\n",
       "          0.5447, 0.5334, 0.5288, 0.5090, 0.5076, 0.4848, 0.4797, 0.4775, 0.4645,\n",
       "          0.4535, 0.4357, 0.4245, 0.4087, 0.4081, 0.4069, 0.3979, 0.3963, 0.3906,\n",
       "          0.3740], device='cuda:0', grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 67\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# # mean=[0.508, 0.492, 0.476], std=[0.241, 0.244, 0.255]\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# m = FasterRCNN(\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#     num_classes=None,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# m.transform\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mfasterrcnn_resnet50_fpn_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 32\u001b[0m, in \u001b[0;36mfasterrcnn_resnet50_fpn_v2\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m backbone \u001b[38;5;241m=\u001b[39m resnet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m rpn_anchor_generator \u001b[38;5;241m=\u001b[39m _default_anchorgen()\n\u001b[1;32m     31\u001b[0m rpn_head \u001b[38;5;241m=\u001b[39m RPNHead(\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m,\n\u001b[1;32m     33\u001b[0m     rpn_anchor_generator\u001b[38;5;241m.\u001b[39mnum_anchors_per_location()[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     34\u001b[0m     conv_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m box_head \u001b[38;5;241m=\u001b[39m FastRCNNConvFCHead(\n\u001b[1;32m     37\u001b[0m     (backbone\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m),\n\u001b[1;32m     38\u001b[0m     [\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m],\n\u001b[1;32m     39\u001b[0m     [\u001b[38;5;241m1024\u001b[39m],\n\u001b[1;32m     40\u001b[0m     norm_layer\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm2d,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m dual_head \u001b[38;5;241m=\u001b[39m DualFastRCNNPredictor()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'out_channels'"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection.faster_rcnn import (\n",
    "    FastRCNNPredictor,\n",
    "    FasterRCNN,\n",
    "    _default_anchorgen,\n",
    "    RPNHead,\n",
    "    FastRCNNConvFCHead,\n",
    ")\n",
    "from torchvision.models.resnet import resnet50\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DualFastRCNNPredictor(nn.Module):\n",
    "    def __init__(self, in_channels=1024):\n",
    "        super(DualFastRCNNPredictor, self).__init__()\n",
    "        self.quantity_predictor = FastRCNNPredictor(in_channels, 65)\n",
    "        self.label_predictor = FastRCNNPredictor(in_channels, 900)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.quantity_predictor(x), self.label_predictor(x))\n",
    "\n",
    "def fasterrcnn_resnet50_fpn_v2(\n",
    "    **kwargs\n",
    ") -> FasterRCNN:\n",
    "    \"\"\"\n",
    "    Constructs an improved Faster R-CNN model with a ResNet-50-FPN backbone from `Benchmarking Detection\n",
    "    Transfer Learning with Vision Transformers <https://arxiv.org/abs/2111.11429>`__ paper.\n",
    "    \"\"\"\n",
    "    backbone = resnet50(weights=None, progress=False)\n",
    "    rpn_anchor_generator = _default_anchorgen()\n",
    "    rpn_head = RPNHead(\n",
    "        backbone.out_channels,\n",
    "        rpn_anchor_generator.num_anchors_per_location()[0],\n",
    "        conv_depth=2,\n",
    "    )\n",
    "    box_head = FastRCNNConvFCHead(\n",
    "        (backbone.out_channels, 7, 7),\n",
    "        [256, 256, 256, 256],\n",
    "        [1024],\n",
    "        norm_layer=nn.BatchNorm2d,\n",
    "    )\n",
    "    dual_head = DualFastRCNNPredictor()\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=None,\n",
    "        rpn_anchor_generator=rpn_anchor_generator,\n",
    "        rpn_head=rpn_head,\n",
    "        box_head=box_head,\n",
    "        box_predictor=dual_head,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# # mean=[0.508, 0.492, 0.476], std=[0.241, 0.244, 0.255]\n",
    "# m = FasterRCNN(\n",
    "#     num_classes=None,\n",
    "#     pretrained=False,\n",
    "#     \n",
    "#     box_predictor=dual_head,\n",
    "# )\n",
    "# m.transform\n",
    "m = fasterrcnn_resnet50_fpn_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FasterRCNN.__init__() missing 1 required positional argument: 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m dual_head \u001b[38;5;241m=\u001b[39m DualFastRCNNPredictor()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# mean=[0.508, 0.492, 0.476], std=[0.241, 0.244, 0.255]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mFasterRCNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.508\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.492\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.476\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.241\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.244\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.255\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdual_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m m\u001b[38;5;241m.\u001b[39mtransform\n",
      "\u001b[0;31mTypeError\u001b[0m: FasterRCNN.__init__() missing 1 required positional argument: 'backbone'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.8073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.6006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.4154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1622, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# from train_fast_rcnn import M2\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
    "import wandb\n",
    "from PIL import Image\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "# model = M2()\n",
    "# model = model.cuda()\n",
    "\n",
    "img_encoder = mobilenet_v3_small(\n",
    "    pretrained=True,\n",
    "    image_mean=[0.508, 0.492, 0.476],\n",
    "    image_std=[0.241, 0.244, 0.255],\n",
    "    min_size=64,\n",
    "    max_size=64,\n",
    ")\n",
    "img_encoder.classifier = nn.Linear(576, len(ALL_ITEMS))\n",
    "img_encoder.cuda()\n",
    "\n",
    "# images are in data/bboxes/*.png\n",
    "class BboxDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = v2.Compose(\n",
    "            [\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                # v2.Normalize(mean=[0.508, 0.492, 0.476], std=[0.241, 0.244, 0.255]),\n",
    "            ]\n",
    "        )\n",
    "        self.paths = glob.glob(f\"{img_dir}/*.png\")\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        target = \"_\".join(img_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1:])\n",
    "        return img, ALL_ITEMS.index(target)\n",
    "\n",
    "\n",
    "dataset = BboxDataset(\"data/bboxes\")\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "m2_optimizer = torch.optim.AdamW(img_encoder.parameters(), lr=0.001)\n",
    "\n",
    "# wandb.init(project=\"plancraft-img-encoder\", entity=\"itl\")  # , mode=\"disabled\")\n",
    "\n",
    "i = 0\n",
    "for images, labels in dataloader:\n",
    "    # for i in range(1000):\n",
    "    encoded_image = img_encoder(images.cuda())\n",
    "    loss = nn.CrossEntropyLoss()(nn.Softmax(dim=1)(encoded_image), labels.cuda())\n",
    "    # wandb.log({\"loss\": loss})\n",
    "    if i%100 == 0:\n",
    "        print(loss)\n",
    "        # torch.save(img_encoder.state_dict(), \"mobilenet.pth\")\n",
    "\n",
    "    m2_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    m2_optimizer.step()\n",
    "        # model(batch.cuda())\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7266, device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = img_encoder(images.cuda()).softmax(dim=1).argmax(dim=1)\n",
    "(preds == labels.cuda()).sum() / len(labels)\n",
    "# for pred, label in zip(preds, labels):\n",
    "#     print(ALL_ITEMS[pred], ALL_ITEMS[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder.classifier = nn.Identity()\n",
    "torch.save(img_encoder.state_dict(), \"mobilenet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.81455</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sea-46</strong> at: <a href='https://wandb.ai/itl/plancraft-img-encoder/runs/s1ojjltx' target=\"_blank\">https://wandb.ai/itl/plancraft-img-encoder/runs/s1ojjltx</a><br/> View project at: <a href='https://wandb.ai/itl/plancraft-img-encoder' target=\"_blank\">https://wandb.ai/itl/plancraft-img-encoder</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240821_103645-s1ojjltx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoded_image[0].softmax()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161026"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(f\"data/bboxes/*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72568/72568 [00:22<00:00, 3201.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_paths = glob(\"data/bboxes/*.png\")\n",
    "img_arrs = []\n",
    "for image_path in tqdm(image_paths):\n",
    "    img = Image.open(image_path)\n",
    "    img_arr = np.array(img)\n",
    "    img_arrs.append(img_arr)\n",
    "\n",
    "# get mean and std\n",
    "img_arrs = np.array(img_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arrs = img_arrs / 255\n",
    "# (72568, 64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(img_arrs, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(img_arrs, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.50857319, 0.49239744, 0.47603964]),\n",
       " array([0.24135882, 0.24416414, 0.254905  ]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=171x164>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from train_fast_rcnn import slot_to_bbox, sample_starting_inv\n",
    "\n",
    "starting_inv = sample_starting_inv()\n",
    "env.fast_reset(new_inventory=starting_inv)\n",
    "obs, _, _, _ = env.step(env.action_space.no_op())\n",
    "\n",
    "img = Image.fromarray(obs[\"pov\"])\n",
    "for item in obs[\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    bbox = slot_to_bbox(item[\"index\"])\n",
    "    draw.rectangle(bbox, outline=\"red\")\n",
    "\n",
    "img_tensor = transforms.ToTensor()(obs[\"pov\"].copy()).cuda()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(img_tensor.unsqueeze(0)) \n",
    "\n",
    "# for box_idx in range(len(predictions[0][\"boxes\"])):\n",
    "#     if predictions[0][\"scores\"][box_idx] < 0.3:\n",
    "#         continue\n",
    "#     box = predictions[0][\"boxes\"][box_idx]\n",
    "#     label = predictions[0][\"labels\"][box_idx]\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"blue\")\n",
    "#     # add text label in middle of box\n",
    "#     draw.text(\n",
    "#         (box[0].item() + (box[2].item() - box[0].item()) / 2, box[1].item() + (box[3].item() - box[1].item()) / 2),\n",
    "#         str(label.item()),\n",
    "#         fill=\"green\",\n",
    "#     )\n",
    "# # show\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACkAKsDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1q71a9S8mRJQqq5UAKOxx3qH+2L//AJ7/APji/wCFQXv/AB/3H/XVv5moKAL39sX/APz3/wDHF/wqnc+Kr23uTAouJ2VBJL5MaHy1JIBIOCc7W4UE8dORltY+oSQWuomX+17WylmiRJFm2ltilsFMsMH5m5IYcDjg5TaW5UISm7RV2dAniSaWcwR6hC8wBYxrsLYDbScezAj6jFVJ/GscFg17/alvJCHEYaNoyGc9FB6Z+pGBycDmsS2uNDtreWNdVsBI88s4mWSMMruW+YZzyFbbk9h6cVQto9OiaZpvEVjK0j27A+cTgRSF8ZeVjznHXA64pc8e5r9Vr/yP7mdRF42aafyEu1MwjhkKbojgSNgc5wcZBOP7y4zkVZh8WC5jlkg1a2ljhG6VkeNgg55JHToevpXN3lxpF1dGddbsU3eRvBmQ58qXzFx8wxnLA9eo9Oakv9lrY2UUWsWEslnaiBN1yseWDRMHz82MGIHGCD0PFHPHuH1Wv/I/uZ2dt4kmvYzJaahDPGDtLRbGAPpkfUVN/bF//wA9/wDxxf8ACuR0i/sbOK4a71qxlnuJvNY/aY2x8qqBkBQeFB+6OuOcZOj/AG5pH/QVsf8AwIT/ABo549w+q1/5H9zN3+2L/wD57/8Aji/4Uf2xf/8APf8A8cX/AArNgnhuYVmglSWJujxsGB7dRUlUYtNOzL39sX//AD3/APHF/wAKP7Yv/wDnv/44v+FUaKBF7+2L/wD57/8Aji/4Uf2xf/8APf8A8cX/AAqjRQBe/ti//wCe/wD44v8AhR/bF/8A89//ABxf8Ko0UAXv7Yv/APnv/wCOL/hR/bF//wA9/wDxxf8ACqNFAF7+2L//AJ7/APji/wCFH9sX/wDz3/8AHF/wqjRQBPe/8f8Acf8AXVv5moKnvf8Aj/uP+urfzNQUAFclr3/IwP8A9esX/ocldbXJa9/yMD/9esX/AKHJUy3X9dDej8FT0/8AbolGp3/5A/8A2/w/+ibioKnf/kD/APb/AA/+ibiie33fmGH+N+kv/SWQVPb/AOrvf+vC7/8ARElQVPb/AOrvf+vC7/8ARElE/hYYX+PD1X5kFFFFUYHZ2n3r3/r/ALv/ANHyVYqvafevf+v+7/8AR8lWKmHwo3xX8efq/wAwoooqjAKKpanqcelwxSSRSy+ZJ5arFjOcE9yB0U1nf8JTD/0Dr7/yF/8AF1LklobRoTklLTXu0vzZvUVg/wDCUw/9A6+/8hf/ABdOj8TwPNFGbG8TzJFjDMI8AsQBnDk9TS51/SZSw1R6K3/gS/zNyiiirOcKKKKAMjX7ux/4SDVItWungs7VYXjSK5kgaWSV5xjMbK7ECH5UU87myGO3byOpanbFkGnWV7BCx4uLzVL99wwTny1nBVTxyzAjoVHWt/xTpU2p+JtYa3TzZLNrG58jfsMwDXylA38JIY4J4yAOM5HIXlzY2zZXUkaG4wqW3ll7lnywMfk7fkfIK85G4fd+bNY1XUvaJjVlUVlA3DrFtbW+mXemLcQXcmoW9rd29xqFxcr5crbcjzJGBB+8rqAcrg4+da0NanuIPEEn2e7urfNrFu8id493zyddpGfxrirnSNQXUtE1aewNna/2hbQp9pl3XUxeaJg7gcIMJ9zI2nPHNdjr3/IwP/16xf8AoclU1dJS/rQ7sLOcYTknZ26f4kQfbtR/6C2qf+B83/xVOlmu3sI7h9R1FpY72NUdr6YlQ0U+cfNweBz1xkdzVap3/wCQP/2/w/8Aom4pThG23b8zbD4ms5v33tLq/wCVh9u1H/oLap/4Hzf/ABVOE13dW17FPqOoyx/Yrlij30xUkQuRkbuRkDg8Gq1T2/8Aq73/AK8Lv/0RJROEeV6BhsTWdeCc3uur7h9u1H/oLap/4Hzf/FUfbtR/6C2qf+B83/xVQUVfJHsYfWq/87+9nYafGsMdzEpYql7dKC7lmwJ36k5JPueat1XtPvXv/X/d/wDo+SqP/CPWn/PSf/vof4UqfwIMU715t93+Zc1Gd7axkmjxvUrjI46ilsr2K+g8yPhhwyHqprJ1DRre0sZJ43lLLjAYjHJA9Kn0bTHt/wDSZtyyMMKmcYHv/h/kWYEPin/j2sP+vr/2nJWFW74p/wCPaw/6+v8A2nJWFUx3f9dEb1vgp+n/ALcwoH/Hzaf9fUH/AKMWigf8fNp/19Qf+jFon8LDC/x4eq/M7uiiiqMAooooApeI9ETU9QneO+vtPnEhDTWUoRpFBbCtkEMAWJGRkZOCNzZ5xPh9BHfyX8fiDW1vJV2SXCyQiR144LeVkjgfkK7e9/4/7j/rq38zUFAHP2fhOKC8iuLzVtU1MQsJIor6ZXjRx0fCqMsOcZzjr1AIra1MYfEEmLe1mzaxf69ZDj55Omx1/XNdTXJa9/yMD/8AXrF/6HJUTV2v66HTh5OMZtdvX7S7kH2x/wDoH6X/AN+5/wD49TpbiZrCOQW2nLGt7HmNYpsOTFPgkmbOAM8DHJHPGDWqd/8AkD/9v8P/AKJuKmcFb7ur7muHxE+d6LaX2Y/yvyD7Y/8A0D9L/wC/c/8A8epwuJpba9SO206EmyucukUxYDyXJAzMRkjIyQcZzg1Wqe3/ANXe/wDXhd/+iJKJwXK/82GGxE3Xhot19mPf0D7Y/wD0D9L/AO/c/wD8eo+2P/0D9L/79z//AB6oKKvkX9NmH1ifZf8AgMf8jsNPEgjuRM6vKL263si7VJ898kAk4Htk/WrdV7T717/1/wB3/wCj5KsUqfwIMU715+r/ADEKhhhgCMg80tFFWYGH4njke1szHDLLsudzCKMuQPLcZwAT1I/OsDE3/Pnff+Akv/xNd3RUWd3Zm6qU3FKcXppo7dW+z7nCYm/5877/AMBJf/iaWKKeS6tQLO8GLmFiWtpFAAkUkkkYHAruqKHGTVm/6+8qFWjCSnGLutd1/wDIhRRRVnMFFFFAD9UuIbW5upriaOGJZW3PIwVR82OSaijkSWNZI3V43AZWU5DA9CDUPiiKaey1eC3haWaVZY0RSBktkdSQO+azDaXz3E96ouY5muoTFG1wdqw4jEgKBin/AD0988jsaANusHU9Ne41h52iumiNvGitAkbchnJB3yJj7w9abNZaybFYo52EltAIN/mEm5+ZCz9Rg7VIGSDudvmAAYx2Gm6hPJCl+96tugm+X7Q0ZBPlbRlZXZukh5Y4yRwMUmrmlOpyX0vfTW/e/S3YjGmxmRoxHqRkUBivk2+QDnBx5/fB/I0+TTnNtHbpaaiVNykzu0cAKhY5V4HnckmQdxwD9KVdO1IE3b+cbueyt0n2T4+ZGzKoGQqllJClcAHccrnJkt9Ovp7uMTPewWGJSIjdHzBnytquwYsfmEjAhjgEDIBK1Lg31/L/ACNYYiMHdU11/m6q38xD/ZI/54ap/wB+Lf8A+P0p054re68m01GSWS2mhRXjgVcvGyDJExwPmz0NPuJNQiv/ALDA1xcCS4gkaZxIhRV8veAQnlkEKxOGXliMZ4PRUODatf8AL/IIYiMJKSpq6/xf/JHM/wBkj/nhqn/fi3/+P0f2SP8Anhqn/fi3/wDj9dNRT5X3/L/In20P+fa/8m/zK1kJDFNJLE0LS3M8wjcgsoeV2GcEjOCOhNWaKKaVlYzqTdSbm+ruFFFFMgKKKKACiiigAooooAKKKKAM7xfd3NpMv2WdoWkvGRmVVJxtc4+YEdQK5z+0dU/6Ck//AH7i/wDiK3fG3+uh/wCv9v8A0CSubrNK7dzrlUdOEOVLVdk+r7otxXuputwTqtx+7tp5hiOLqkbOP4OmVFR/2jqn/QUn/wC/cX/xFFv/AKu9/wCvC7/9ESVBQorma/VlSry9jGVle7+zHsvIn/tHVP8AoKT/APfuL/4ipLm91OG/vIF1W4Kw3M0KkxxZIR2UZ+TrgVUqe+/5DGp/9f8Ac/8Ao56HFcyX6vyCNeXsZSsr3X2Y9n5B/aOqf9BSf/v3F/8AEVJcXupxNbBdVuD5tsJmzHF1Mkqf3OmEH61Uqe8/1lj/ANeC/wDo+ehxV1/mwpV5OM20tF/LHuvIP7R1T/oKT/8AfuL/AOIqRr3UxYef/atxu+0xw48uLGGSVj/B1zGP1qpU7/8AIH/7f4f/AETcUSikv+CwoV5Sk00tn9mPZ+Qf2jqn/QUn/wC/cX/xFSRXuputwTqtx+7tp5hiOLqkbOP4OmVFVKnt/wDV3v8A14Xf/oiSicUot/qww9eUq0ItKza+zHv6B/aOqf8AQUn/AO/cX/xFH9o6p/0FJ/8Av3F/8RUFFVyL+mzH6xPsv/AY/wCRbub3U4b+8gXVbgrDczQqTHFkhHZRn5OuBUf9o6p/0FJ/+/cX/wARRff8hjU/+v8Auf8A0c9QVMIpxT/Vm2IryjWnFJWTf2Y9/Qt3F7qcTWwXVbg+bbCZsxxdTJKn9zphB+tR/wBo6p/0FJ/+/cX/AMRRef6yx/68F/8AR89QURimv+Cwr15Rkkktl9mPZeRba91MWHn/ANq3G77THDjy4sYZJWP8HXMY/Wo/7R1T/oKT/wDfuL/4ih/+QP8A9v8AD/6JuKgojFXf+bCpXkowaS1X8se78i3Fe6m63BOq3H7u2nmGI4uqRs4/g6ZUV2FcXb/6u9/68Lv/ANESV2lOKtJr+upnVlz0YyaV7vZJdI9jK8aMqXMDPEZVF+2UWTyyfkk/i2tj8jWB9otf+gXN/wCDIf8Axit3xt/rof8Ar/b/ANAkrm6STbev9WKnOMacE4p6db/zPs0WROhtr0W+mskhsrn5pL/cAPJfccCEZOM4GRzjmm/aLX/oFzf+DIf/ABii3/1d7/14Xf8A6IkqCkovmev5eZcqsPYR9xbv+btHzJ/tFr/0C5v/AAZD/wCMU6adBf3/ANp01mn+23HmGO/2rnzXyADCTgdM9+uB0qtU99/yGNT/AOv+5/8ARz0OL5lr+XkEasPYy9xbr+btLzD7Ra/9Aub/AMGQ/wDjFOuJ0+025l01jH9iTylW/wCQPOmyWPk8nOeAOAByc8VqnvP9ZY/9eC/+j56JRd1r+XYKVWHJP3Ft/e/mXmH2i1/6Bc3/AIMh/wDGKdLOhsI9umssAvY94N/lmPlT7QD5IAH3iTz0AxzkVqnf/kD/APb/AA/+ibiicXbft27hh6sOd+4tpfzfyvzD7Ra/9Aub/wAGQ/8AjFOE6G2vRb6aySGyufmkv9wA8l9xwIRk4zgZHOOarVPb/wCrvf8Arwu//RElE4vlev5BhqsPbw9xbr+bv6h9otf+gXN/4Mh/8Yo+0Wv/AEC5v/BkP/jFQUVfK+/5f5GHtof8+1/5N/mWZp0F/f8A2nTWaf7bceYY7/aufNfIAMJOB0z364HSm/aLX/oFzf8AgyH/AMYovv8AkMan/wBf9z/6OeoKiEXyrX8jfE1Ye2n7i3f83f1LNxOn2m3MumsY/sSeUq3/ACB502Sx8nk5zwBwAOTnhv2i1/6Bc3/gyH/xii8/1lj/ANeC/wDo+eoKIRdt+/buGIqw517i2j/N/KvMsyzobCPbprLAL2PeDf5Zj5U+0A+SAB94k89AMc5DftFr/wBAub/wZD/4xQ//ACB/+3+H/wBE3FQURi7vX8uwVasOSHuLb+9/M/MsidDbXot9NZJDZXPzSX+4AeS+44EIycZwMjnHNdlXF2/+rvf+vC7/APREldpVRVpPXt+pnVkpUI2ilrLa/aPdsyvGgQ3MAklESfb2y7KzAfJJ2UE/kKwNlj/0FYf/AAFuf/jVbvjb/XQ/9f7f+gSVzdJXu7f1oObpqnDnTenR2+0/JlkNZw216636zMbK5UJHa3GSTC47xgAc5JJAAyabssf+grD/AOAtz/8AGqLf/V3v/Xhd/wDoiSoKSUuZ6/1r5lylR9hH3Xu+q7R/uk+yx/6CsP8A4C3P/wAap0zWc9/fytfrCXvbhvLktbjcoMr4ziM4Pt1HfB4qtU99/wAhjU/+v+5/9HPQ1LmWv9aeYRlR9jL3Xuuq7S/uhssf+grD/wCAtz/8ap1w1nJc26fb1RY7JFDta3GHPnTH5f3ecDIGSAM5Azg4rVPef6yx/wCvBf8A0fPRJSutf6t6hSlR5J+69u6/mX90Nlj/ANBWH/wFuf8A41TpWs1sI4hfqwe9jYyC1uNqART9cx9TkYAyevYE1Wqd/wDkD/8Ab/D/AOibiialbft+fqGHlR537r2l1X8r/uhssf8AoKw/+Atz/wDGqcGs4ba9db9ZmNlcqEjtbjJJhcd4wAOckkgAZNVqnt/9Xe/9eF3/AOiJKJqXK9f6+8MNKj7eFovddV3/AMIbLH/oKw/+Atz/APGqNlj/ANBWH/wFuf8A41UFFXaXf8P+CYc1D+V/ev8A5EszNZz39/K1+sJe9uG8uS1uNygyvjOIzg+3Ud8Him7LH/oKw/8AgLc//GqL7/kMan/1/wBz/wCjnqCogpcq1/r7zfEyo+2neL3fVd/8JZuGs5Lm3T7eqLHZIodrW4w586Y/L+7zgZAyQBnIGcHDdlj/ANBWH/wFuf8A41Ref6yx/wCvBf8A0fPUFEFK2/f8/UMRKjzq8XtHqv5V/dLMrWa2EcQv1YPexsZBa3G1AIp+uY+pyMAZPXsCabssf+grD/4C3P8A8aof/kD/APb/AA/+ibioKIqV3r/VvUKsqPJD3Xt3X8z/ALpZDWcNteut+szGyuVCR2txkkwuO8YAHOSSQAMmuyri7f8A1d7/ANeF3/6IkrtKqN+Z38v1M6rg6EeRNay3d+kfJGZ4wgluby3igieWRr98Iilif3ch6Csb+xNW/wCgZe/+A7/4VueLriG3kImsIL0SXbIscxAUHDnPKnsCOneua+1Wn/Qs6V/30P8A41U8zTdv1/yNVRjUpwcnbT+73feSLi6XqFtbX8s9hdRRrYXWXeFlA/cOOpFM/sTVv+gZe/8AgO/+FQpPauJSPDOlfu4ZJjlx0RC5/wCWXXCmmfarT/oWdK/76H/xqlzvm2/P/I0eGh7JLm0u/wCXy/v/ANXLP9iat/0DL3/wHf8Awp9xpeoXOpalLBYXUsbX9zh0hZgf3zjqBVP7Vaf9CzpX/fQ/+NU+We1huZ4G8M6UWhmeFiHGCUYqcfuumRRzvmWn5/5AsND2TXNpdfy+f9/+rE39iat/0DL3/wAB3/wp91peoSXNrElhdNJHYJvQQsSuZ5yMjHGap/arT/oWdK/76H/xqnyz2sRiDeGdKPmwiZcOOhd05/ddcof0oc3dafn/AJBTw0FGaUt1/d01X98m/sTVv+gZe/8AgO/+FPl0vUI9NSJ7C6WSS/i2IYWBbEM5OBjnFU/tVp/0LOlf99D/AONU8z2otvP/AOEZ0rb5yw43jOWV2H/LLpiM/pRKba2/P/IKOGgpXUr6P+Xs/wC/0Jv7E1b/AKBl7/4Dv/hT10vULa2v5Z7C6ijWwusu8LKB+4cdSKp/arT/AKFnSv8Avof/ABqnpPauJSPDOlfu4ZJjlx0RC5/5ZdcKaJTbi9Pz/wAgoYaEasWpXd1/L/8AJk39iat/0DL3/wAB3/wo/sTVv+gZe/8AgO/+FVvtVp/0LOlf99D/AONUfarT/oWdK/76H/xqnzvt+f8AkZfVaf8AP/6T/wDJly40vULnUtSlgsLqWNr+5w6QswP75x1Apn9iat/0DL3/AMB3/wAKhlntYbmeBvDOlFoZnhYhxglGKnH7rpkUz7Vaf9CzpX/fQ/8AjVKM2orT8/8AI1r4aEqsm5Wd3/L/APJly60vUJLm1iSwumkjsE3oIWJXM85GRjjNM/sTVv8AoGXv/gO/+FQyz2sRiDeGdKPmwiZcOOhd05/ddcof0pn2q0/6FnSv++h/8aojN22/P/IK2Gg5JuVtF/L2X9/qXJdL1CPTUiewulkkv4tiGFgWxDOTgY5xTP7E1b/oGXv/AIDv/hUJntRbef8A8IzpW3zlhxvGcsrsP+WXTEZ/SmfarT/oWdK/76H/AMaoU3d6fn/kFTDQcYJy2X93XV/3y4ul6hbW1/LPYXUUa2F1l3hZQP3DjqRXU1xqT2riUjwzpX7uGSY5cdEQuf8All1wprsqqDvJmGIpqFKKTurvt5dmzI8bf66H/r/b/wBAkrm66rxdbtcXUSjcFW+ZmYRu+0bJBkhFJ6kDp3rD/s9P+fs/+AN3/wDGaFJJu7/qwpUalSnBwi3p0XmyC3/1d7/14Xf/AKIkqCr/ANlWC3vGE0krNaTxpGllc5ZniZVHMQHUjqaT+z0/5+z/AOAN3/8AGaFOPM9f61Klhq3sYrke76PtEo1Pff8AIY1P/r/uf/Rz1P8A2en/AD9n/wAAbv8A+M0stqt1eXtx50kSzXc8iq9lc52tKxU8REcgg9e9JzjzLX+tAjhq3sZLke66PtIoVPef6yx/68F/9Hz1P/Z6f8/Z/wDAG7/+M0s9qs1xAomkCw2iRmQ2Vzhm82ZiB+6zwGXqB14zTlON1qFLDVlCd4Pbs+6KFTv/AMgf/t/h/wDRNxU/9np/z9n/AMAbv/4zSy2qizjt1mkdnu0kLCyucIqxTAk5iz1dRwD1pTnG2/b8woYaspO8HtLo+zKFT2/+rvf+vC7/APRElT/2en/P2f8AwBu//jNL9lWC3vGE0krNaTxpGllc5ZniZVHMQHUjqac5x5XqGGw1ZVoNwe66PuUKKvf2en/P2f8AwBu//jNH9np/z9n/AMAbv/4zVc8e5j9Vr/yP7mQX3/IY1P8A6/7n/wBHPUFX5bVbq8vbjzpIlmu55FV7K5ztaVip4iI5BB696T+z0/5+z/4A3f8A8ZqITjyrU2xOGrOtNqD3fR9yC8/1lj/14L/6PnqCr89qs1xAomkCw2iRmQ2Vzhm82ZiB+6zwGXqB14zSf2en/P2f/AG7/wDjNOM4237hXw1ZzVoPaPR9kQP/AMgf/t/h/wDRNxXBXPiHVpZGaNntsZHlJAGwcngk857Hp06V6LLaqLOO3WaR2e7SQsLK5wirFMCTmLPV1HAPWs+78KaZfSmWdv3h4Z0tLxC3QfMViGenespO9+WVv6RqqVSKhem3p2fdmV4S1e51W21RbmMZhsbhRKowHJt5Mg9sjAJx/eHAr1GuQh0630+xuktydptLiOOGCwuVyzxOoAzEAOW9RXX1pDfe+36mFeEo0o80bay0+UTyeHx34hv4Y7yS8RHuFErKkKbQWGSBkE457mn/APCYa9/z/wD/AJBT/wCJoorU5A/4TDXv+f8A/wDIKf8AxNH/AAmGvf8AP/8A+QU/+JoooAP+Ew17/n//APIKf/E0f8Jhr3/P/wD+QU/+JoooAP8AhMNe/wCf/wD8gp/8TR/wmGvf8/8A/wCQU/8AiaKKAD/hMNe/5/8A/wAgp/8AE0f8Jhr3/P8A/wDkFP8A4miigA/4TDXv+f8A/wDIKf8AxNH/AAmGvf8AP/8A+QU/+JoooAP+Ew17/n//APIKf/E0f8Jhr3/P/wD+QU/+JoooAP8AhMNe/wCf/wD8gp/8TR/wmGvf8/8A/wCQU/8AiaKKAD/hMNe/5/8A/wAgp/8AE0f8Jhr3/P8A/wDkFP8A4miigA/4TDXv+f8A/wDIKf8AxNH/AAmGvf8AP/8A+QU/+JoooAP+Ew17/n//APIKf/E1Vm8QavPK0j6lchm6hJCg/IYAoooA/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=171x164>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from train_fast_rcnn import slot_to_bbox, sample_starting_inv\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "img = Image.fromarray(obs[\"pov\"])\n",
    "for item in obs[\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    bbox = slot_to_bbox(item[\"index\"])\n",
    "    draw.rectangle(bbox, outline=\"red\")\n",
    "\n",
    "# img_tensor = transforms.ToTensor()(obs[\"pov\"].copy()).cuda()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(img_tensor.unsqueeze(0)) \n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "# for box_idx in range(len(predictions[0][\"boxes\"])):\n",
    "#     if predictions[0][\"scores\"][box_idx] < 0.3:\n",
    "#         continue\n",
    "#     box = predictions[0][\"boxes\"][box_idx]\n",
    "#     label = predictions[0][\"labels\"][box_idx]\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"blue\")\n",
    "#     # add text label in middle of box\n",
    "#     draw.text(\n",
    "#         (box[0].item() + (box[2].item() - box[0].item()) / 2, box[1].item() + (box[3].item() - box[1].item()) / 2),\n",
    "#         str(label.item()+1),\n",
    "#         fill=\"red\",\n",
    "#     )\n",
    "\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'air', 'quantity': 0, 'index': 0},\n",
       " {'type': 'infested_stone_bricks', 'quantity': 35, 'index': 1},\n",
       " {'type': 'stripped_spruce_log', 'quantity': 59, 'index': 2},\n",
       " {'type': 'terracotta', 'quantity': 59, 'index': 3},\n",
       " {'type': 'air', 'quantity': 0, 'index': 4},\n",
       " {'type': 'warped_hyphae', 'quantity': 32, 'index': 5},\n",
       " {'type': 'air', 'quantity': 0, 'index': 6},\n",
       " {'type': 'orange_terracotta', 'quantity': 36, 'index': 7},\n",
       " {'type': 'air', 'quantity': 0, 'index': 8},\n",
       " {'type': 'air', 'quantity': 0, 'index': 9},\n",
       " {'type': 'salmon_spawn_egg', 'quantity': 47, 'index': 10},\n",
       " {'type': 'polished_diorite_slab', 'quantity': 40, 'index': 11},\n",
       " {'type': 'pufferfish_spawn_egg', 'quantity': 41, 'index': 12},\n",
       " {'type': 'prismarine_wall', 'quantity': 3, 'index': 13},\n",
       " {'type': 'air', 'quantity': 0, 'index': 14},\n",
       " {'type': 'cornflower', 'quantity': 41, 'index': 15},\n",
       " {'type': 'warped_pressure_plate', 'quantity': 34, 'index': 16},\n",
       " {'type': 'warped_stairs', 'quantity': 42, 'index': 17},\n",
       " {'type': 'feather', 'quantity': 64, 'index': 18},\n",
       " {'type': 'light_blue_dye', 'quantity': 11, 'index': 19},\n",
       " {'type': 'shield', 'quantity': 1, 'index': 20},\n",
       " {'type': 'air', 'quantity': 0, 'index': 21},\n",
       " {'type': 'honey_block', 'quantity': 54, 'index': 22},\n",
       " {'type': 'red_glazed_terracotta', 'quantity': 13, 'index': 23},\n",
       " {'type': 'bookshelf', 'quantity': 17, 'index': 24},\n",
       " {'type': 'white_concrete', 'quantity': 17, 'index': 25},\n",
       " {'type': 'cracked_nether_bricks', 'quantity': 49, 'index': 26},\n",
       " {'type': 'brown_banner', 'quantity': 9, 'index': 27},\n",
       " {'type': 'beetroot_seeds', 'quantity': 10, 'index': 28},\n",
       " {'type': 'magenta_glazed_terracotta', 'quantity': 49, 'index': 29},\n",
       " {'type': 'spruce_leaves', 'quantity': 32, 'index': 30},\n",
       " {'type': 'prismarine_wall', 'quantity': 24, 'index': 31},\n",
       " {'type': 'apple', 'quantity': 33, 'index': 32},\n",
       " {'type': 'birch_slab', 'quantity': 42, 'index': 33},\n",
       " {'type': 'yellow_glazed_terracotta', 'quantity': 54, 'index': 34},\n",
       " {'type': 'emerald_ore', 'quantity': 62, 'index': 35},\n",
       " {'type': 'air', 'quantity': 0, 'index': 36},\n",
       " {'type': 'air', 'quantity': 0, 'index': 37},\n",
       " {'type': 'lava_bucket', 'quantity': 1, 'index': 38},\n",
       " {'type': 'air', 'quantity': 0, 'index': 39},\n",
       " {'type': 'air', 'quantity': 0, 'index': 40},\n",
       " {'type': 'golden_pickaxe', 'quantity': 1, 'index': 41},\n",
       " {'type': 'air', 'quantity': 0, 'index': 42},\n",
       " {'type': 'wooden_shovel', 'quantity': 1, 'index': 43},\n",
       " {'type': 'dark_prismarine', 'quantity': 5, 'index': 44},\n",
       " {'type': 'air', 'quantity': 0, 'index': 45}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[\"inventory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpov\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy())\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(img_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "model.eval()\n",
    "\n",
    "idx = 0\n",
    "img_tensor = transforms.ToTensor()(data[idx][\"pov\"].copy()).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_tensor.unsqueeze(0))  \n",
    "\n",
    "img = Image.fromarray(data[idx][\"pov\"])\n",
    "for item in data[-1][\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(item[\"bbox\"], outline=\"red\")\n",
    "\n",
    "for box_idx in range(len(predictions[0][\"boxes\"])):\n",
    "    if predictions[0][\"scores\"][box_idx] < 0.5:\n",
    "        continue\n",
    "    box = predictions[0][\"boxes\"][box_idx]\n",
    "    label = predictions[0][\"labels\"][box_idx]\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle([box[0].item(), box[1].item(), box[2].item(), box[3].item()], outline=\"blue\")\n",
    "\n",
    "    # add text label in middle of box\n",
    "    draw.text(\n",
    "        (box[0].item() + (box[2].item() - box[0].item()) / 2, box[1].item() + (box[3].item() - box[1].item()) / 2),\n",
    "        str(label.item()+1),\n",
    "        fill=\"red\",\n",
    "    )\n",
    "\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def tmp():\n",
    "    n = 0\n",
    "    while n < 10:\n",
    "        if n == 5:\n",
    "            return\n",
    "        yield n\n",
    "        n += 1\n",
    "\n",
    "for i in tmp():\n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.7765,  ..., 0.7765, 0.7765, 0.7765],\n",
       "          ...,\n",
       "          [1.0000, 0.7765, 0.7765,  ..., 0.7765, 0.7765, 0.3333],\n",
       "          [0.7765, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class InventoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(self.data[idx][\"pov\"].copy())\n",
    "        target = {\n",
    "            \"labels\": [],\n",
    "            \"boxes\": []\n",
    "        }\n",
    "\n",
    "        for i in self.data[idx][\"inventory\"]:\n",
    "            target[\"labels\"].append(i[\"quantity\"])\n",
    "            target[\"boxes\"].append(i[\"bbox\"])\n",
    "        \n",
    "        target[\"labels\"] = torch.tensor(target[\"labels\"], dtype=torch.int64)-1\n",
    "        target[\"boxes\"] = torch.tensor(target[\"boxes\"], dtype=torch.int64)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "# N = len(data)\n",
    "train_dataset = InventoryDataset(data)\n",
    "# val_dataset = InventoryDataset(data[int(N*0.9):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.require(\"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=collate_fn)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplancraft-img-encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:1195\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:1181\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m   1180\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:785\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "lr = 0.005\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_stack = []\n",
    "    target_stack = []\n",
    "    for img, target in batch:\n",
    "        image_stack.append(img)\n",
    "        target_stack.append(target)\n",
    "    return torch.stack(image_stack), target_stack\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "wandb.init(project=\"plancraft-img-encoder\", entity=\"itl\")\n",
    "\n",
    "batch_num = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.cuda()\n",
    "        for i in range(len(labels)):\n",
    "            labels[i][\"boxes\"] = labels[i][\"boxes\"].cuda()\n",
    "            labels[i][\"labels\"] = labels[i][\"labels\"].cuda()\n",
    "        loss_dict = model(imgs, labels)\n",
    "        wandb.log(loss_dict)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_num}, Loss: {losses}\")\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for imgs, labels in val_loader:\n",
    "    #         imgs = imgs.cuda()\n",
    "    #         for i in range(len(labels)):\n",
    "    #             labels[i][\"boxes\"] = labels[i][\"boxes\"].cuda()\n",
    "    #             labels[i][\"labels\"] = labels[i][\"labels\"].cuda()\n",
    "\n",
    "    #         loss_dict = model(imgs, labels)\n",
    "\n",
    "    #         val_loss_dict = {}\n",
    "    #         for k, v in loss_dict.items():\n",
    "    #             val_loss_dict[f\"val_{k}\"] = v.item()\n",
    "    #         wandb.log(val_loss_dict)\n",
    "\n",
    "    #         losses = sum(loss for loss in loss_dict.values())\n",
    "    # print(f\"Epoch {epoch}, Loss: {losses}\")\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class GenerativeDataset:\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        return 1\n",
    "\n",
    "a = GenerativeDataset()\n",
    "\n",
    "c = 0\n",
    "for i in a:\n",
    "    print(i)\n",
    "    c += 1\n",
    "    if c == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=171x164>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = sum(loss for loss in loss_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[0][\"pov\"]\n",
    "torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module torchvision.models.detection.generalized_rcnn:\n",
      "\n",
      "forward(images, targets=None) method of torchvision.models.detection.faster_rcnn.FasterRCNN instance\n",
      "    Args:\n",
      "        images (list[Tensor]): images to be processed\n",
      "        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
      "    \n",
      "    Returns:\n",
      "        result (list[BoxList] or dict[Tensor]): the output from the model.\n",
      "            During training, it returns a dict[Tensor] which contains the losses.\n",
      "            During testing, it returns list[BoxList] contains additional fields\n",
      "            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/plancraft/outputs/oracle_real/train/0/TRAIN0157.json\n",
      "/plancraft/outputs/oracle_real/train/0/TRAIN0071.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "observed_data = []\n",
    "\n",
    "for f in glob.glob(\"/plancraft/outputs/oracle_real/train/0/*.json\"):\n",
    "    with open(f, \"r\") as file:\n",
    "        inventories = json.load(file)[\"model_trace\"][\"inventory_history\"]\n",
    "    gif_path = str(f).replace(\".json\", \".gif\")\n",
    "    # load gif as list of images\n",
    "    gif = Image.open(gif_path)\n",
    "    frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "    if len(frames) != len(inventories):\n",
    "        print(f)\n",
    "    else:\n",
    "        for frame, inv in zip(frames, inventories):\n",
    "            clean_inv = []\n",
    "            for item in inv:\n",
    "                if item[\"quantity\"] > 0:\n",
    "                    clean_inv.append(\n",
    "                        {\n",
    "                            \"type\": item[\"type\"],\n",
    "                            \"slot\": item[\"index\"],\n",
    "                            \"quantity\": item[\"quantity\"],\n",
    "                            \"bbox\": slot_to_bbox(item[\"index\"]),\n",
    "                        }\n",
    "                    )\n",
    "            observed_data.append(\n",
    "                {\"inventory\": clean_inv, \"pov\": np.array(frame.convert(\"RGB\"))}\n",
    "            )\n",
    "    # assert len(frames) == len(inv), (len(frames), len(inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class InventoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        pov = item[\"pov\"]\n",
    "        pov = self.transform(pov)\n",
    "\n",
    "        inventory = item[\"inventory\"]\n",
    "        types = [i[\"type\"] for i in inventory]\n",
    "        slots = [i[\"slot\"] for i in inventory]\n",
    "        quantities = [i[\"quantity\"] for i in inventory]\n",
    "        bboxes = [i[\"bbox\"] for i in inventory]\n",
    "\n",
    "        return pov, types, slots, quantities, bboxes\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = InventoryDataset(observed_data)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 11M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class InventoryModel(nn.Module):\n",
    "    def __init__(self, num_types, num_slots):\n",
    "        super(InventoryModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the classification layer\n",
    "\n",
    "        # Bounding box head\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),  # 4 coordinates for the bounding box\n",
    "        )\n",
    "\n",
    "        # Slot index prediction head\n",
    "        self.slot_head = nn.Sequential(\n",
    "            nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, num_slots), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Quantity prediction head\n",
    "        self.quantity_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Softmax(dim=1), \n",
    "        )\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad) // 1000000\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        bbox = self.bbox_head(features)\n",
    "        # types = self.type_head(features)\n",
    "        slots = self.slot_head(features)\n",
    "        quantity = self.quantity_head(features)\n",
    "        return bbox, slots, quantity\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = InventoryModel(num_types=100, num_slots=45)  # Replace with actual numbers\n",
    "model = model.cuda()\n",
    "# Count number of parameters\n",
    "print(f\"Model has {model.count_parameters()}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get oracle O,A Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "oracle_trajectories_train = []\n",
    "oracle_results = {\n",
    "    \"/plancraft/outputs/oracle_real/train/0/*.json\": [],\n",
    "    \"/plancraft/outputs/oracle_real/val/0/*.json\": [],\n",
    "}\n",
    "for path in oracle_results.keys():\n",
    "    for f in glob.glob(path):\n",
    "        with open(f, \"r\") as file:\n",
    "            traj = json.load(file)\n",
    "\n",
    "        images = []\n",
    "        gif_path = str(f).replace(\".json\", \".gif\")\n",
    "        gif = Image.open(gif_path)\n",
    "        for frame in ImageSequence.Iterator(gif):\n",
    "            images.append(np.array(frame.convert(\"RGB\")))\n",
    "        traj[\"model_trace\"][\"images\"] = images\n",
    "\n",
    "        if (\n",
    "            len(traj[\"model_trace\"][\"images\"])\n",
    "            == len(traj[\"model_trace\"][\"inventory_history\"])\n",
    "            == len(traj[\"model_trace\"][\"action_history\"])\n",
    "        ):\n",
    "            oracle_results[path].append(traj)\n",
    "        else:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from plancraft.models.react_prompts import REACT_SYSTEM_PROMPT\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def convert_obs_to_text(objective: str, inventory: list[dict]):\n",
    "    return f\"TASK: {objective}\\ninventory={json.dumps(inventory)}\"\n",
    "\n",
    "\n",
    "def convert_action_to_text(action: dict):\n",
    "    # {'action_type': 'move', 'slot_from': 17, 'slot_to': 1, 'quantity': 1}\n",
    "    return f\"act: {action['action_type']} from slot {action['slot_from']} to slot {action['slot_to']} with quantity {action['quantity']}\"\n",
    "\n",
    "\n",
    "# convert action and inventory to dialogue history\n",
    "def convert_trajectory_to_base_dialogue(traj: dict):\n",
    "    dialogue = [{\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT}]\n",
    "    objective = traj[\"model_trace\"][\"objective\"]\n",
    "    for _, action, inventory in zip(\n",
    "        traj[\"model_trace\"][\"images\"],\n",
    "        traj[\"model_trace\"][\"action_history\"],\n",
    "        traj[\"model_trace\"][\"inventory_history\"],\n",
    "    ):\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": convert_obs_to_text(objective, inventory),\n",
    "            }\n",
    "        )\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": convert_action_to_text(action),\n",
    "            }\n",
    "        )\n",
    "    example = {\n",
    "        \"messages\": dialogue,\n",
    "        \"example_id\": traj[\"example_id\"],\n",
    "    }\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert action and inventory to dialogue history\n",
    "def convert_trajectory_to_image_dialogue(traj: dict):\n",
    "    dialogue = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": REACT_SYSTEM_PROMPT}]}\n",
    "    ]\n",
    "    objective = traj[\"model_trace\"][\"objective\"]\n",
    "    images = []\n",
    "    for image, action, inventory in zip(\n",
    "        traj[\"model_trace\"][\"images\"],\n",
    "        traj[\"model_trace\"][\"action_history\"],\n",
    "        traj[\"model_trace\"][\"inventory_history\"],\n",
    "    ):\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": objective}],\n",
    "            }\n",
    "        )\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": convert_action_to_text(action)}],\n",
    "            }\n",
    "        )\n",
    "        images.append(image)\n",
    "    example = {\n",
    "        \"messages\": dialogue,\n",
    "        \"example_id\": traj[\"example_id\"],\n",
    "    }\n",
    "    return example, images\n",
    "\n",
    "\n",
    "text_data = defaultdict(list)\n",
    "mm_data = defaultdict(list)\n",
    "for path, trajs in oracle_results.items():\n",
    "    split = path.split(\"/\")[-3]\n",
    "    for traj in trajs:\n",
    "        text_example = convert_trajectory_to_base_dialogue(traj)\n",
    "        text_data[split].append(text_example)\n",
    "        mm_example, example_imgs = convert_trajectory_to_image_dialogue(traj)\n",
    "        mm_data[split].append(mm_example)\n",
    "        # save imgs as png in format \"data/oracle/{split}/{example_id}_{step}.gif\"\n",
    "        os.makedirs(f\"data/oracle/{split}\", exist_ok=True)\n",
    "        for i, img in enumerate(example_imgs):\n",
    "            Image.fromarray(img).save(\n",
    "                f\"data/oracle/{split}/{traj['example_id']}_{i}.png\"\n",
    "            )\n",
    "\n",
    "    # save as jsonl file\n",
    "    with open(f\"data/oracle/{split}.jsonl\", \"w\") as f:\n",
    "        for example in text_data[split]:\n",
    "            f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "    with open(f\"data/oracle/{split}.mm.jsonl\", \"w\") as f:\n",
    "        for example in mm_data[split]:\n",
    "            f.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Thought traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "split = \"train\"\n",
    "with open(f\"data/{split}.json\", \"r\") as f:\n",
    "    examples = json.load(f)\n",
    "df = pd.DataFrame(examples)\n",
    "\n",
    "dialogues = []\n",
    "with open(f\"data/oracle/{split}.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        dialogues.append(json.loads(line))\n",
    "\n",
    "dialogue_df = pd.DataFrame(dialogues)\n",
    "\n",
    "df = pd.merge(df, dialogue_df, left_on=\"id\", right_on=\"example_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "You are crafting in Minecraft.\n",
    "\n",
    "The first 10 slots in the inventory are reserved for crafting and correspond to the minecraft crafting table.\n",
    "[1, 2, 3]\n",
    "[4, 5, 6] -> [0]\n",
    "[7, 8, 9]\n",
    "\n",
    "The crafting matrix is a 3x3 grid, and the output is sent to slot 0.\n",
    "You cannot move or smelt items into output slot 0.\n",
    "The remaining slots (10-45) are for storing items.\n",
    "\n",
    "For a given observation action pair, you should explain the reasoning behind taking that action. Mention the state of the environment, what the next recipe to craft is (in the context of the greater planning problem) and why this move should be taken.\n",
    "\n",
    "Examples:\n",
    "\n",
    "TASK: Craft an item of type: andesite\n",
    "Crafting path: [andesite]\n",
    "\n",
    "1. inventory='[{\"type\": \"diorite\", \"slot\": 27, \"quantity\": 1},{\"type\": \"cobblestone\", \"slot\": 39, \"quantity\": 1}]'\n",
    "action: move from slot 27 to slot 4 with quantity 1\n",
    "thought: To solve this task I need to craft andesite. Andesite requires placing 1 diorite and 1 cobblestone side by side in the crafting table, therefore I will first need to move the diorite from slot 27 into the crafting grid. \n",
    "\n",
    "2. inventory=[{\"type\": \"diorite\", \"slot\": 4, \"quantity\": 1},{\"type\": \"cobblestone\", \"slot\": 39, \"quantity\": 1}]\n",
    "action: move from slot 39 to slot 5 with quantity 1\n",
    "thought: Since the diorite has been moved into the crafting grid, I now need to move the cobblestone to the right of it. Slot 5 is to the right of slot 4, and therefore I will move the cobblestone to slot 5.\n",
    "\n",
    "3. inventory=[{\"type\": \"andesite\", \"slot\": 0, \"quantity\": 1},{\"type\": \"diorite\", \"slot\": 4, \"quantity\": 1},{\"type\": \"cobblestone\", \"slot\": 5, \"quantity\": 1}]\n",
    "action: move from slot 0 to slot 15 with quantity 1\n",
    "thought: Now I can craft the andesite by moving it from the craft slot (0) to a free inventory slot (eg., 15). \n",
    "\n",
    "TASK: Craft an item of type: iron_ingot\n",
    "Crafting path: [iron_ingot]\n",
    "\n",
    "1. inventory='[{\"type\": \"iron_ore\", \"slot\": 45, \"quantity\": 1},{\"type\": \"cobblestone\", \"slot\": 39, \"quantity\": 1}]\n",
    "action: smelt from slot 45 to slot 44 with quantity 1\n",
    "thought: To craft an iron_ingot, I need to smelt the iron_ore at slot 45 into an empty slot (eg., 44).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|| 30/30 [00:24<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"/nfs/public/hf/models/meta-llama/Meta-Llama-3-70B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StopStringCriteria, StoppingCriteriaList\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopStringCriteria(\n",
    "    tokenizer=tokenizer, stop_strings=[\"\\n\", \"\\n\\n\", \".\\n\\n\", \"\\n\\n\\n\"]\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'TASK: Craft an item of type: red_dye\\ninventory=[{\"type\": \"torch\", \"quantity\": 57, \"index\": 19}, {\"type\": \"wooden_pickaxe\", \"quantity\": 1, \"index\": 21}, {\"type\": \"map\", \"quantity\": 22, \"index\": 26}, {\"type\": \"beetroot\", \"quantity\": 1, \"index\": 35}, {\"type\": \"lodestone\", \"quantity\": 50, \"index\": 36}]'}, {'role': 'assistant', 'content': 'To craft a red_dye, I need to move the beetroot from slot 35 to slot 1.'}, {'role': 'user', 'content': 'Ok'}, {'role': 'assistant', 'content': 'act: move from slot 35 to slot 1 with quantity 1'}, {'role': 'user', 'content': 'TASK: Craft an item of type: red_dye\\ninventory=[{\"type\": \"red_dye\", \"quantity\": 1, \"index\": 0}, {\"type\": \"beetroot\", \"quantity\": 1, \"index\": 1}, {\"type\": \"torch\", \"quantity\": 57, \"index\": 19}, {\"type\": \"wooden_pickaxe\", \"quantity\": 1, \"index\": 21}, {\"type\": \"map\", \"quantity\": 22, \"index\": 26}, {\"type\": \"lodestone\", \"quantity\": 50, \"index\": 36}]'}, {'role': 'assistant', 'content': 'Now that the red_dye is in the crafting slot, I can move it to the inventory slot 11.'}, {'role': 'user', 'content': 'Ok'}, {'role': 'assistant', 'content': 'act: move from slot 0 to slot 11 with quantity 1'}]\n"
     ]
    }
   ],
   "source": [
    "def generate_thoughts(row):\n",
    "    step = 1\n",
    "    task_message = BASE_PROMPT + row.messages[1][\"content\"].split(\"\\n\")[0]\n",
    "    task_message += f\"\\nCrafting path: {row.optimal_path}\"\n",
    "    OTA_messages = []\n",
    "    for i in range(1, len(row.messages), 2):\n",
    "        user_entry = row.messages[i]\n",
    "        assert user_entry[\"role\"] == \"user\"\n",
    "        assistant_entry = row.messages[i + 1]\n",
    "        assert assistant_entry[\"role\"] == \"assistant\"\n",
    "        inventory = user_entry[\"content\"].split(\"inventory=\")[1]\n",
    "        action = assistant_entry[\"content\"]\n",
    "        task_message += f\"\\n\\n{step}. inventory={inventory}\\naction: {action}\\nthought:\"\n",
    "        tokenized_prompt = tokenizer(task_message, return_tensors=\"pt\")\n",
    "        outputs = model.generate(\n",
    "            **tokenized_prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "        )\n",
    "        # Decode the generated output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).rstrip(\n",
    "            \"\\n\"\n",
    "        )\n",
    "        thought = generated_text.split(\"thought:\")[-1].strip()\n",
    "        OTA_messages.append(user_entry)  # Observation\n",
    "        OTA_messages.append({\"role\": \"assistant\", \"content\": f\"thought: {thought}\"})  # Thought\n",
    "        OTA_messages.append({\"role\": \"user\", \"content\": \"Ok\"})\n",
    "        OTA_messages.append(assistant_entry) # Action\n",
    "        task_message = generated_text\n",
    "        step += 1\n",
    "    return OTA_messages\n",
    "\n",
    "# Convert and print the result\n",
    "output = generate_thoughts(df.iloc[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRAIN0001'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"id\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
