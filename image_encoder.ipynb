{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from plancraft.environments.env_real import RealPlancraft\n",
    "\n",
    "env = RealPlancraft(\n",
    "    inventory=[],\n",
    "    symbolic_action_space=True,\n",
    "    symbolic_observation_space=True,\n",
    "    resolution=[512, 512],\n",
    "    crop=True,\n",
    ")\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.json\", \"r\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "seen_items = set()\n",
    "\n",
    "for example in train:\n",
    "    seen_items.update(example[\"inventory\"].keys())\n",
    "    seen_items.add(example[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'crimson_stairs', 'slot': 35, 'quantity': 57},\n",
       " {'type': 'yellow_terracotta', 'slot': 42, 'quantity': 8}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def slot_to_bbox(slot: int):\n",
    "    # crafting slot\n",
    "    if slot == 0:\n",
    "        # slot size: 25x25\n",
    "        # top left corner: (x= 118, y=30)\n",
    "        box_size = 25\n",
    "        left_x = 117\n",
    "        top_y = 29\n",
    "    # crafting grid\n",
    "    elif slot < 10:\n",
    "        # slot size: 18x18\n",
    "        # top left corner: (x = 28 + 18 * col, y = 16 + 18 * row)\n",
    "        box_size = 18\n",
    "        row = (slot - 1) // 3\n",
    "        col = (slot - 1) % 3\n",
    "        left_x = 27 + (box_size * col)\n",
    "        top_y = 15 + (box_size * row)\n",
    "    # inventory\n",
    "    elif slot < 37:\n",
    "        # slot size: 18x18\n",
    "        # top left corner: (x= 6 + 18 * col, y=83 + 18 * row)\n",
    "        box_size = 18\n",
    "        row = (slot - 10) // 9\n",
    "        col = (slot - 10) % 9\n",
    "        left_x = 5 + (box_size * col)\n",
    "        top_y = 82 + (box_size * row)\n",
    "    # hotbar\n",
    "    else:\n",
    "        # slot size: 18x18\n",
    "        # top left corner: (x= 6 + 18 * col, y=141)\n",
    "        box_size = 18\n",
    "        col = (slot - 37) % 9\n",
    "        left_x = 5 + (box_size * col)\n",
    "        top_y = 140\n",
    "    return [left_x, top_y, left_x + box_size, top_y + box_size]\n",
    "\n",
    "\n",
    "def sample_starting_inv():\n",
    "    inventory = []\n",
    "    for _ in range(random.randint(0, 35)):\n",
    "        slot = random.randint(10, 44)\n",
    "        inventory.append(\n",
    "            {\n",
    "                \"type\": random.choice(list(seen_items)),\n",
    "                \"slot\": slot,\n",
    "                \"quantity\": random.randint(1, 64),\n",
    "            }\n",
    "        )\n",
    "    # sort by slot\n",
    "    inventory = sorted(inventory, key=lambda x: x[\"slot\"])\n",
    "    return inventory\n",
    "\n",
    "sample_starting_inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "\n",
    "peft_model_id = \"HuggingFaceM4/idefics2-8b-chatty\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"/nfs/public/hf/models/HuggingFaceM4/idefics2-8b-chatty\",\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_adapter(peft_model_id)\n",
    "# model.model.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraModel, LoraConfig\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     task_type=\"SEQ_2_SEQ_LM\",\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.01,\n",
    "# )\n",
    "# lora_model = LoraModel(model, config, \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(1):\n",
    "    starting_inv = sample_starting_inv()\n",
    "    env.fast_reset(new_inventory=starting_inv)\n",
    "    obs, _, _, _ = env.step(env.action_space.no_op())\n",
    "\n",
    "    # clean up inventory\n",
    "    clean_inv = []\n",
    "    for item in starting_inv:\n",
    "        if item[\"quantity\"] > 0:\n",
    "            clean_inv.append(\n",
    "                {\n",
    "                    \"type\": item[\"type\"],\n",
    "                    \"index\": item[\"slot\"],\n",
    "                    \"quantity\": item[\"quantity\"],\n",
    "                    \"bbox\": slot_to_bbox(item[\"slot\"]),\n",
    "                }\n",
    "            )\n",
    "    obs[\"inventory\"] = clean_inv\n",
    "    data.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=171x164>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "img = Image.fromarray(data[-1][\"pov\"])\n",
    "for item in data[-1][\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(item[\"bbox\"], outline=\"red\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/plancraft/outputs/oracle_real/train/0/TRAIN0157.json\n",
      "/plancraft/outputs/oracle_real/train/0/TRAIN0071.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "observed_data = []\n",
    "\n",
    "for f in glob.glob(\"/plancraft/outputs/oracle_real/train/0/*.json\"):\n",
    "    with open(f, \"r\") as file:\n",
    "        inventories = json.load(file)[\"model_trace\"][\"inventory_history\"]\n",
    "    gif_path = str(f).replace(\".json\", \".gif\")\n",
    "    # load gif as list of images\n",
    "    gif = Image.open(gif_path)\n",
    "    frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "    if len(frames) != len(inventories):\n",
    "        print(f)\n",
    "    else:\n",
    "        for frame, inv in zip(frames, inventories):\n",
    "            clean_inv = []\n",
    "            for item in inv:\n",
    "                if item[\"quantity\"] > 0:\n",
    "                    clean_inv.append(\n",
    "                        {\n",
    "                            \"type\": item[\"type\"],\n",
    "                            \"slot\": item[\"index\"],\n",
    "                            \"quantity\": item[\"quantity\"],\n",
    "                            \"bbox\": slot_to_bbox(item[\"index\"]),\n",
    "                        }\n",
    "                    )\n",
    "            observed_data.append(\n",
    "                {\"inventory\": clean_inv, \"pov\": np.array(frame.convert(\"RGB\"))}\n",
    "            )\n",
    "    # assert len(frames) == len(inv), (len(frames), len(inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class InventoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        pov = item[\"pov\"]\n",
    "        pov = self.transform(pov)\n",
    "\n",
    "        inventory = item[\"inventory\"]\n",
    "        types = [i[\"type\"] for i in inventory]\n",
    "        slots = [i[\"slot\"] for i in inventory]\n",
    "        quantities = [i[\"quantity\"] for i in inventory]\n",
    "        bboxes = [i[\"bbox\"] for i in inventory]\n",
    "\n",
    "        return pov, types, slots, quantities, bboxes\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = InventoryDataset(observed_data)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 11M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class InventoryModel(nn.Module):\n",
    "    def __init__(self, num_types, num_slots):\n",
    "        super(InventoryModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the classification layer\n",
    "\n",
    "        # Bounding box head\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),  # 4 coordinates for the bounding box\n",
    "        )\n",
    "\n",
    "        # Slot index prediction head\n",
    "        self.slot_head = nn.Sequential(\n",
    "            nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, num_slots), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Quantity prediction head\n",
    "        self.quantity_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Softmax(dim=1), \n",
    "        )\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad) // 1000000\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        bbox = self.bbox_head(features)\n",
    "        # types = self.type_head(features)\n",
    "        slots = self.slot_head(features)\n",
    "        quantity = self.quantity_head(features)\n",
    "        return bbox, slots, quantity\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = InventoryModel(num_types=100, num_slots=45)  # Replace with actual numbers\n",
    "model = model.cuda()\n",
    "# Count number of parameters\n",
    "print(f\"Model has {model.count_parameters()}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pov, types, slots, quantities, bboxes = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack(pov)\n",
    "images = images.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images\n",
    "model.model.vision_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# device = \"cuda\"\n",
    "\n",
    "# # Define loss functions for each head\n",
    "# bbox_loss_fn = nn.MSELoss()\n",
    "# type_loss_fn = nn.CrossEntropyLoss()\n",
    "# slot_loss_fn = nn.CrossEntropyLoss()\n",
    "# quantity_loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# def train_model(train_loader, model, optimizer, num_epochs=10):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for pov, types, slots, quantities, bboxes in train_loader:\n",
    "#             pov = pov.to(device)\n",
    "#             bboxes = bboxes.to(device)\n",
    "#             types = torch.tensor(types).to(device)\n",
    "#             slots = torch.tensor(slots).to(device)\n",
    "#             quantities = torch.tensor(quantities).float().to(device)\n",
    "\n",
    "#             # optimizer.zero_grad()\n",
    "\n",
    "#             pred_bboxes, pred_types, pred_slots, pred_quantities = model(pov)\n",
    "\n",
    "#             # loss_bbox = bbox_loss_fn(pred_bboxes, bboxes)\n",
    "#             # loss_type = type_loss_fn(pred_types, types)\n",
    "#             # loss_slot = slot_loss_fn(pred_slots, slots)\n",
    "#             # loss_quantity = quantity_loss_fn(pred_quantities, quantities)\n",
    "\n",
    "#             # total_loss = loss_bbox + loss_type + loss_slot + loss_quantity\n",
    "#             # total_loss.backward()\n",
    "#             # optimizer.step()\n",
    "\n",
    "#         # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(train_loader, model, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get oracle O,A Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/plancraft/outputs/oracle_real/train/0/TRAIN0341.json\n",
      "/plancraft/outputs/oracle_real/train/0/TRAIN0735.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "with open(\"data/train.json\", \"r\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "oracle_trajectories_train = []\n",
    "\n",
    "oracle_results = {\n",
    "    \"/plancraft/outputs/oracle_real/train/0/*.json\": [],\n",
    "    \"/plancraft/outputs/oracle_real/val/0/*.json\": [],\n",
    "}\n",
    "for path in oracle_results.keys():\n",
    "    for f in glob.glob(path):\n",
    "        with open(f, \"r\") as file:\n",
    "            traj = json.load(file)\n",
    "\n",
    "        images = []\n",
    "        gif_path = str(f).replace(\".json\", \".gif\")\n",
    "        gif = Image.open(gif_path)\n",
    "        for frame in ImageSequence.Iterator(gif):\n",
    "            images.append(np.array(frame.convert(\"RGB\")))\n",
    "        traj[\"model_trace\"][\"images\"] = images\n",
    "\n",
    "        if (\n",
    "            len(traj[\"model_trace\"][\"images\"])\n",
    "            == len(traj[\"model_trace\"][\"inventory_history\"])\n",
    "            == len(traj[\"model_trace\"][\"action_history\"])\n",
    "        ):\n",
    "            oracle_results[path].append(traj)\n",
    "        else:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plancraft.models.react_prompts import REACT_SYSTEM_PROMPT\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def convert_obs_to_text(objective: str, inventory: list[dict]):\n",
    "    return f\"TASK: {objective}\\ninventory={json.dumps(inventory)}\"\n",
    "\n",
    "\n",
    "def convert_action_to_text(action: dict):\n",
    "    # {'action_type': 'move', 'slot_from': 17, 'slot_to': 1, 'quantity': 1}\n",
    "    return f\"act: {action['action_type']} from slot {action['slot_from']} to slot {action['slot_to']} with quantity {action['quantity']}\"\n",
    "\n",
    "\n",
    "# convert action and inventory to dialogue history\n",
    "def convert_trajectory_to_base_dialogue(traj: dict):\n",
    "    dialogue = [{\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT}]\n",
    "    objective = traj[\"model_trace\"][\"objective\"]\n",
    "    for _, action, inventory in zip(\n",
    "        traj[\"model_trace\"][\"images\"],\n",
    "        traj[\"model_trace\"][\"action_history\"],\n",
    "        traj[\"model_trace\"][\"inventory_history\"],\n",
    "    ):\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": convert_obs_to_text(objective, inventory),\n",
    "            }\n",
    "        )\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": convert_action_to_text(action),\n",
    "            }\n",
    "        )\n",
    "    example = {\n",
    "        \"messages\": dialogue,\n",
    "        \"example_id\": traj[\"example_id\"],\n",
    "    }\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert action and inventory to dialogue history\n",
    "def convert_trajectory_to_image_dialogue(traj: dict):\n",
    "    dialogue = [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": REACT_SYSTEM_PROMPT}]}]\n",
    "    objective = traj[\"model_trace\"][\"objective\"]\n",
    "    images = []\n",
    "    for image, action, inventory in zip(\n",
    "        traj[\"model_trace\"][\"images\"],\n",
    "        traj[\"model_trace\"][\"action_history\"],\n",
    "        traj[\"model_trace\"][\"inventory_history\"],\n",
    "    ):\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": objective}\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": convert_action_to_text(action)}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        images.append(image)\n",
    "    example = {\n",
    "        \"messages\": dialogue,\n",
    "        \"example_id\": traj[\"example_id\"],\n",
    "    }\n",
    "    return example, images\n",
    "\n",
    "import os\n",
    "text_data = defaultdict(list)\n",
    "mm_data = defaultdict(list)\n",
    "for path,trajs in oracle_results.items():\n",
    "    split = path.split(\"/\")[-3]\n",
    "    for traj in trajs:\n",
    "        text_example = convert_trajectory_to_base_dialogue(traj)\n",
    "        text_data[split].append(text_example)\n",
    "        mm_example, example_imgs = convert_trajectory_to_image_dialogue(traj)\n",
    "        mm_data[split].append(mm_example)\n",
    "        # save imgs as png in format \"data/oracle/{split}/{example_id}_{step}.gif\"\n",
    "        os.makedirs(f\"data/oracle/{split}\", exist_ok=True)        \n",
    "        for i, img in enumerate(example_imgs):\n",
    "            Image.fromarray(img).save(f\"data/oracle/{split}/{traj['example_id']}_{i}.png\")\n",
    "\n",
    "    # save as jsonl file\n",
    "    with open(f\"data/oracle/{split}.jsonl\", \"w\") as f:\n",
    "        for example in text_data[split]:\n",
    "            f.write(json.dumps(example) + \"\\n\")\n",
    "    \n",
    "    with open(f\"data/oracle/{split}.mm.jsonl\", \"w\") as f:\n",
    "        for example in mm_data[split]:\n",
    "            f.write(json.dumps(example) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Llama on the oracle O,A dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "data = defaultdict(list)\n",
    "for split in [\"train\", \"val\"]:\n",
    "    with open(f\"data/oracle/{split}.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            data[split].append(json.loads(line))\n",
    "\n",
    "MAX_WINDOW_SIZE = 30\n",
    "NUM_OVERSAMPLING = 3\n",
    "\n",
    "def sample_window(example):\n",
    "    # add system message\n",
    "    new_messages = [example[\"messages\"][0]]\n",
    "    num_steps = len(example[\"messages\"])-1\n",
    "\n",
    "    start = random.randint(1, num_steps)\n",
    "    if start%2 == 0:\n",
    "        start = start + 1\n",
    "    window_size = min(MAX_WINDOW_SIZE, start)\n",
    "    new_messages = new_messages + example[\"messages\"][start - window_size+1 : start]\n",
    "    # print(f\"window size: {window_size}, start: {start}, num_steps: {num_steps}\")\n",
    "    # new_messages = new_messages + example[\"messages\"][start : start + window_size]\n",
    "    return new_messages\n",
    "\n",
    "\n",
    "def oversample_long_dialogue_dataset(examples: list[dict]):\n",
    "    window_train = []\n",
    "    for example in examples:\n",
    "        if len(example[\"messages\"]) > MAX_WINDOW_SIZE:\n",
    "            for _ in range(NUM_OVERSAMPLING):\n",
    "                window_train.append({\"messages\": sample_window(example)})\n",
    "        else:\n",
    "            window_train.append({\"messages\": example[\"messages\"]})\n",
    "    return window_train\n",
    "\n",
    "train_dataset = Dataset.from_list(oversample_long_dialogue_dataset(data[\"train\"]))\n",
    "val_dataset = Dataset.from_list(oversample_long_dialogue_dataset(data[\"val\"]))\n",
    "\n",
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "val_dataset = val_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert messages to text format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying chat template to train dataset (num_proc=6): 100%|██████████| 1325/1325 [00:00<00:00, 1891.94 examples/s]\n",
      "Applying chat template to val dataset (num_proc=6): 100%|██████████| 667/667 [00:00<00:00, 1329.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: apply_chat_template(x, tokenizer),\n",
    "    batched=False,\n",
    "    num_proc=6,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Applying chat template to train dataset\",\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: apply_chat_template(x, tokenizer),\n",
    "    batched=False,\n",
    "    num_proc=6,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Applying chat template to val dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='len', ylabel='Count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1XElEQVR4nO3de3RU5b3/8U8SciUmgQSSoAlB5BIg3BUD6lFMDaBUBK3a0IJSrZ5ADWnUYqUIVkK9IOqJWFka7FHKKT1g1SocDALaBgyRKChSPcIvVGYSKU5CuCQx8/z+8DB1CiQhTGbPTt6vtWat7L2f7PnO7EY+nee7nwkyxhgBAADYULDVBQAAALQVQQYAANgWQQYAANgWQQYAANgWQQYAANgWQQYAANgWQQYAANhWF6sLaG9ut1sHDx7Ueeedp6CgIKvLAQAArWCM0ZEjR9SrVy8FB5/5c5cOH2QOHjyolJQUq8sAAABtcODAAV1wwQVnPN7hg8x5550n6ds3IiYmxuJqAABAa9TW1iolJcXz7/iZdPggc3I6KSYmhiADAIDNtNQWQrMvAACwLUuDTFpamoKCgk555ObmSpJOnDih3NxcxcfHKzo6WtOmTVNVVZWVJQMAgABiaZApKyuTw+HwPDZu3ChJuummmyRJc+fO1euvv641a9Zoy5YtOnjwoKZOnWplyQAAIIAEGWOM1UWclJeXpzfeeEOfffaZamtr1aNHD61atUo33nijJOnTTz9Venq6SktLdemll572HPX19aqvr/dsn2wWqqmpoUcGAOBTTU1NamxstLoMWwoNDVVISMgZj9fW1io2NrbFf78Dptm3oaFBL7/8svLz8xUUFKTy8nI1NjYqKyvLM2bgwIFKTU1tNsgUFhZq4cKF/iobANAJGWPkdDrlcrmsLsXW4uLilJSUdE7rvAVMkHn11Vflcrk0c+ZMSZLT6VRYWJji4uK8xiUmJsrpdJ7xPPPmzVN+fr5n++QnMgAA+MrJENOzZ09FRUWx4OpZMsbo2LFjqq6uliQlJye3+VwBE2ReeOEFTZw4Ub169Tqn84SHhys8PNxHVQEA4K2pqckTYuLj460ux7YiIyMlSdXV1erZs2ez00zNCYjbr//f//t/evvtt/WTn/zEsy8pKUkNDQ2nfGxXVVWlpKQkP1cIAMC3TvbEREVFWVyJ/Z18D8+lzygggkxxcbF69uypa6+91rNv1KhRCg0NVUlJiWff3r17VVlZqczMTCvKBADAg+mkc+eL99DyqSW3263i4mLNmDFDXbr8s5zY2FjNmjVL+fn56t69u2JiYjRnzhxlZmaesdEXAAB0LpYHmbfffluVlZW6/fbbTzn25JNPKjg4WNOmTVN9fb2ys7P17LPPWlAlAAAIRJZPLV1zzTUyxqh///6nHIuIiFBRUZEOHz6so0ePau3atfTHAABwlq688krl5eV5ttPS0rRs2TLL6vElyz+RAQAA/lVWVqauXbt6toOCgrRu3TpNmTLFuqLaiCADAEAn06NHD6tL8BnLp5YAAEDL/vjHPyojI0ORkZGKj49XVlaWjh49qpkzZ2rKlClauHChevTooZiYGN11111qaGg447m+O7WUlpYmSbrhhhsUFBTk2bYLPpEBOomM4SPkdDiaHZOUnKxdFTv9VBGA1nI4HLr11lv16KOP6oYbbtCRI0f07rvv6uTXJZaUlCgiIkKbN2/W/v37ddtttyk+Pl6PPPJIi+cuKytTz549VVxcrAkTJrR5YTqrEGSATsLpcOiBl7c2O2bx9Cv8VA2As+FwOPTNN99o6tSp6t27tyQpIyPDczwsLEwvvviioqKiNHjwYC1atEj33nuvHn74YQUHNz/5cnKa6eT3HtkNU0sAAAS4YcOG6eqrr1ZGRoZuuukmrVixQl9//bXX8e+uNJyZmam6ujodOHDAinL9iiADAECACwkJ0caNG/XWW29p0KBBeuaZZzRgwADt27fP6tIsR5ABAMAGgoKCNG7cOC1cuFA7d+5UWFiY1q1bJ0n68MMPdfz4cc/Ybdu2KTo6WikpKa06d2hoqJqamtql7vZGkAEAIMBt375dixcv1o4dO1RZWam1a9fqq6++Unp6uiSpoaFBs2bN0ieffKI333xTCxYs0OzZs1vsjzkpLS1NJSUlcjqdXlNWdkCQAQAgwMXExGjr1q2aNGmS+vfvrwcffFBPPPGEJk6cKEm6+uqr1a9fP11xxRW6+eab9f3vf18PPfRQq8//xBNPaOPGjUpJSdGIESPa6VW0D+5aAgAgwKWnp2v9+vXNjlm4cKEWLlx42mObN2/22t6/f7/X9uTJkzV58uRzKdEyfCIDAABsiyADAABsi6mlDobVWwGgc1m5cqXVJViKINPBsHorAKAzYWoJAADYFkEGAADYFkEGAADYFkEGAADYFs2+AAD4SGVlpQ4dOuS350tISFBqaqrfnu+70tLSlJeXp7y8PEue/ySCDAAAPlBZWamB6ek6fuyY354zMipKn+7Z0+owc+WVV2r48OFatmzZOT93WVmZunbtes7nOVcEGQAAfODQoUM6fuyYcu5/TImpfdv9+aoq/1ev/OZeHTp0yGefyhhj1NTUpC5dWo4HPXr08MlzniuCDAAAPpSY2lcX9BtsdRmnmDlzprZs2aItW7boqaeekiQVFxfrtttu05tvvqkHH3xQu3bt0v/8z/8oJSVF+fn52rZtm44ePar09HQVFhYqKyvLc75/nVoKCgrSihUr9Oc//1kbNmzQ+eefryeeeELf//732/V10ewLAEAn8NRTTykzM1N33HGHHA6HHA6HUlJSJEm/+MUvtGTJEu3Zs0dDhw5VXV2dJk2apJKSEu3cuVMTJkzQ5MmTVVlZ2exzLFy4UD/4wQ/00UcfadKkScrJydHhw4fb9XURZAAA6ARiY2MVFhamqKgoJSUlKSkpSSEhIZKkRYsW6Xvf+5769u2r7t27a9iwYfrpT3+qIUOGqF+/fnr44YfVt29fvfbaa80+x8yZM3Xrrbfqoosu0uLFi1VXV6f333+/XV8XQQYAgE5u9OjRXtt1dXUqKChQenq64uLiFB0drT179rT4iczQoUM9P3ft2lUxMTGqrq5ul5pPokcGAIBO7l/vPiooKNDGjRv1+OOP66KLLlJkZKRuvPFGNTQ0NHue0NBQr+2goCC53W6f1/tdBBkAADqJsLAwNTU1tTjuL3/5i2bOnKkbbrhB0ref0Ozfv7+dq2sbppYAAOgk0tLStH37du3fv1+HDh0646cl/fr109q1a1VRUaEPP/xQP/zhD9v9k5W24hMZAAB8qKryfwP2eQoKCjRjxgwNGjRIx48fV3Fx8WnHLV26VLfffrvGjh2rhIQE3X///aqtrT3XktsFQQYAAB9ISEhQZFSUXvnNvX57zsioKCUkJLR6fP/+/VVaWuq1b+bMmaeMS0tL06ZNm7z25ebmem3/61STMeaU87hcrlbX1lYEGQAAfCA1NVWf7tnTab5rKVAQZAAA8JHU1NROHyz8jWZfAABgWwQZAABgWwQZAABgWwQZAABgWwQZAABgWwQZAABgWwQZAABgW6wjAwCAj1RWVnboBfHS0tKUl5envLw8vz1nSywPMl9++aXuv/9+vfXWWzp27JguuugiFRcXa/To0ZK+XfJ4wYIFWrFihVwul8aNG6fly5erX79+FlcOAMA/VVZWKj19oI4dO+6354yKitSePZ926kX4LA0yX3/9tcaNG6errrpKb731lnr06KHPPvtM3bp184x59NFH9fTTT+ull15Snz59NH/+fGVnZ+uTTz5RRESEhdUDAPBPhw4d0rFjx/XyAz9QemqPdn++PZVfafriP+jQoUMEGav85je/UUpKite3b/bp08fzszFGy5Yt04MPPqjrr79ekvS73/1OiYmJevXVV3XLLbeccs76+nrV19d7tgP12zoBAB1TemoPjex/vtVlnOL555/XQw89pL///e8KDv5ni+z111+v+Ph4/fKXv1R+fr62bdumo0ePKj09XYWFhcrKyrKw6pZZ2uz72muvafTo0brpppvUs2dPjRgxQitWrPAc37dvn5xOp9ebGBsbqzFjxpzy7Z0nFRYWKjY21vNISUlp99cBAECgu+mmm/SPf/xD77zzjmff4cOHtX79euXk5Kiurk6TJk1SSUmJdu7cqQkTJmjy5MmqrKy0sOqWWRpkvvjiC0+/y4YNG3T33XfrZz/7mV566SVJktPplCQlJiZ6/V5iYqLn2L+aN2+eampqPI8DBw6074sAAMAGunXrpokTJ2rVqlWefX/84x+VkJCgq666SsOGDdNPf/pTDRkyRP369dPDDz+svn376rXXXrOw6pZZGmTcbrdGjhypxYsXa8SIEbrzzjt1xx136LnnnmvzOcPDwxUTE+P1AAAAUk5Ojv77v//b04Lxyiuv6JZbblFwcLDq6upUUFCg9PR0xcXFKTo6Wnv27OETmeYkJydr0KBBXvvS09M9b1pSUpIkqaqqymtMVVWV5xgAAGidyZMnyxijP//5zzpw4IDeffdd5eTkSJIKCgq0bt06LV68WO+++64qKiqUkZGhhoYGi6tunqXNvuPGjdPevXu99v3tb39T7969JX3b+JuUlKSSkhINHz5c0rfNu9u3b9fdd9/t73IBALC1iIgITZ06Va+88oo+//xzDRgwQCNHjpQk/eUvf9HMmTN1ww03SJLq6uq0f/9+C6ttHUuDzNy5czV27FgtXrxYP/jBD/T+++/r+eef1/PPPy9JCgoKUl5enn7961+rX79+ntuve/XqpSlTplhZuk9lDB8hp8PR4rik5GTtqtjph4papzV1B1rNANDZ5eTk6LrrrtPHH3+s6dOne/b369dPa9eu1eTJkxUUFKT58+fL7XZbWGnrWBpkLr74Yq1bt07z5s3TokWL1KdPHy1btszzMZck3XfffTp69KjuvPNOuVwuXXbZZVq/fn2HWkPG6XDogZe3tjhu8fQr/FBN67Wm7kCrGQDa257KrwL6ecaPH6/u3btr7969+uEPf+jZv3TpUt1+++0aO3asEhISdP/999tiCRPLV/a97rrrdN11153xeFBQkBYtWqRFixb5sSoAAM5OQkKCoqIiNX3xH/z2nFFRkUpISDir3wkODtbBgwdP2Z+WlqZNmzZ57cvNzfXaDsSpJsuDDAAAHUFqaqr27Pm0Q3/XUiAiyAAA4COpqamdPlj4m6W3XwMAAJwLggwAALAtggwAAG1gjLG6BNvzxXtIkAEA4CyEhoZKko4dO2ZxJfZ38j08+Z62Bc2+AACchZCQEMXFxam6ulqSFBUVpaCgIIurshdjjI4dO6bq6mrFxcUpJCSkzeciyAAAcJZOft/fyTCDtomLizvn704kyAAAcJaCgoKUnJysnj17qrGx0epybCk0NPScPok5iSADAEAbhYSE+OQfY7Qdzb4AAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2WBAPnUrG8BFyOhzNjklKTtauip1+qggAcC4IMuhUnA6HHnh5a7NjFk+/wk/VAADOFVNLAADAtggyAADAtggyAADAtuiRAf6Fy1WjHolJzY6hIRgAAgNBBvgXbrebhmAAsAmmlgAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG11sboAoDUyho+Q0+FodkxScrJ2Vez0U0UAgEBgaZB56KGHtHDhQq99AwYM0KeffipJOnHihH7+859r9erVqq+vV3Z2tp599lklJiZaUS4s5HQ49MDLW5sds3j6FX6qBgAQKCyfWho8eLAcDofn8d5773mOzZ07V6+//rrWrFmjLVu26ODBg5o6daqF1QIAgEBi+dRSly5dlJSUdMr+mpoavfDCC1q1apXGjx8vSSouLlZ6erq2bdumSy+91N+lAgCAAGP5JzKfffaZevXqpQsvvFA5OTmqrKyUJJWXl6uxsVFZWVmesQMHDlRqaqpKS0vPeL76+nrV1tZ6PQAAQMdkaZAZM2aMVq5cqfXr12v58uXat2+fLr/8ch05ckROp1NhYWGKi4vz+p3ExEQ5nc4znrOwsFCxsbGeR0pKSju/CgAAYBVLp5YmTpzo+Xno0KEaM2aMevfurT/84Q+KjIxs0znnzZun/Px8z3ZtbS1hBgCADsryqaXviouLU//+/fX5558rKSlJDQ0NcrlcXmOqqqpO21NzUnh4uGJiYrweAACgYwqoIFNXV6f//d//VXJyskaNGqXQ0FCVlJR4ju/du1eVlZXKzMy0sEoAABAoLJ1aKigo0OTJk9W7d28dPHhQCxYsUEhIiG699VbFxsZq1qxZys/PV/fu3RUTE6M5c+YoMzOTO5YAAIAki4PM3//+d9166636xz/+oR49euiyyy7Ttm3b1KNHD0nSk08+qeDgYE2bNs1rQTw7ac2KtC5XjZ+qAQCgY7E0yKxevbrZ4xERESoqKlJRUZGfKvK91qxIWzApw0/VAADQsQRUjwwAAMDZIMgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbsnRBPMCuXK4a9Ug885eXSlJScrJ2Vez0U0UA0DkRZIA2cLvdLa7YvHj6FX6qBgA6L6aWAACAbRFkAACAbRFkAACAbRFkAACAbdHsi9PKGD5CToej2TEuV42fqum4WvM+c/cTAJwZQQan5XQ4Wrwrp2BShp+q6bha8z5z9xMAnBlTSwAAwLYIMgAAwLYIMgAAwLbokUG7Yil/AEB7IsigXbGUPwCgPTG1BAAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbItvv0abGWO09PFHWxwDAEB7IcigzYyM8m8c2+yY/JKX/FQNAKAzYmoJAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYVsAEmSVLligoKEh5eXmefSdOnFBubq7i4+MVHR2tadOmqaqqyroiAQBAQAmIIFNWVqbf/va3Gjp0qNf+uXPn6vXXX9eaNWu0ZcsWHTx4UFOnTrWoSgAAEGgsDzJ1dXXKycnRihUr1K1bN8/+mpoavfDCC1q6dKnGjx+vUaNGqbi4WH/961+1bds2CysGAACBwvIgk5ubq2uvvVZZWVle+8vLy9XY2Oi1f+DAgUpNTVVpaekZz1dfX6/a2lqvBwAA6JgsXdl39erV+uCDD1RWVnbKMafTqbCwMMXFxXntT0xMlNPpPOM5CwsLtXDhQl+XinbkctWoR2JSi2MAAPhXlgWZAwcO6J577tHGjRsVERHhs/POmzdP+fn5nu3a2lqlpKT47PzwPbfbrQde3trsmIJJGX6qBgBgJ5ZNLZWXl6u6ulojR45Uly5d1KVLF23ZskVPP/20unTposTERDU0NMjlcnn9XlVVlZKSzvz/3sPDwxUTE+P1AAAAHZNln8hcffXV2rVrl9e+2267TQMHDtT999+vlJQUhYaGqqSkRNOmTZMk7d27V5WVlcrMzLSiZAAAEGDaFGQuvPBClZWVKT4+3mu/y+XSyJEj9cUXX7R4jvPOO09Dhgzx2te1a1fFx8d79s+aNUv5+fnq3r27YmJiNGfOHGVmZurSSy9tS9kAAKCDaVOQ2b9/v5qamk7ZX19fry+//PKcizrpySefVHBwsKZNm6b6+nplZ2fr2Wef9dn5AQCAvZ1VkHnttdc8P2/YsEGxsbGe7aamJpWUlCgtLa3NxWzevNlrOyIiQkVFRSoqKmrzOQEAQMd1VkFmypQpkqSgoCDNmDHD61hoaKjS0tL0xBNP+Kw4AACA5pxVkHG73ZKkPn36qKysTAkJCe1SFAAAQGu0qUdm3759vq4DAADgrLX59uuSkhKVlJSourra80nNSS+++OI5FwYAANCSNgWZhQsXatGiRRo9erSSk5MVFBTk67oAAABa1KYg89xzz2nlypX60Y9+5Ot6AAAAWq1NX1HQ0NCgsWPH+roWAACAs9KmIPOTn/xEq1at8nUtAAAAZ6VNU0snTpzQ888/r7fffltDhw5VaGio1/GlS5f6pDicPZfLpaWPP9riGAAAOoI2BZmPPvpIw4cPlyTt3r3b6xiNv9Zyu93Kv7H5ab+CTb/zUzUAALSvNgWZd955x9d1AAAAnLU29cgAAAAEgjZ9InPVVVc1O4W0adOmNhcEAADQWm0KMif7Y05qbGxURUWFdu/efcqXSQIIDDSCA+iI2hRknnzyydPuf+ihh1RXV3dOBQFoHzSCA+iIfNojM336dL5nCQAA+I1Pg0xpaakiIiJ8eUoAAIAzatPU0tSpU722jTFyOBzasWOH5s+f75PCAAAAWtKmIBMbG+u1HRwcrAEDBmjRokW65pprfFIYAABAS9oUZIqLi31dB/zIbaQeiUnNjnG5avxUTcflctX47X3OGD5CToej2THG+OSpACCgtCnInFReXq49e/ZIkgYPHqwRI0b4pCi0N6MHXt7a7IiCSRl+qqXjcrvdfnufnQ5Hi8+VP3GwT54LAAJJm4JMdXW1brnlFm3evFlxcXGSvl1/4qqrrtLq1avVo0cPX9YIAABwWm26a2nOnDk6cuSIPv74Yx0+fFiHDx/W7t27VVtbq5/97Ge+rhEAAOC02vSJzPr16/X2228rPT3ds2/QoEEqKiqi2RcAAPhNm4KM2+1WaGjoKftDQ0PldrvPuSgAsKvWNF4nJSdrV8VOP1UEdGxtCjLjx4/XPffco9///vfq1auXJOnLL7/U3LlzdfXVV/u0QACwk9Y0Xi+efoWfqgE6vjb1yPzHf/yHamtrlZaWpr59+6pv377q06ePamtr9cwzz/i6RgAAgNNq0ycyKSkp+uCDD/T222/r008/lSSlp6crKyvLp8UBAAA056w+kdm0aZMGDRqk2tpaBQUF6Xvf+57mzJmjOXPm6OKLL9bgwYP17rvvtletAAAAXs4qyCxbtkx33HGHYmJiTjkWGxurn/70p1q6dKnPigMAAGjOWQWZDz/8UBMmTDjj8WuuuUbl5eXnXBQAAEBrnFWQqaqqOu1t1yd16dJFX3311TkXBQAA0BpnFWTOP/987d69+4zHP/roIyUnJ59zUQAAAK1xVkFm0qRJmj9/vk6cOHHKsePHj2vBggW67rrrfFYcAABAc87q9usHH3xQa9euVf/+/TV79mwNGDBAkvTpp5+qqKhITU1N+uUvf9kuhQKwD1a3BeAvZxVkEhMT9de//lV333235s2bJ2OMJCkoKEjZ2dkqKipSYmJiuxQKwD5Y3RaAv5z1gni9e/fWm2++qa+//lqff/65jDHq16+funXr1h71AQAAnFGbVvaVpG7duuniiy/2ZS0AAABnpU3ftQQAABAI2vyJDAC0N5qGAbSEIAMgYNE0DKAllk4tLV++XEOHDlVMTIxiYmKUmZmpt956y3P8xIkTys3NVXx8vKKjozVt2jRVVVVZWDEAAAgklgaZCy64QEuWLFF5ebl27Nih8ePH6/rrr9fHH38sSZo7d65ef/11rVmzRlu2bNHBgwc1depUK0sGAAABxNKppcmTJ3ttP/LII1q+fLm2bdumCy64QC+88IJWrVql8ePHS5KKi4uVnp6ubdu26dJLLz3tOevr61VfX+/Zrq2tbb8XAAAALBUwPTJNTU1as2aNjh49qszMTJWXl6uxsVFZWVmeMQMHDlRqaqpKS0vPGGQKCwu1cOFCf5UNm3G5XFr6+KPNjjm50CMAIPBZHmR27dqlzMxMnThxQtHR0Vq3bp0GDRqkiooKhYWFKS4uzmt8YmKinE7nGc83b9485efne7Zra2uVkpLSXuXDZtxut/JvHNvsmPySl/xUDQDgXFkeZAYMGKCKigrV1NToj3/8o2bMmKEtW7a0+Xzh4eEKDw/3YYUAACBQWR5kwsLCdNFFF0mSRo0apbKyMj311FO6+eab1dDQIJfL5fWpTFVVlZKSkiyqFgAABJKAW9nX7Xarvr5eo0aNUmhoqEpKSjzH9u7dq8rKSmVmZlpYIQAACBSWfiIzb948TZw4UampqTpy5IhWrVqlzZs3a8OGDYqNjdWsWbOUn5+v7t27KyYmRnPmzFFmZuYZG30BAEDnYmmQqa6u1o9//GM5HA7FxsZq6NCh2rBhg773ve9Jkp588kkFBwdr2rRpqq+vV3Z2tp599lkrS+4wuHMHANARWBpkXnjhhWaPR0REqKioSEVFRX6qqPPgzh0AQEcQcD0yAAAArUWQAQAAtkWQAQAAtkWQAQAAtkWQAQAAtkWQAQAAtkWQAQAAtkWQAQAAtmX5l0ai9VyuGvVIbP4LM1mQF2g/GcNHyOlwNDvG5app8Tyt+VtOSk7WroqdZ1Uf0BkRZGzE7XbrgZe3Njsmf+JgP1UDdD5Oh6PFv8GCSRktnqc1f8uLp19xVrUBnRVTSwAAwLYIMgAAwLYIMgAAwLbokQkAxhgtffzRVo1D59OaxtDWNJgGmo76ugD4F0EmABgZ5d84tsVx+SUv+aEaBJrWNIa2psE00HTU1wXAv5haAgAAtkWQAQAAtkWQAQAAtkWPDDoMVj4GgM6HIIMOg5WPAaDzYWoJAADYFkEGAADYFkEGAADYFj0yOKPWrDYMAICVCDI4o5ZWG2alYQCA1ZhaAgAAtkWQAQAAtkWQAQAAtkWQAQAAtkWzL3AaLd2xZQLsuw6MMX67yyxj+Ag5HY5mx7hcNX6pBQAIMsBp2O2OLSPjt5qdDkeLXwVRMCnDJ88FAC1hagkAANgWQQYAANgWQQYAANgWPTLngKbHzs0XDcGtadINtMbijoq/Z8CeCDLngKbHzs0XzbX+bNJF8/h7BuyJqSUAAGBbBBkAAGBbBBkAAGBbBBkAAGBbBBkAAGBblgaZwsJCXXzxxTrvvPPUs2dPTZkyRXv37vUac+LECeXm5io+Pl7R0dGaNm2aqqqqLKoYAAAEEkuDzJYtW5Sbm6tt27Zp48aNamxs1DXXXKOjR496xsydO1evv/661qxZoy1btujgwYOaOnWqhVUDAIBAYek6MuvXr/faXrlypXr27Kny8nJdccUVqqmp0QsvvKBVq1Zp/PjxkqTi4mKlp6dr27ZtuvTSS085Z319verr6z3btbW17fsiAACAZQJqQbyamm9Xzezevbskqby8XI2NjcrKyvKMGThwoFJTU1VaWnraIFNYWKiFCxf6p2D4DSvg4kxcrhr1SExqdkzd0aOK7tq1xfMAsJ+ACTJut1t5eXkaN26chgwZIklyOp0KCwtTXFyc19jExEQ5nc7TnmfevHnKz8/3bNfW1iolJaXd6oZ/sAIuzsTtdrdqRd4H1pW3OAaA/QRMkMnNzdXu3bv13nvvndN5wsPDFR4e7qOqAABAIAuI269nz56tN954Q++8844uuOACz/6kpCQ1NDTI5XJ5ja+qqlJSUvMfJQMAgI7P0iBjjNHs2bO1bt06bdq0SX369PE6PmrUKIWGhqqkpMSzb+/evaqsrFRmZqa/ywUAAAHG0qml3NxcrVq1Sn/605903nnnefpeYmNjFRkZqdjYWM2aNUv5+fnq3r27YmJiNGfOHGVmZp620Rf21JpGXrvqqK8LAAKFpUFm+fLlkqQrr7zSa39xcbFmzpwpSXryyScVHBysadOmqb6+XtnZ2Xr22Wf9XCnaU0du5O2orwsAAoWlQaY1t8tGRESoqKhIRUVFfqgIAADYSUA0+wIAALQFQQYAANgWQQYAANhWwCyIZ0cul6vFu1Lcxs2dK7ANt1GLy/23Zil/vlICgL8QZM6B2+1u1V0p3LkC+zCtWu6/5bN03DvRAAQWppYAAIBtEWQAAIBtEWQAAIBt0SMDAD5EozPgXwQZAPAhGp0B/2JqCQAA2BZBBgAA2BZBBgAA2BY9MgC8BFKjKo2zAFpCkAHgJZAaVWmcBdASppYAAIBtEWQAAIBtEWQAAIBt0SNjMy01PgYiO9YMoH1kDB8hp8PR7Jik5GTtqtjpp4pgdwQZm7Fj46MdawbQPpwOhx54eWuzYxZPv8JP1aAjYGoJAADYFkEGAADYFkEGAADYFj0yAM5aZ23gbs1Kw77ictWoR2JSs2N81RRLAy7sjCAD4Kx11gZuf6407Ha7/dYUSwMu7IypJQAAYFsEGQAAYFsEGQAAYFv0yACwvZYacI0xfqoE/kKDMk4iyACwvc7afNyZ0aCMk5haAgAAtkWQAQAAtkWQAQAAtkWQAQAAtkWzLzqUzrp0Pjqn1nyNQd3Ro4ru2rXF8wB2RZBBh8LdK+hMWvM1BgWTMvTAuvIWxwB2xdQSAACwLYIMAACwLYIMAACwLXpkABugiRkATs/SILN161Y99thjKi8vl8Ph0Lp16zRlyhTPcWOMFixYoBUrVsjlcmncuHFavny5+vXrZ13RgAVoYgaA07N0auno0aMaNmyYioqKTnv80Ucf1dNPP63nnntO27dvV9euXZWdna0TJ074uVIAABCILP1EZuLEiZo4ceJpjxljtGzZMj344IO6/vrrJUm/+93vlJiYqFdffVW33HKLP0sFAAABKGCbffft2yen06msrCzPvtjYWI0ZM0alpaVn/L36+nrV1tZ6PQAAQMcUsM2+TqdTkpSYmOi1PzEx0XPsdAoLC7Vw4cJ2rQ2Ab/izibml5zLG+KmSbwVaPYGkNSsWsxoxTgrYINNW8+bNU35+vme7trZWKSkpFlYE4Ez82cQcaA3TgVZPIGntisWAFMBTS0lJ36bxqqoqr/1VVVWeY6cTHh6umJgYrwcAAOiYAjbI9OnTR0lJSSopKfHsq62t1fbt25WZmWlhZQAAIFBYOrVUV1enzz//3LO9b98+VVRUqHv37kpNTVVeXp5+/etfq1+/furTp4/mz5+vXr16ea01AwAAOi9Lg8yOHTt01VVXebZP9rbMmDFDK1eu1H333aejR4/qzjvvlMvl0mWXXab169crIiLCqpIBwG9oCAZaZmmQufLKK5v9QwwKCtKiRYu0aNEiP1YFAIGBhmCgZQHbIwMAANASggwAALAtggwAALCtDrcgHgAAUutWCE5KTtauip3NjskYPkJOh+Ocz+MrgVaP1QgyAIAOqTUrBC+efkWL53E6HD45j68EWj1WY2oJAADYFkEGAADYFkEGAADYFj0yAIAW+apxNtC05nW5XDV+qgZtQZABALTIV42zgaY1r6tgUoafqkFbMLUEAABsiyADAABsiyADAABsiyADAABsi2ZfAPg/Sx9/1OoSzlpLNRtjWjyHMabF87iNu8UxLperxefqqDrqXV12QJABgP+Tf+PY5o+XvOSnSlrPFzUbmVadp6UxBZt+1+JzdVQd9a4uO2BqCQAA2BZBBgAA2BZBBgAA2BY9MgCADqk1TcytaYb2FRqC2wdBBgDQIbW2idlfaAhuH0wtAQAA2yLIAAAA2yLIAAAA26JHBgDgE26jFptZXa6aFs8TaE26gVZPa/iqsXjU8Aw5HI5mxyQnJ6u8YtdZ1+grBBkAgI+YFptZCyZltOIsgdWkG2j1tIavGosdDocOrv55s2N63fLEWdXma0wtAQAA2yLIAAAA2yLIAAAA26JHBgDgN61pnG0tu52nNQ3BrWnSbU3DtK985apTj2lLmh3jOlrvp2pOjyADAPAbXzbOdsTztKZJtzUN077idrv1wG+ebHZMwb/f5adqTo+pJQAAYFsEGQAAYFsEGQAAYFv0yAAA4CeBtEJwaxqLA2zB4tMiyAAA4CeBtEJwaxqL8ycO9lM1bcfUEgAAsC2CDAAAsC2CDAAAsC2CDAAAsC2afQGgg/PVEvyB9lwdVUvvodu4A+ruJ6vZIsgUFRXpsccek9Pp1LBhw/TMM8/okksusbosALAFf94pE0h35dhVa95D3ud/Cvippf/6r/9Sfn6+FixYoA8++EDDhg1Tdna2qqurrS4NAABYLOCDzNKlS3XHHXfotttu06BBg/Tcc88pKipKL774otWlAQAAiwX01FJDQ4PKy8s1b948z77g4GBlZWWptLT0tL9TX1+v+vp/fqV4Tc23X3deW1vr8/qMMTpx/HhLg3wzxpfnYgxjGMMYxnT8MUfrWhjS8pjWPJcxpl3+jT15zhb7fUwA+/LLL40k89e//tVr/7333msuueSS0/7OggULjCQePHjw4MGDRwd4HDhwoNmsENCfyLTFvHnzlJ+f79l2u906fPiw4uPjFRQU1OLv19bWKiUlRQcOHFBMTEx7lopzxLWyF66XfXCt7KWjXi9jjI4cOaJevXo1Oy6gg0xCQoJCQkJUVVXltb+qqkpJSaf/oqvw8HCFh4d77YuLizvr546JielQ/4PoyLhW9sL1sg+ulb10xOsVGxvb4piAbvYNCwvTqFGjVFJS4tnndrtVUlKizMxMCysDAACBIKA/kZGk/Px8zZgxQ6NHj9Yll1yiZcuW6ejRo7rtttusLg0AAFgs4IPMzTffrK+++kq/+tWv5HQ6NXz4cK1fv16JiYnt8nzh4eFasGDBKdNTCDxcK3vhetkH18peOvv1CjKmE61jDAAAOpSA7pEBAABoDkEGAADYFkEGAADYFkEGAADYFkHmO4qKipSWlqaIiAiNGTNG77//vtUldXiFhYW6+OKLdd5556lnz56aMmWK9u7d6zXmxIkTys3NVXx8vKKjozVt2rRTFkmsrKzUtddeq6ioKPXs2VP33nuvvvnmG68xmzdv1siRIxUeHq6LLrpIK1eubO+X16EtWbJEQUFBysvL8+zjWgWWL7/8UtOnT1d8fLwiIyOVkZGhHTt2eI4bY/SrX/1KycnJioyMVFZWlj777DOvcxw+fFg5OTmKiYlRXFycZs2apbo67+/n+eijj3T55ZcrIiJCKSkpevTRR/3y+jqKpqYmzZ8/X3369FFkZKT69u2rhx9+2Os7hrhWzfDBVyJ1CKtXrzZhYWHmxRdfNB9//LG54447TFxcnKmqqrK6tA4tOzvbFBcXm927d5uKigozadIkk5qaaurq6jxj7rrrLpOSkmJKSkrMjh07zKWXXmrGjh3rOf7NN9+YIUOGmKysLLNz507z5ptvmoSEBDNv3jzPmC+++MJERUWZ/Px888knn5hnnnnGhISEmPXr1/v19XYU77//vklLSzNDhw4199xzj2c/1ypwHD582PTu3dvMnDnTbN++3XzxxRdmw4YN5vPPP/eMWbJkiYmNjTWvvvqq+fDDD833v/9906dPH3P8+HHPmAkTJphhw4aZbdu2mXfffddcdNFF5tZbb/Ucr6mpMYmJiSYnJ8fs3r3b/P73vzeRkZHmt7/9rV9fr5098sgjJj4+3rzxxhtm3759Zs2aNSY6Oto89dRTnjFcqzMjyPyfSy65xOTm5nq2m5qaTK9evUxhYaGFVXU+1dXVRpLZsmWLMcYYl8tlQkNDzZo1azxj9uzZYySZ0tJSY4wxb775pgkODjZOp9MzZvny5SYmJsbU19cbY4y57777zODBg72e6+abbzbZ2dnt/ZI6nCNHjph+/fqZjRs3mn/7t3/zBBmuVWC5//77zWWXXXbG42632yQlJZnHHnvMs8/lcpnw8HDz+9//3hhjzCeffGIkmbKyMs+Yt956ywQFBZkvv/zSGGPMs88+a7p16+a5fiefe8CAAb5+SR3Wtddea26//XavfVOnTjU5OTnGGK5VS5haktTQ0KDy8nJlZWV59gUHBysrK0ulpaUWVtb51NTUSJK6d+8uSSovL1djY6PXtRk4cKBSU1M916a0tFQZGRleiyRmZ2ertrZWH3/8sWfMd89xcgzX9+zl5ubq2muvPeX95FoFltdee02jR4/WTTfdpJ49e2rEiBFasWKF5/i+ffvkdDq93uvY2FiNGTPG63rFxcVp9OjRnjFZWVkKDg7W9u3bPWOuuOIKhYWFecZkZ2dr7969+vrrr9v7ZXYIY8eOVUlJif72t79Jkj788EO99957mjhxoiSuVUsCfmVffzh06JCamppOWS04MTFRn376qUVVdT5ut1t5eXkaN26chgwZIklyOp0KCws75Ys/ExMT5XQ6PWNOd+1OHmtuTG1trY4fP67IyMj2eEkdzurVq/XBBx+orKzslGNcq8DyxRdfaPny5crPz9cDDzygsrIy/exnP1NYWJhmzJjheb9P915/91r07NnT63iXLl3UvXt3rzF9+vQ55Rwnj3Xr1q1dXl9H8otf/EK1tbUaOHCgQkJC1NTUpEceeUQ5OTmSxLVqAUEGASM3N1e7d+/We++9Z3UpOI0DBw7onnvu0caNGxUREWF1OWiB2+3W6NGjtXjxYknSiBEjtHv3bj333HOaMWOGxdXhu/7whz/olVde0apVqzR48GBVVFQoLy9PvXr14lq1AlNLkhISEhQSEnLK3RVVVVVKSkqyqKrOZfbs2XrjjTf0zjvv6IILLvDsT0pKUkNDg1wul9f4716bpKSk0167k8eaGxMTE8P/w2+l8vJyVVdXa+TIkerSpYu6dOmiLVu26Omnn1aXLl2UmJjItQogycnJGjRokNe+9PR0VVZWSvrn+93cf/eSkpJUXV3tdfybb77R4cOHz+qaonn33nuvfvGLX+iWW25RRkaGfvSjH2nu3LkqLCyUxLVqCUFGUlhYmEaNGqWSkhLPPrfbrZKSEmVmZlpYWcdnjNHs2bO1bt06bdq06ZSPPUeNGqXQ0FCva7N3715VVlZ6rk1mZqZ27drl9Ue8ceNGxcTEeP5DnpmZ6XWOk2O4vq139dVXa9euXaqoqPA8Ro8erZycHM/PXKvAMW7cuFOWMvjb3/6m3r17S5L69OmjpKQkr/e6trZW27dv97peLpdL5eXlnjGbNm2S2+3WmDFjPGO2bt2qxsZGz5iNGzdqwIABtp2q8Ldjx44pONj7n+OQkBC53W5JXKsWWd1tHChWr15twsPDzcqVK80nn3xi7rzzThMXF+d1dwV87+677zaxsbFm8+bNxuFweB7Hjh3zjLnrrrtMamqq2bRpk9mxY4fJzMw0mZmZnuMnb+m95pprTEVFhVm/fr3p0aPHaW/pvffee82ePXtMUVERt/T6wHfvWjKGaxVI3n//fdOlSxfzyCOPmM8++8y88sorJioqyrz88sueMUuWLDFxcXHmT3/6k/noo4/M9ddff9pbekeMGGG2b99u3nvvPdOvXz+vW3pdLpdJTEw0P/rRj8zu3bvN6tWrTVRUlO1v6fWnGTNmmPPPP99z+/XatWtNQkKCue+++zxjuFZnRpD5jmeeecakpqaasLAwc8kll5ht27ZZXVKHJ+m0j+LiYs+Y48ePm3//93833bp1M1FRUeaGG24wDofD6zz79+83EydONJGRkSYhIcH8/Oc/N42NjV5j3nnnHTN8+HATFhZmLrzwQq/nQNv8a5DhWgWW119/3QwZMsSEh4ebgQMHmueff97ruNvtNvPnzzeJiYkmPDzcXH311Wbv3r1eY/7xj3+YW2+91URHR5uYmBhz2223mSNHjniN+fDDD81ll11mwsPDzfnnn2+WLFnS7q+tI6mtrTX33HOPSU1NNREREebCCy80v/zlL71uk+ZanVmQMd9ZOhAAAMBG6JEBAAC2RZABAAC2RZABAAC2RZABAAC2RZABAAC2RZABAAC2RZABAAC2RZABAAC2RZABELCuvPJK5eXlWV0GgABGkAEAALZFkAEAALZFkAFgC/X19SooKND555+vrl27asyYMdq8ebPn+MqVKxUXF6cNGzYoPT1d0dHRmjBhghwOh3VFA2h3BBkAtjB79myVlpZq9erV+uijj3TTTTdpwoQJ+uyzzzxjjh07pscff1z/+Z//qa1bt6qyslIFBQUWVg2gvRFkAAS8yspKFRcXa82aNbr88svVt29fFRQU6LLLLlNxcbFnXGNjo5577jmNHj1aI0eO1OzZs1VSUmJh5QDaWxerCwCAluzatUtNTU3q37+/1/76+nrFx8d7tqOiotS3b1/PdnJysqqrq/1WJwD/I8gACHh1dXUKCQlReXm5QkJCvI5FR0d7fg4NDfU6FhQUJGOMX2oEYA2CDICAN2LECDU1Nam6ulqXX3651eUACCD0yAAIeP3791dOTo5+/OMfa+3atdq3b5/ef/99FRYW6s9//rPV5QGwEEEGgC0UFxfrxz/+sX7+859rwIABmjJlisrKypSammp1aQAsFGSYQAYAADbFJzIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2/j/CZKmzV7b6LwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "lengths = [{\"split\": \"train\", \"len\": len(tokenizer.encode(x[\"text\"]))} for x in train_dataset]\n",
    "lengths += [{\"split\": \"val\", \"len\": len(tokenizer.encode(x[\"text\"]))} for x in val_dataset]\n",
    "df = pd.DataFrame(lengths)\n",
    "sns.histplot(df, x=\"len\", hue=\"split\", bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Device map\n",
    "device_map = 'auto'  # for PP and running with `python test_sft.py`\n",
    "# Load the model\n",
    "model_name = \"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "# model_kwargs = dict(\n",
    "#     attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "#     torch_dtype=\"auto\",\n",
    "#     use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "#     device_map=device_map,\n",
    "# )\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# PEFT config\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 32  # 64\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    modules_to_save=[\"embed_tokens\", \"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n",
    ")\n",
    "# Args\n",
    "max_seq_length = 8142\n",
    "output_dir = \"./outputs/training\"\n",
    "per_device_train_batch_size = 1  # reduced batch size to avoid OOM\n",
    "gradient_accumulation_steps = 8 #2\n",
    "optim = \"adamw_torch\"\n",
    "save_steps = 10\n",
    "logging_steps = 1\n",
    "learning_rate = 2e-4 #2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    do_eval=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    max_seq_length=max_seq_length,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,  # gradient checkpointing\n",
    "    report_to=\"wandb\",\n",
    "    dataset_text_field=\"text\",\n",
    "    seed=42,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    run_name=\"sft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1325/1325 [00:06<00:00, 211.01 examples/s]\n",
      "Map: 100%|██████████| 667/667 [00:02<00:00, 224.30 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='495' max='495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [495/495 1:24:14, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.085315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.075241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.074846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 16.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/plancraft/outputs/training/checkpoint-490\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEFICS2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", do_image_splitting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class PlancraftDialogueDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: str = \"data/oracle\",\n",
    "        mm=False,\n",
    "        split=\"train\",\n",
    "        max_window_size=30,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        file_path = f\"{dataset_dir}/{split}.jsonl\"\n",
    "        if mm:\n",
    "            file_path = f\"{dataset_dir}/{split}.mm.jsonl\"\n",
    "\n",
    "        print(\"Loading dialogue\")\n",
    "        data = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "\n",
    "        if mm:\n",
    "            print(\"Loading images\")\n",
    "            # load images\n",
    "            for example in data:\n",
    "                example[\"images\"] = []\n",
    "                example[\"message_idx_to_image_idx\"] = {}\n",
    "                i = 0\n",
    "                for message_idx, message in enumerate(example[\"messages\"]):\n",
    "                    for content in message[\"content\"]:\n",
    "                        if content[\"type\"] == \"image\":\n",
    "                            img_path = (\n",
    "                                f\"{dataset_dir}/{split}/{example['example_id']}_{i}.png\"\n",
    "                            )\n",
    "                            img = Image.open(img_path).convert(\"RGB\")\n",
    "                            example[\"images\"].append(img)\n",
    "                            example[\"message_idx_to_image_idx\"][message_idx] = i\n",
    "                            i += 1\n",
    "\n",
    "        self.dataset = data\n",
    "        self.max_window_size = max_window_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[dict, list]:\n",
    "        example = self.dataset[idx]\n",
    "        if len(example[\"messages\"]) > self.max_window_size:\n",
    "            # add system message\n",
    "            messages = [example[\"messages\"][0]]\n",
    "            # sample window\n",
    "            user_messages_idxs = list(\n",
    "                range(self.max_window_size, len(example[\"messages\"]), 2)\n",
    "            )\n",
    "            end = random.choice(user_messages_idxs)\n",
    "            start = end - self.max_window_size + 1\n",
    "            assert start != 0\n",
    "            # add window\n",
    "            messages = messages + example[\"messages\"][start : end + 1]\n",
    "            images = []\n",
    "            if \"images\" in example:\n",
    "                for message_idx in range(start, end):\n",
    "                    if message_idx in example[\"message_idx_to_image_idx\"]:\n",
    "                        image_idx = example[\"message_idx_to_image_idx\"][message_idx]\n",
    "                        images.append(example[\"images\"][image_idx])\n",
    "        else:\n",
    "            messages = example[\"messages\"]\n",
    "            images = example.get(\"images\", [])\n",
    "        return messages, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, Idefics2ForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\", do_image_splitting=False\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    ")\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dialogue\n",
      "Loading images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "TEMPLATES = {\n",
    "    \"idefics2\": {\n",
    "        \"assistant\": \"\\nAssistant:\",\n",
    "        \"user\": \"\\nUser:\",\n",
    "    },\n",
    "    \"llama3\": {\n",
    "        \"assistant\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"user\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def track_assistant_response(\n",
    "    batch,\n",
    "    tokenizer,\n",
    "    template_name: str = \"llama3\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Mask that returns 1 for tokens in the assistant response and 0 otherwise.\n",
    "    \"\"\"\n",
    "    assistant_template = TEMPLATES[template_name][\"assistant\"]\n",
    "    user_template = TEMPLATES[template_name][\"user\"]\n",
    "    start_seq = tokenizer.encode(\n",
    "        assistant_template,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )[0]\n",
    "    end_seq = tokenizer.encode(\n",
    "        user_template,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )[0]\n",
    "    encoded_label_ids = batch[\"labels\"]\n",
    "    mask = torch.zeros_like(encoded_label_ids)\n",
    "    for seq_idx, seq in enumerate(encoded_label_ids):\n",
    "        in_masked_response = False\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i + len(start_seq) < len(seq) and torch.all(\n",
    "                seq[i : i + len(start_seq)].eq(start_seq)\n",
    "            ):\n",
    "                in_masked_response = True\n",
    "                i += len(start_seq)\n",
    "                continue\n",
    "            if i + len(end_seq) < len(seq) and torch.all(\n",
    "                seq[i : i + len(end_seq)].eq(end_seq)\n",
    "            ):\n",
    "                in_masked_response = False\n",
    "                i += len(end_seq)\n",
    "                continue\n",
    "            if in_masked_response:\n",
    "                mask[seq_idx, i] = 1\n",
    "            else:\n",
    "                mask[seq_idx, i] = 0\n",
    "            i += 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_collate_fn(\n",
    "    tokenizer=None,\n",
    "    processor=None,\n",
    "    max_length=8142,\n",
    "    only_assistant=False,\n",
    "    template_name: str = \"llama3\",\n",
    "):\n",
    "    assert tokenizer or processor and not (tokenizer and processor)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        messages_batch = []\n",
    "        images_batch = []\n",
    "        for messages, images in batch:\n",
    "            if processor:\n",
    "                text = processor.apply_chat_template(\n",
    "                    messages, add_generation_prompt=False, tokenize=False\n",
    "                )\n",
    "                images_batch.append(images)\n",
    "            else:\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages, add_generation_prompt=False, tokenize=False\n",
    "                )\n",
    "            messages_batch.append(text)\n",
    "        if processor:\n",
    "            batch = processor(\n",
    "                text=messages_batch,\n",
    "                images=images_batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "            labels[labels == model.config.image_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        else:\n",
    "            batch = tokenizer(\n",
    "                messages_batch,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            batch[\"labels\"] = labels\n",
    "\n",
    "        # add mask for assistant response\n",
    "        if only_assistant:\n",
    "            if processor:\n",
    "                mask = track_assistant_response(\n",
    "                    batch, processor.tokenizer, template_name=template_name\n",
    "                )\n",
    "            else:\n",
    "                mask = track_assistant_response(\n",
    "                    batch, tokenizer, template_name=template_name\n",
    "                )\n",
    "            labels[mask == 0] = -100\n",
    "\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "dataset = PlancraftDialogueDataset(mm=True, max_window_size=30)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=get_collate_fn(\n",
    "        processor=processor, only_assistant=True, template_name=\"idefics2\"\n",
    "    ),\n",
    ")\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcuda_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:167\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:380\u001b[0m, in \u001b[0;36mAlignDevicesHook.post_forward\u001b[0;34m(self, module, output)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_same_device \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:186\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 186\u001b[0m         {\n\u001b[1;32m    187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:187\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    186\u001b[0m         {\n\u001b[0;32m--> 187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model(**cuda_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Idefics2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, labels = batch\n",
    "\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_attention_mask=pixel_attention_mask,\n",
    "            max_new_tokens=768,\n",
    "        )\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(\n",
    "            generated_ids[:, input_ids.size(1) :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            collate_fn=train_collate_fn,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            val_dataset,\n",
    "            collate_fn=eval_collate_fn,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', ':', ' move', ' from', ' slot', ' ', '27', ' to', ' slot', ' ', '1', ' with', ' quantity', ' ', '1', '<|eot_id|>', 'act', ':', ' move', ' from', ' slot', ' ', '10', ' to', ' slot', ' ', '2', ' with', ' quantity', ' ', '1', '<|eot_id|>', 'act', ':', ' move', ' from', ' slot', ' ', '10', ' to', ' slot', ' ', '3', ' with', ' quantity', ' ', '1', '<|eot_id|>', 'act', ':', ' move', ' from', ' slot', ' ', '0', ' to', ' slot', ' ', '11', ' with', ' quantity', ' ', '6', '<|eot_id|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(batch[\"labels\"][0][batch[\"labels\"][0] != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForVision2Seq\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "USE_LORA = True\n",
    "USE_QLORA = False\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Next we can add our LoRa adapters if required.\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "  lora_config = LoraConfig(\n",
    "          r=32,\n",
    "          lora_alpha=16,\n",
    "          lora_dropout=0.1,\n",
    "          target_modules=\".*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$\",\n",
    "          use_dora=False if USE_QLORA else True,\n",
    "          init_lora_weights=\"gaussian\",\n",
    "      )\n",
    "\n",
    "  model = prepare_model_for_kbit_training(model)\n",
    "  model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def train_collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images_example, question, answer = example\n",
    "\n",
    "        content = [{\"type\": \"image\"} for _ in range(len(images_example))]\n",
    "        content += [{\"type\": \"text\", \"text\": question}]\n",
    "        # Create inputs\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": answer},\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(prompt)\n",
    "        images.append(images_example)\n",
    "\n",
    "    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == model.config.image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_attention_mask = batch[\"pixel_attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, pixel_attention_mask, labels\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    # we feed the prompt to the model\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for example in examples:\n",
    "        images_example, question, answer = example\n",
    "\n",
    "        content = [{\"type\": \"image\"} for _ in range(len(images_example))]\n",
    "        content += [{\"type\": \"text\", \"text\": question}]\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        images.append(images_example)\n",
    "        texts.append(text.strip())\n",
    "        answers.append(answer)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_attention_mask = batch[\"pixel_attention_mask\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, pixel_attention_mask, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Idefics2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, labels = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                pixel_values=pixel_values,\n",
    "                                pixel_attention_mask=pixel_attention_mask,\n",
    "                                labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                       pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask,\n",
    "                                       max_new_tokens=MAX_LENGTH)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 10,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 2,\n",
    "          \"precision\": \"16-mixed\", # we'll use mixed precision\n",
    "          # \"seed\":2022, # can be used for reproducibility\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "}\n",
    "\n",
    "model_module = Idefics2ModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(FINETUNED_REPO_ID,\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(FINETUNED_REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(FINETUNED_REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=WANDB_PROJECT, name=WANDB_NAME)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        precision=config.get(\"precision\"),\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[PushToHubCallback(), early_stop_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
