{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plancraft.environments.env_real import RealPlancraft\n",
    "\n",
    "env = RealPlancraft(\n",
    "    inventory=[{\"type\": \"beetroot_soup\", \"quantity\": 1, \"slot\": 45}],\n",
    "    symbolic_action_space=True,\n",
    "    symbolic_observation_space=True,\n",
    "    resolution=[512, 512],\n",
    "    crop=True,\n",
    ")\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train_fast_rcnn import M1, M2, prepare_m2_batch, sample_environment\n",
    "\n",
    "# m1_path = \"m1.pth\"\n",
    "# m2_path = \"m2.pth\"\n",
    "# M1_model = M1(weights_path = m1_path)\n",
    "# M1_model = M1_model.cuda()\n",
    "M2_model = M2()\n",
    "M2_model = M2_model.cuda()\n",
    "print(\"Loaded model\")\n",
    "\n",
    "N = 1000\n",
    "lr = 0.001\n",
    "batch_size = 4\n",
    "save_every = 100\n",
    "count = 0\n",
    "# m1_optimizer = torch.optim.AdamW(M1_model.parameters(), lr=lr)\n",
    "m2_optimizer = torch.optim.AdamW(M2_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets, raw_images, inv in sample_environment(N=N, batch_size=batch_size):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# train M2\n",
    "for img, inventory in zip(raw_images, inv):\n",
    "    i = Image.fromarray(img.copy())\n",
    "    m2_batch = prepare_m2_batch(i, inventory)\n",
    "    m2_batch[\"bbox_tensors\"] = torch.stack(m2_batch[\"bbox_tensors\"]).cuda()\n",
    "    m2_batch[\"item_quantity\"] = m2_batch[\"item_quantity\"].unsqueeze(1).cuda()\n",
    "    m2_batch[\"item_locations\"] = m2_batch[\"item_locations\"].cuda()\n",
    "    m2_batch[\"item_types\"] = m2_batch[\"item_types\"].cuda()\n",
    "    break\n",
    "    # M2_model.train()\n",
    "    # M2_model(m2_batch)\n",
    "    # losses = sum(loss for loss in loss_dict.values())\n",
    "    # m2_optimizer.zero_grad()\n",
    "    # losses.backward()\n",
    "    # m2_optimizer.step()\n",
    "\n",
    "# N = len(m2_batch[\"item_types\"])\n",
    "# encoded_img = M2_model.img_encoder(m2_batch[\"bbox_tensors\"]).reshape(N, -1)\n",
    "\n",
    "# M2_model.quantity_encoder(m2_batch[\"item_quantity\"].unsqueeze(1))\n",
    "\n",
    "# m2_batch[\"item_quantity\"].shape\n",
    "# \n",
    "# m2_batch[\"item_quantity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_paths = glob.glob(\"data/oracle/train/imgs/*.png\")\n",
    "# load images\n",
    "image_arrays = [Image.open(p) for p in image_paths]\n",
    "# to tensor\n",
    "image_tensors = [np.array(i)/255 for i in image_arrays]\n",
    "\n",
    "# stack\n",
    "image_tensors = np.stack(image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensors_mean = image_tensors.mean(axis=(0, 1, 2))\n",
    "image_tensors_std = image_tensors.std(axis=(0, 1, 2))\n",
    "print(image_tensors_mean, image_tensors_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minerl.herobraine.hero.mc import ALL_ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.roi_heads import (\n",
    "    RoIHeads,\n",
    "    fastrcnn_loss,\n",
    "    maskrcnn_loss,\n",
    "    maskrcnn_inference,\n",
    "    keypointrcnn_loss,\n",
    "    keypointrcnn_inference,\n",
    ")\n",
    "\n",
    "\n",
    "class OverrideRoIHeads(RoIHeads):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.quantity_predictor = nn.Sequential(nn.Linear(1024, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features,  # type: Dict[str, Tensor]\n",
    "        proposals,  # type: List[Tensor]\n",
    "        image_shapes,  # type: List[Tuple[int, int]]\n",
    "        targets=None,  # type: Optional[List[Dict[str, Tensor]]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (List[Tensor])\n",
    "            proposals (List[Tensor[N, 4]])\n",
    "            image_shapes (List[Tuple[H, W]])\n",
    "            targets (List[Dict])\n",
    "        \"\"\"\n",
    "        if targets is not None:\n",
    "            for t in targets:\n",
    "                # TODO: https://github.com/pytorch/pytorch/issues/26731\n",
    "                floating_point_types = (torch.float, torch.double, torch.half)\n",
    "                if not t[\"boxes\"].dtype in floating_point_types:\n",
    "                    raise TypeError(\n",
    "                        f\"target boxes must of float type, instead got {t['boxes'].dtype}\"\n",
    "                    )\n",
    "                if not t[\"labels\"].dtype == torch.int64:\n",
    "                    raise TypeError(\n",
    "                        f\"target labels must of int64 type, instead got {t['labels'].dtype}\"\n",
    "                    )\n",
    "                if self.has_keypoint():\n",
    "                    if not t[\"keypoints\"].dtype == torch.float32:\n",
    "                        raise TypeError(\n",
    "                            f\"target keypoints must of float type, instead got {t['keypoints'].dtype}\"\n",
    "                        )\n",
    "\n",
    "        if self.training:\n",
    "            proposals, matched_idxs, labels, regression_targets = (\n",
    "                self.select_training_samples(proposals, targets)\n",
    "            )\n",
    "        else:\n",
    "            labels = None\n",
    "            regression_targets = None\n",
    "            matched_idxs = None\n",
    "\n",
    "        box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
    "        box_features = self.box_head(box_features)\n",
    "        class_logits, box_regression = self.box_predictor(box_features)\n",
    "        quantity_logits = self.quantity_predictor(box_features)\n",
    "\n",
    "        result: List[Dict[str, torch.Tensor]] = []\n",
    "        losses = {}\n",
    "        if self.training:\n",
    "            if labels is None:\n",
    "                raise ValueError(\"labels cannot be None\")\n",
    "            if regression_targets is None:\n",
    "                raise ValueError(\"regression_targets cannot be None\")\n",
    "            loss_classifier, loss_box_reg = fastrcnn_loss(\n",
    "                class_logits, box_regression, labels, regression_targets\n",
    "            )\n",
    "\n",
    "\n",
    "            losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "        else:\n",
    "            boxes, scores, labels = self.postprocess_detections(\n",
    "                class_logits, box_regression, proposals, image_shapes\n",
    "            )\n",
    "            num_images = len(boxes)\n",
    "            for i in range(num_images):\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"boxes\": boxes[i],\n",
    "                        \"labels\": labels[i],\n",
    "                        \"scores\": scores[i],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if self.has_mask():\n",
    "            mask_proposals = [p[\"boxes\"] for p in result]\n",
    "            if self.training:\n",
    "                if matched_idxs is None:\n",
    "                    raise ValueError(\"if in training, matched_idxs should not be None\")\n",
    "\n",
    "                # during training, only focus on positive boxes\n",
    "                num_images = len(proposals)\n",
    "                mask_proposals = []\n",
    "                pos_matched_idxs = []\n",
    "                for img_id in range(num_images):\n",
    "                    pos = torch.where(labels[img_id] > 0)[0]\n",
    "                    mask_proposals.append(proposals[img_id][pos])\n",
    "                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n",
    "            else:\n",
    "                pos_matched_idxs = None\n",
    "\n",
    "            if self.mask_roi_pool is not None:\n",
    "                mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)\n",
    "                mask_features = self.mask_head(mask_features)\n",
    "                mask_logits = self.mask_predictor(mask_features)\n",
    "            else:\n",
    "                raise Exception(\"Expected mask_roi_pool to be not None\")\n",
    "\n",
    "            loss_mask = {}\n",
    "            if self.training:\n",
    "                if targets is None or pos_matched_idxs is None or mask_logits is None:\n",
    "                    raise ValueError(\n",
    "                        \"targets, pos_matched_idxs, mask_logits cannot be None when training\"\n",
    "                    )\n",
    "\n",
    "                gt_masks = [t[\"masks\"] for t in targets]\n",
    "                gt_labels = [t[\"labels\"] for t in targets]\n",
    "                rcnn_loss_mask = maskrcnn_loss(\n",
    "                    mask_logits, mask_proposals, gt_masks, gt_labels, pos_matched_idxs\n",
    "                )\n",
    "                loss_mask = {\"loss_mask\": rcnn_loss_mask}\n",
    "            else:\n",
    "                labels = [r[\"labels\"] for r in result]\n",
    "                masks_probs = maskrcnn_inference(mask_logits, labels)\n",
    "                for mask_prob, r in zip(masks_probs, result):\n",
    "                    r[\"masks\"] = mask_prob\n",
    "\n",
    "            losses.update(loss_mask)\n",
    "\n",
    "        # keep none checks in if conditional so torchscript will conditionally\n",
    "        # compile each branch\n",
    "        if (\n",
    "            self.keypoint_roi_pool is not None\n",
    "            and self.keypoint_head is not None\n",
    "            and self.keypoint_predictor is not None\n",
    "        ):\n",
    "            keypoint_proposals = [p[\"boxes\"] for p in result]\n",
    "            if self.training:\n",
    "                # during training, only focus on positive boxes\n",
    "                num_images = len(proposals)\n",
    "                keypoint_proposals = []\n",
    "                pos_matched_idxs = []\n",
    "                if matched_idxs is None:\n",
    "                    raise ValueError(\"if in trainning, matched_idxs should not be None\")\n",
    "\n",
    "                for img_id in range(num_images):\n",
    "                    pos = torch.where(labels[img_id] > 0)[0]\n",
    "                    keypoint_proposals.append(proposals[img_id][pos])\n",
    "                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n",
    "            else:\n",
    "                pos_matched_idxs = None\n",
    "\n",
    "            keypoint_features = self.keypoint_roi_pool(\n",
    "                features, keypoint_proposals, image_shapes\n",
    "            )\n",
    "            keypoint_features = self.keypoint_head(keypoint_features)\n",
    "            keypoint_logits = self.keypoint_predictor(keypoint_features)\n",
    "\n",
    "            loss_keypoint = {}\n",
    "            if self.training:\n",
    "                if targets is None or pos_matched_idxs is None:\n",
    "                    raise ValueError(\n",
    "                        \"both targets and pos_matched_idxs should not be None when in training mode\"\n",
    "                    )\n",
    "\n",
    "                gt_keypoints = [t[\"keypoints\"] for t in targets]\n",
    "                rcnn_loss_keypoint = keypointrcnn_loss(\n",
    "                    keypoint_logits, keypoint_proposals, gt_keypoints, pos_matched_idxs\n",
    "                )\n",
    "                loss_keypoint = {\"loss_keypoint\": rcnn_loss_keypoint}\n",
    "            else:\n",
    "                if keypoint_logits is None or keypoint_proposals is None:\n",
    "                    raise ValueError(\n",
    "                        \"both keypoint_logits and keypoint_proposals should not be None when not in training mode\"\n",
    "                    )\n",
    "\n",
    "                keypoints_probs, kp_scores = keypointrcnn_inference(\n",
    "                    keypoint_logits, keypoint_proposals\n",
    "                )\n",
    "                for keypoint_prob, kps, r in zip(keypoints_probs, kp_scores, result):\n",
    "                    r[\"keypoints\"] = keypoint_prob\n",
    "                    r[\"keypoints_scores\"] = kps\n",
    "            losses.update(loss_keypoint)\n",
    "\n",
    "        return result, losses\n",
    "    \n",
    "\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "model.roi_heads = OverrideRoIHeads(model.roi_heads.box_roi_pool, model.roi_heads.box_head, model.roi_heads.box_predictor, model.roi_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2\n",
    "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
    "\n",
    "class M2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M2, self).__init__()\n",
    "        self.box_feats = None\n",
    "        self.proposals = None\n",
    "        self.image_shapes = None\n",
    "\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            pretrained=False,\n",
    "            image_mean=[0.63, 0.63, 0.63],\n",
    "            image_std=[0.21, 0.21, 0.21],\n",
    "            min_size=64,\n",
    "            max_size=64,\n",
    "            num_classes=len(ALL_ITEMS),\n",
    "        )\n",
    "\n",
    "        # need to save the intermediate features\n",
    "        # self.model.roi_heads.box_predictor.cls_score.register_pre_forward_hook(\n",
    "        self.model.roi_heads.box_roi_pool.register_forward_pre_hook(\n",
    "            self.save_prop_img_shape\n",
    "        )\n",
    "        self.model.roi_heads.box_predictor.register_forward_hook(self.save_box_feats)\n",
    "        # self.model.roi_heads.box_predictor.bbox_pred.register_forward_pre_hook(\n",
    "        #     self.save_feats\n",
    "        # )\n",
    "        self.quantity_prediction = nn.Sequential(nn.Linear(1024, 1), nn.Sigmoid())\n",
    "        \n",
    "    def save_prop_img_shape(self, _module, input):\n",
    "        # Save the input features to be used later\n",
    "        self.proposals = input[1]\n",
    "        print(\"self.proposals\", self.proposals[0].shape)\n",
    "        self.image_shapes = input[2]\n",
    "        # print(\"self.proposals\", self.proposals.shape)\n",
    "        # print(\"self.image_shapes\", self.image_shapes)\n",
    "    \n",
    "    def save_box_feats(self, _module, input, output):\n",
    "        self.box_feats = input[0]\n",
    "        self.class_logits = output[0]\n",
    "\n",
    "    def forward(self, x, targets=None, quantity_targets=None):\n",
    "        if self.training:\n",
    "            # assert targets is not None and quantity_targets is not None\n",
    "            print(self.box_feats)\n",
    "            loss_dict = self.model(x, targets)\n",
    "            print(\"box_feats\", self.box_feats.shape)\n",
    "            print(\"class_logits\", self.class_logits.shape)\n",
    "            print(\"targets\", targets[\"labels\"])\n",
    "\n",
    "\n",
    "            # custom_classification_loss = F.cross_entropy(\n",
    "            #     self.class_logits, targets[\"labels\"]\n",
    "            # )\n",
    "            # print(custom_classification_loss)\n",
    "\n",
    "            # loss_classifier \n",
    "            # Compute the loss for the quantity classifier\n",
    "            # predicted_quantities = self.quantity_prediction(self.feats)\n",
    "            # loss_dict[\"quantity_loss\"] = F.mse_loss(predicted_quantities, quantity_targets)\n",
    "            return loss_dict\n",
    "        else:\n",
    "            print(self.box_feats)\n",
    "            preds = self.model(x)\n",
    "            print(self.box_feats.shape)\n",
    "            # preds[\"predicted_quantities\"] = self.quantity_prediction(\n",
    "            #     self.feats\n",
    "            # )\n",
    "            return preds\n",
    "\n",
    "m = M2()\n",
    "m.train()\n",
    "m.cuda()\n",
    "\n",
    "fake_inputs = [torch.rand(3, 64, 64).cuda(), torch.rand(3, 64, 64).cuda()]\n",
    "fake_targets = [\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[10, 10, 20, 20], [30, 30, 40, 40]]).cuda(),\n",
    "        \"labels\": torch.tensor([1, 2]).cuda(),\n",
    "    },\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[10, 10, 20, 20], [30, 30, 40, 40]]).cuda(),\n",
    "        \"labels\": torch.tensor([1, 2]).cuda(),\n",
    "    },\n",
    "]\n",
    "\n",
    "m(fake_inputs, targets=fake_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.roi_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import (\n",
    "    FastRCNNPredictor,\n",
    "    FasterRCNN,\n",
    "    _default_anchorgen,\n",
    "    RPNHead,\n",
    "    FastRCNNConvFCHead,\n",
    ")\n",
    "from torchvision.models.resnet import resnet50\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DualFastRCNNPredictor(nn.Module):\n",
    "    def __init__(self, in_channels=1024):\n",
    "        super(DualFastRCNNPredictor, self).__init__()\n",
    "        self.quantity_predictor = FastRCNNPredictor(in_channels, 65)\n",
    "        self.label_predictor = FastRCNNPredictor(in_channels, 900)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.quantity_predictor(x), self.label_predictor(x))\n",
    "\n",
    "def fasterrcnn_resnet50_fpn_v2(\n",
    "    **kwargs\n",
    ") -> FasterRCNN:\n",
    "    \"\"\"\n",
    "    Constructs an improved Faster R-CNN model with a ResNet-50-FPN backbone from `Benchmarking Detection\n",
    "    Transfer Learning with Vision Transformers <https://arxiv.org/abs/2111.11429>`__ paper.\n",
    "    \"\"\"\n",
    "    backbone = resnet50(weights=None, progress=False)\n",
    "    rpn_anchor_generator = _default_anchorgen()\n",
    "    rpn_head = RPNHead(\n",
    "        backbone.out_channels,\n",
    "        rpn_anchor_generator.num_anchors_per_location()[0],\n",
    "        conv_depth=2,\n",
    "    )\n",
    "    box_head = FastRCNNConvFCHead(\n",
    "        (backbone.out_channels, 7, 7),\n",
    "        [256, 256, 256, 256],\n",
    "        [1024],\n",
    "        norm_layer=nn.BatchNorm2d,\n",
    "    )\n",
    "    dual_head = DualFastRCNNPredictor()\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=None,\n",
    "        rpn_anchor_generator=rpn_anchor_generator,\n",
    "        rpn_head=rpn_head,\n",
    "        box_head=box_head,\n",
    "        box_predictor=dual_head,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# # mean=[0.508, 0.492, 0.476], std=[0.241, 0.244, 0.255]\n",
    "# m = FasterRCNN(\n",
    "#     num_classes=None,\n",
    "#     pretrained=False,\n",
    "#     \n",
    "#     box_predictor=dual_head,\n",
    "# )\n",
    "# m.transform\n",
    "m = fasterrcnn_resnet50_fpn_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train_fast_rcnn import M2\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
    "import wandb\n",
    "from PIL import Image\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "# model = M2()\n",
    "# model = model.cuda()\n",
    "\n",
    "img_encoder = mobilenet_v3_small(\n",
    "    pretrained=True,\n",
    "    image_mean=[0.508, 0.492, 0.476],\n",
    "    image_std=[0.241, 0.244, 0.255],\n",
    "    min_size=64,\n",
    "    max_size=64,\n",
    ")\n",
    "img_encoder.classifier = nn.Linear(576, len(ALL_ITEMS))\n",
    "img_encoder.cuda()\n",
    "\n",
    "# images are in data/bboxes/*.png\n",
    "class BboxDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = v2.Compose(\n",
    "            [\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                # v2.Normalize(mean=[0.508, 0.492, 0.476], std=[0.241, 0.244, 0.255]),\n",
    "            ]\n",
    "        )\n",
    "        self.paths = glob.glob(f\"{img_dir}/*.png\")\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        target = \"_\".join(img_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1:])\n",
    "        return img, ALL_ITEMS.index(target)\n",
    "\n",
    "\n",
    "dataset = BboxDataset(\"data/bboxes\")\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "m2_optimizer = torch.optim.AdamW(img_encoder.parameters(), lr=0.001)\n",
    "\n",
    "# wandb.init(project=\"plancraft-img-encoder\", entity=\"itl\")  # , mode=\"disabled\")\n",
    "\n",
    "i = 0\n",
    "for images, labels in dataloader:\n",
    "    # for i in range(1000):\n",
    "    encoded_image = img_encoder(images.cuda())\n",
    "    loss = nn.CrossEntropyLoss()(nn.Softmax(dim=1)(encoded_image), labels.cuda())\n",
    "    # wandb.log({\"loss\": loss})\n",
    "    if i%100 == 0:\n",
    "        print(loss)\n",
    "        # torch.save(img_encoder.state_dict(), \"mobilenet.pth\")\n",
    "\n",
    "    m2_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    m2_optimizer.step()\n",
    "        # model(batch.cuda())\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = img_encoder(images.cuda()).softmax(dim=1).argmax(dim=1)\n",
    "(preds == labels.cuda()).sum() / len(labels)\n",
    "# for pred, label in zip(preds, labels):\n",
    "#     print(ALL_ITEMS[pred], ALL_ITEMS[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder.classifier = nn.Identity()\n",
    "torch.save(img_encoder.state_dict(), \"mobilenet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_image[0].softmax()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob.glob(f\"data/bboxes/*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_paths = glob(\"data/bboxes/*.png\")\n",
    "img_arrs = []\n",
    "for image_path in tqdm(image_paths):\n",
    "    img = Image.open(image_path)\n",
    "    img_arr = np.array(img)\n",
    "    img_arrs.append(img_arr)\n",
    "\n",
    "# get mean and std\n",
    "img_arrs = np.array(img_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arrs = img_arrs / 255\n",
    "# (72568, 64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(img_arrs, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(img_arrs, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from train_fast_rcnn import slot_to_bbox, sample_starting_inv\n",
    "\n",
    "starting_inv = sample_starting_inv()\n",
    "env.fast_reset(new_inventory=starting_inv)\n",
    "obs, _, _, _ = env.step(env.action_space.no_op())\n",
    "\n",
    "img = Image.fromarray(obs[\"pov\"])\n",
    "for item in obs[\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    bbox = slot_to_bbox(item[\"index\"])\n",
    "    draw.rectangle(bbox, outline=\"red\")\n",
    "\n",
    "img_tensor = transforms.ToTensor()(obs[\"pov\"].copy()).cuda()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(img_tensor.unsqueeze(0)) \n",
    "\n",
    "# for box_idx in range(len(predictions[0][\"boxes\"])):\n",
    "#     if predictions[0][\"scores\"][box_idx] < 0.3:\n",
    "#         continue\n",
    "#     box = predictions[0][\"boxes\"][box_idx]\n",
    "#     label = predictions[0][\"labels\"][box_idx]\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"blue\")\n",
    "#     # add text label in middle of box\n",
    "#     draw.text(\n",
    "#         (box[0].item() + (box[2].item() - box[0].item()) / 2, box[1].item() + (box[3].item() - box[1].item()) / 2),\n",
    "#         str(label.item()),\n",
    "#         fill=\"green\",\n",
    "#     )\n",
    "# # show\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from train_fast_rcnn import slot_to_bbox, sample_starting_inv\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "img = Image.fromarray(obs[\"pov\"])\n",
    "for item in obs[\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    bbox = slot_to_bbox(item[\"index\"])\n",
    "    draw.rectangle(bbox, outline=\"red\")\n",
    "\n",
    "# img_tensor = transforms.ToTensor()(obs[\"pov\"].copy()).cuda()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(img_tensor.unsqueeze(0)) \n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "# for box_idx in range(len(predictions[0][\"boxes\"])):\n",
    "#     if predictions[0][\"scores\"][box_idx] < 0.3:\n",
    "#         continue\n",
    "#     box = predictions[0][\"boxes\"][box_idx]\n",
    "#     label = predictions[0][\"labels\"][box_idx]\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"blue\")\n",
    "#     # add text label in middle of box\n",
    "#     draw.text(\n",
    "#         (box[0].item() + (box[2].item() - box[0].item()) / 2, box[1].item() + (box[3].item() - box[1].item()) / 2),\n",
    "#         str(label.item()+1),\n",
    "#         fill=\"red\",\n",
    "#     )\n",
    "\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[\"inventory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "model.eval()\n",
    "\n",
    "idx = 0\n",
    "img_tensor = transforms.ToTensor()(data[idx][\"pov\"].copy()).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_tensor.unsqueeze(0))  \n",
    "\n",
    "img = Image.fromarray(data[idx][\"pov\"])\n",
    "for item in data[-1][\"inventory\"]:\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(item[\"bbox\"], outline=\"red\")\n",
    "\n",
    "for box_idx in range(len(predictions[0][\"boxes\"])):\n",
    "    if predictions[0][\"scores\"][box_idx] < 0.5:\n",
    "        continue\n",
    "    box = predictions[0][\"boxes\"][box_idx]\n",
    "    label = predictions[0][\"labels\"][box_idx]\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle([box[0].item(), box[1].item(), box[2].item(), box[3].item()], outline=\"blue\")\n",
    "\n",
    "    # add text label in middle of box\n",
    "    draw.text(\n",
    "        (box[0].item() + (box[2].item() - box[0].item()) / 2, box[1].item() + (box[3].item() - box[1].item()) / 2),\n",
    "        str(label.item()+1),\n",
    "        fill=\"red\",\n",
    "    )\n",
    "\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp():\n",
    "    n = 0\n",
    "    while n < 10:\n",
    "        if n == 5:\n",
    "            return\n",
    "        yield n\n",
    "        n += 1\n",
    "\n",
    "for i in tmp():\n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class InventoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(self.data[idx][\"pov\"].copy())\n",
    "        target = {\n",
    "            \"labels\": [],\n",
    "            \"boxes\": []\n",
    "        }\n",
    "\n",
    "        for i in self.data[idx][\"inventory\"]:\n",
    "            target[\"labels\"].append(i[\"quantity\"])\n",
    "            target[\"boxes\"].append(i[\"bbox\"])\n",
    "        \n",
    "        target[\"labels\"] = torch.tensor(target[\"labels\"], dtype=torch.int64)-1\n",
    "        target[\"boxes\"] = torch.tensor(target[\"boxes\"], dtype=torch.int64)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "# N = len(data)\n",
    "train_dataset = InventoryDataset(data)\n",
    "# val_dataset = InventoryDataset(data[int(N*0.9):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.require(\"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "lr = 0.005\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_stack = []\n",
    "    target_stack = []\n",
    "    for img, target in batch:\n",
    "        image_stack.append(img)\n",
    "        target_stack.append(target)\n",
    "    return torch.stack(image_stack), target_stack\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "wandb.init(project=\"plancraft-img-encoder\", entity=\"itl\")\n",
    "\n",
    "batch_num = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.cuda()\n",
    "        for i in range(len(labels)):\n",
    "            labels[i][\"boxes\"] = labels[i][\"boxes\"].cuda()\n",
    "            labels[i][\"labels\"] = labels[i][\"labels\"].cuda()\n",
    "        loss_dict = model(imgs, labels)\n",
    "        wandb.log(loss_dict)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_num}, Loss: {losses}\")\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for imgs, labels in val_loader:\n",
    "    #         imgs = imgs.cuda()\n",
    "    #         for i in range(len(labels)):\n",
    "    #             labels[i][\"boxes\"] = labels[i][\"boxes\"].cuda()\n",
    "    #             labels[i][\"labels\"] = labels[i][\"labels\"].cuda()\n",
    "\n",
    "    #         loss_dict = model(imgs, labels)\n",
    "\n",
    "    #         val_loss_dict = {}\n",
    "    #         for k, v in loss_dict.items():\n",
    "    #             val_loss_dict[f\"val_{k}\"] = v.item()\n",
    "    #         wandb.log(val_loss_dict)\n",
    "\n",
    "    #         losses = sum(loss for loss in loss_dict.values())\n",
    "    # print(f\"Epoch {epoch}, Loss: {losses}\")\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeDataset:\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        return 1\n",
    "\n",
    "a = GenerativeDataset()\n",
    "\n",
    "c = 0\n",
    "for i in a:\n",
    "    print(i)\n",
    "    c += 1\n",
    "    if c == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = sum(loss for loss in loss_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0][\"pov\"]\n",
    "torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "observed_data = []\n",
    "\n",
    "for f in glob.glob(\"/plancraft/outputs/oracle_real/train/0/*.json\"):\n",
    "    with open(f, \"r\") as file:\n",
    "        inventories = json.load(file)[\"model_trace\"][\"inventory_history\"]\n",
    "    gif_path = str(f).replace(\".json\", \".gif\")\n",
    "    # load gif as list of images\n",
    "    gif = Image.open(gif_path)\n",
    "    frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "    if len(frames) != len(inventories):\n",
    "        print(f)\n",
    "    else:\n",
    "        for frame, inv in zip(frames, inventories):\n",
    "            clean_inv = []\n",
    "            for item in inv:\n",
    "                if item[\"quantity\"] > 0:\n",
    "                    clean_inv.append(\n",
    "                        {\n",
    "                            \"type\": item[\"type\"],\n",
    "                            \"slot\": item[\"index\"],\n",
    "                            \"quantity\": item[\"quantity\"],\n",
    "                            \"bbox\": slot_to_bbox(item[\"index\"]),\n",
    "                        }\n",
    "                    )\n",
    "            observed_data.append(\n",
    "                {\"inventory\": clean_inv, \"pov\": np.array(frame.convert(\"RGB\"))}\n",
    "            )\n",
    "    # assert len(frames) == len(inv), (len(frames), len(inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class InventoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        pov = item[\"pov\"]\n",
    "        pov = self.transform(pov)\n",
    "\n",
    "        inventory = item[\"inventory\"]\n",
    "        types = [i[\"type\"] for i in inventory]\n",
    "        slots = [i[\"slot\"] for i in inventory]\n",
    "        quantities = [i[\"quantity\"] for i in inventory]\n",
    "        bboxes = [i[\"bbox\"] for i in inventory]\n",
    "\n",
    "        return pov, types, slots, quantities, bboxes\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = InventoryDataset(observed_data)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class InventoryModel(nn.Module):\n",
    "    def __init__(self, num_types, num_slots):\n",
    "        super(InventoryModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the classification layer\n",
    "\n",
    "        # Bounding box head\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),  # 4 coordinates for the bounding box\n",
    "        )\n",
    "\n",
    "        # Slot index prediction head\n",
    "        self.slot_head = nn.Sequential(\n",
    "            nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, num_slots), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Quantity prediction head\n",
    "        self.quantity_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Softmax(dim=1), \n",
    "        )\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad) // 1000000\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        bbox = self.bbox_head(features)\n",
    "        # types = self.type_head(features)\n",
    "        slots = self.slot_head(features)\n",
    "        quantity = self.quantity_head(features)\n",
    "        return bbox, slots, quantity\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = InventoryModel(num_types=100, num_slots=45)  # Replace with actual numbers\n",
    "model = model.cuda()\n",
    "# Count number of parameters\n",
    "print(f\"Model has {model.count_parameters()}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get oracle O,A Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "oracle_trajectories_train = []\n",
    "oracle_results = {\n",
    "    \"/plancraft/outputs/oracle_real/train/0/*.json\": [],\n",
    "    \"/plancraft/outputs/oracle_real/val/0/*.json\": [],\n",
    "}\n",
    "for path in oracle_results.keys():\n",
    "    for f in glob.glob(path):\n",
    "        with open(f, \"r\") as file:\n",
    "            traj = json.load(file)\n",
    "\n",
    "        images = []\n",
    "        gif_path = str(f).replace(\".json\", \".gif\")\n",
    "        gif = Image.open(gif_path)\n",
    "        for frame in ImageSequence.Iterator(gif):\n",
    "            images.append(np.array(frame.convert(\"RGB\")))\n",
    "        traj[\"model_trace\"][\"images\"] = images\n",
    "\n",
    "        if (\n",
    "            len(traj[\"model_trace\"][\"images\"])\n",
    "            == len(traj[\"model_trace\"][\"inventory_history\"])\n",
    "            == len(traj[\"model_trace\"][\"action_history\"])\n",
    "        ):\n",
    "            oracle_results[path].append(traj)\n",
    "        else:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from plancraft.models.react_prompts import REACT_SYSTEM_PROMPT\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def convert_obs_to_text(objective: str, inventory: list[dict]):\n",
    "    return f\"TASK: {objective}\\ninventory={json.dumps(inventory)}\"\n",
    "\n",
    "\n",
    "def convert_action_to_text(action: dict):\n",
    "    # {'action_type': 'move', 'slot_from': 17, 'slot_to': 1, 'quantity': 1}\n",
    "    return f\"act: {action['action_type']} from slot {action['slot_from']} to slot {action['slot_to']} with quantity {action['quantity']}\"\n",
    "\n",
    "\n",
    "# convert action and inventory to dialogue history\n",
    "def convert_trajectory_to_base_dialogue(traj: dict):\n",
    "    dialogue = [{\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT}]\n",
    "    objective = traj[\"model_trace\"][\"objective\"]\n",
    "    for _, action, inventory in zip(\n",
    "        traj[\"model_trace\"][\"images\"],\n",
    "        traj[\"model_trace\"][\"action_history\"],\n",
    "        traj[\"model_trace\"][\"inventory_history\"],\n",
    "    ):\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": convert_obs_to_text(objective, inventory),\n",
    "            }\n",
    "        )\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": convert_action_to_text(action),\n",
    "            }\n",
    "        )\n",
    "    example = {\n",
    "        \"messages\": dialogue,\n",
    "        \"example_id\": traj[\"example_id\"],\n",
    "    }\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert action and inventory to dialogue history\n",
    "def convert_trajectory_to_image_dialogue(traj: dict):\n",
    "    dialogue = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": REACT_SYSTEM_PROMPT}]}\n",
    "    ]\n",
    "    objective = traj[\"model_trace\"][\"objective\"]\n",
    "    images = []\n",
    "    for image, action, inventory in zip(\n",
    "        traj[\"model_trace\"][\"images\"],\n",
    "        traj[\"model_trace\"][\"action_history\"],\n",
    "        traj[\"model_trace\"][\"inventory_history\"],\n",
    "    ):\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": objective}],\n",
    "            }\n",
    "        )\n",
    "        dialogue.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": convert_action_to_text(action)}],\n",
    "            }\n",
    "        )\n",
    "        images.append(image)\n",
    "    example = {\n",
    "        \"messages\": dialogue,\n",
    "        \"example_id\": traj[\"example_id\"],\n",
    "    }\n",
    "    return example, images\n",
    "\n",
    "\n",
    "text_data = defaultdict(list)\n",
    "mm_data = defaultdict(list)\n",
    "for path, trajs in oracle_results.items():\n",
    "    split = path.split(\"/\")[-3]\n",
    "    for traj in trajs:\n",
    "        text_example = convert_trajectory_to_base_dialogue(traj)\n",
    "        text_data[split].append(text_example)\n",
    "        mm_example, example_imgs = convert_trajectory_to_image_dialogue(traj)\n",
    "        mm_data[split].append(mm_example)\n",
    "        # save imgs as png in format \"data/oracle/{split}/{example_id}_{step}.gif\"\n",
    "        os.makedirs(f\"data/oracle/{split}\", exist_ok=True)\n",
    "        for i, img in enumerate(example_imgs):\n",
    "            Image.fromarray(img).save(\n",
    "                f\"data/oracle/{split}/{traj['example_id']}_{i}.png\"\n",
    "            )\n",
    "\n",
    "    # save as jsonl file\n",
    "    with open(f\"data/oracle/{split}.jsonl\", \"w\") as f:\n",
    "        for example in text_data[split]:\n",
    "            f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "    with open(f\"data/oracle/{split}.mm.jsonl\", \"w\") as f:\n",
    "        for example in mm_data[split]:\n",
    "            f.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train image classifier with Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
      "  self.hub = sentry_sdk.Hub(client)\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env loaded\n"
     ]
    }
   ],
   "source": [
    "from train_fast_rcnn import sample_environment\n",
    "\n",
    "\n",
    "for batch in sample_environment(batch_size=32, N=100):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, raw_images, inv = batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=171x164>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.fromarray((images[0].cpu().numpy()*255).astype(np.uint8).transpose(1, 2, 0))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_repo)\n",
    "model = AutoModelForObjectDetection.from_pretrained(model_repo)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_format = []\n",
    "for i in range(len(targets)):\n",
    "    annotations = []\n",
    "    for j in range(len(targets[i][\"boxes\"])):\n",
    "        box = targets[i][\"boxes\"][j].tolist()  # Convert box tensor to list\n",
    "        area = box[2] * box[3]  # Calculate area (width * height)\n",
    "\n",
    "        annotation = {\n",
    "            \"bbox\": box,  # Bounding box in [x, y, width, height]\n",
    "            \"category_id\": targets[i][\"labels\"][\n",
    "                j\n",
    "            ].item(),  # Class label as integer\n",
    "            \"area\": area,  # Area of the bounding box\n",
    "            \"iscrowd\": 0,  # Default iscrowd to 0 unless specified\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    coco_format.append(\n",
    "        {\n",
    "            \"image_id\": i,  # Image ID\n",
    "            \"annotations\": annotations,  # List of annotations for the current image\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now pass the correctly formatted COCO annotations to the processor\n",
    "inputs= processor(\n",
    "    images=images,\n",
    "    annotations=coco_format,\n",
    "    return_tensors=\"pt\",\n",
    "    do_rescale=False,\n",
    ")\n",
    "\n",
    "# cast to device\n",
    "# inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "# inputs = [tensor.to(device) for tensor in inputs]\n",
    "inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "for i in range(len(inputs[\"labels\"])):\n",
    "    for k in inputs[\"labels\"][i].keys():\n",
    "        inputs[\"labels\"][i][k] = inputs[\"labels\"][i][k].to(device)    \n",
    "    \n",
    "#     inputs[\"annotations\"][i][\"iscrowd\"] = inputs[\"annotations\"][i][\"iscrowd\"].to(device)\n",
    "#     inputs[\"annotations\"][i][\"area\"] = inputs[\"annotations\"][i][\"area\"].to(device)\n",
    "#     inputs[\"annotations\"][i][\"bbox\"] = inputs[\"annotations\"][i][\"bbox\"].to(device)\n",
    "#     inputs[\"annotations\"][i][\"category_id\"] = inputs[\"annotations\"][i][\"category_id\"].to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [134,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [162,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [163,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [26,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;66;03m# labels=ta\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;66;03m# rgets)# target_sizes = torch.tensor([[image.size[1], image.size[0]]])\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/conditional_detr/modeling_conditional_detr.py:1766\u001b[0m, in \u001b[0;36mConditionalDetrForObjectDetection.forward\u001b[0;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     auxiliary_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_aux_loss(outputs_class, outputs_coord)\n\u001b[1;32m   1764\u001b[0m     outputs_loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auxiliary_outputs\n\u001b[0;32m-> 1766\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# Fourth: compute total loss, as a weighted sum of the various losses\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_ce\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcls_loss_coefficient, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbbox_loss_coefficient}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/conditional_detr/modeling_conditional_detr.py:2384\u001b[0m, in \u001b[0;36mConditionalDetrLoss.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m   2381\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2383\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[0;32m-> 2384\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2386\u001b[0m \u001b[38;5;66;03m# Compute the average number of target boxes across all nodes, for normalization purposes\u001b[39;00m\n\u001b[1;32m   2387\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/conditional_detr/modeling_conditional_detr.py:2506\u001b[0m, in \u001b[0;36mConditionalDetrHungarianMatcher.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m   2503\u001b[0m class_cost \u001b[38;5;241m=\u001b[39m pos_cost_class[:, target_ids] \u001b[38;5;241m-\u001b[39m neg_cost_class[:, target_ids]\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;66;03m# Compute the L1 cost between boxes\u001b[39;00m\n\u001b[0;32m-> 2506\u001b[0m bbox_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[38;5;66;03m# Compute the giou cost between boxes\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m giou_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgeneralized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py:1336\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1334\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model(**inputs) \n",
    "                # labels=ta\n",
    "                # rgets)# target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "targets[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=171x164>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/plancraft/data/oracle/train/imgs/TRAIN0001_0.png\")\n",
    "image.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: labels.\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "model.train()\n",
    "inputs = processor(images=[image], return_tensors=\"pt\", labels=labels)\n",
    "outputs = model(**inputs.to(device))\n",
    "target_sizes = torch.tensor([[image.size[1], image.size[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
       "          [ 2.2489,  2.2489,  1.2728,  ...,  1.2728,  1.2728,  1.2728],\n",
       "          ...,\n",
       "          [ 2.2489,  1.2728,  1.2728,  ...,  1.2728,  1.2728, -0.6623],\n",
       "          [ 1.2728, -0.6623, -0.6623,  ..., -0.6623, -0.6623, -0.6623],\n",
       "          [-2.1179, -0.6623, -0.6623,  ..., -0.6623, -0.6623, -0.6623]],\n",
       "\n",
       "         [[ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "          [ 2.4286,  2.4286,  1.4307,  ...,  1.4307,  1.4307,  1.4307],\n",
       "          ...,\n",
       "          [ 2.4286,  1.4307,  1.4307,  ...,  1.4307,  1.4307, -0.5476],\n",
       "          [ 1.4307, -0.5476, -0.5476,  ..., -0.5476, -0.5476, -0.5476],\n",
       "          [-2.0357, -0.5476, -0.5476,  ..., -0.5476, -0.5476, -0.5476]],\n",
       "\n",
       "         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "          [ 2.6400,  2.6400,  1.6465,  ...,  1.6465,  1.6465,  1.6465],\n",
       "          ...,\n",
       "          [ 2.6400,  1.6465,  1.6465,  ...,  1.6465,  1.6465, -0.3230],\n",
       "          [ 1.6465, -0.3230, -0.3230,  ..., -0.3230, -0.3230, -0.3230],\n",
       "          [-1.8044, -0.3230, -0.3230,  ..., -0.3230, -0.3230, -0.3230]]]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = processor.post_process_object_detection(\n",
    "    outputs, threshold=0.01, target_sizes=target_sizes\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Mask with confidence 0.025 at location [17.05, 45.76, 29.83, 54.08]\n",
      "Detected Mask with confidence 0.024 at location [1.4, 38.42, 12.66, 46.27]\n",
      "Detected Mask with confidence 0.024 at location [0.92, 46.64, 11.6, 54.3]\n",
      "Detected Mask with confidence 0.024 at location [18.08, 37.82, 31.14, 46.2]\n",
      "Detected Coverall with confidence 0.024 at location [23.22, 45.6, 41.37, 103.64]\n",
      "Detected Mask with confidence 0.023 at location [13.59, 42.19, 27.49, 50.67]\n",
      "Detected Mask with confidence 0.023 at location [5.0, 36.51, 17.4, 44.53]\n",
      "Detected Mask with confidence 0.023 at location [6.91, 46.65, 19.87, 54.95]\n",
      "Detected Mask with confidence 0.023 at location [17.47, 36.82, 31.06, 45.46]\n",
      "Detected Gloves with confidence 0.022 at location [134.93, 96.55, 143.92, 103.31]\n",
      "Detected Gloves with confidence 0.022 at location [135.66, 108.57, 144.82, 116.13]\n",
      "Detected Gloves with confidence 0.022 at location [139.54, 102.9, 150.05, 111.32]\n",
      "Detected Gloves with confidence 0.022 at location [0.92, 46.64, 11.6, 54.3]\n",
      "Detected Gloves with confidence 0.022 at location [137.06, 109.46, 146.2, 116.9]\n",
      "Detected Mask with confidence 0.022 at location [11.05, 34.58, 24.8, 43.34]\n",
      "Detected Gloves with confidence 0.022 at location [137.2, 97.71, 147.51, 105.72]\n",
      "Detected Gloves with confidence 0.022 at location [137.11, 104.72, 145.33, 110.87]\n",
      "Detected Gloves with confidence 0.022 at location [1.4, 38.42, 12.66, 46.27]\n",
      "Detected Goggles with confidence 0.021 at location [1.4, 38.42, 12.66, 46.27]\n",
      "Detected Mask with confidence 0.021 at location [18.84, 30.5, 32.43, 40.35]\n",
      "Detected Mask with confidence 0.021 at location [5.9, 50.62, 17.75, 58.33]\n",
      "Detected Goggles with confidence 0.021 at location [5.0, 36.51, 17.4, 44.53]\n",
      "Detected Mask with confidence 0.021 at location [20.6, 49.38, 33.01, 57.54]\n",
      "Detected Coverall with confidence 0.021 at location [36.69, 39.39, 57.53, 83.34]\n",
      "Detected Mask with confidence 0.021 at location [17.17, 47.74, 31.05, 56.48]\n",
      "Detected Goggles with confidence 0.021 at location [135.66, 108.57, 144.82, 116.13]\n",
      "Detected Gloves with confidence 0.021 at location [140.51, 107.99, 151.3, 116.78]\n",
      "Detected Goggles with confidence 0.021 at location [134.93, 96.55, 143.92, 103.31]\n",
      "Detected Goggles with confidence 0.02 at location [0.92, 46.64, 11.6, 54.3]\n",
      "Detected Gloves with confidence 0.02 at location [17.05, 45.76, 29.83, 54.08]\n",
      "Detected Gloves with confidence 0.02 at location [5.0, 36.51, 17.4, 44.53]\n",
      "Detected Gloves with confidence 0.02 at location [140.94, 92.09, 151.24, 100.1]\n",
      "Detected Coverall with confidence 0.02 at location [15.63, 45.63, 30.07, 80.51]\n",
      "Detected Goggles with confidence 0.02 at location [137.06, 109.46, 146.2, 116.9]\n",
      "Detected Goggles with confidence 0.02 at location [137.2, 97.71, 147.51, 105.72]\n",
      "Detected Goggles with confidence 0.02 at location [17.47, 36.82, 31.06, 45.46]\n",
      "Detected Goggles with confidence 0.02 at location [11.05, 34.58, 24.8, 43.34]\n",
      "Detected Gloves with confidence 0.02 at location [97.09, 41.44, 108.58, 48.76]\n",
      "Detected Mask with confidence 0.02 at location [21.5, 44.79, 35.68, 54.21]\n",
      "Detected Goggles with confidence 0.02 at location [137.11, 104.72, 145.33, 110.87]\n",
      "Detected Gloves with confidence 0.02 at location [6.91, 46.65, 19.87, 54.95]\n",
      "Detected Mask with confidence 0.02 at location [94.53, 1.82, 109.45, 12.9]\n",
      "Detected Mask with confidence 0.02 at location [14.43, 50.46, 27.46, 58.72]\n",
      "Detected Gloves with confidence 0.02 at location [13.59, 42.19, 27.49, 50.67]\n",
      "Detected Mask with confidence 0.02 at location [74.29, 18.82, 89.79, 29.78]\n",
      "Detected Gloves with confidence 0.02 at location [141.01, 93.65, 153.19, 103.02]\n",
      "Detected Mask with confidence 0.02 at location [94.14, 12.04, 108.59, 23.34]\n",
      "Detected Coverall with confidence 0.02 at location [10.52, 56.5, 23.68, 84.02]\n",
      "Detected Goggles with confidence 0.02 at location [13.59, 42.19, 27.49, 50.67]\n",
      "Detected Coverall with confidence 0.02 at location [151.76, 47.95, 165.37, 86.62]\n",
      "Detected Mask with confidence 0.019 at location [158.79, 3.01, 168.95, 12.58]\n",
      "Detected Gloves with confidence 0.019 at location [18.08, 37.82, 31.14, 46.2]\n",
      "Detected Mask with confidence 0.019 at location [127.28, 1.88, 142.95, 11.88]\n",
      "Detected Gloves with confidence 0.019 at location [133.49, 111.48, 142.12, 117.69]\n",
      "Detected Mask with confidence 0.019 at location [115.11, 15.27, 129.62, 25.07]\n",
      "Detected Mask with confidence 0.019 at location [18.57, 30.91, 32.38, 41.08]\n",
      "Detected Mask with confidence 0.019 at location [152.73, 11.45, 165.14, 22.16]\n",
      "Detected Mask with confidence 0.019 at location [119.6, 4.46, 134.22, 14.06]\n",
      "Detected Mask with confidence 0.019 at location [109.26, 4.18, 123.22, 14.39]\n",
      "Detected Mask with confidence 0.019 at location [90.12, 2.71, 104.87, 14.28]\n",
      "Detected Mask with confidence 0.019 at location [116.63, 2.04, 130.34, 11.42]\n",
      "Detected Mask with confidence 0.019 at location [114.15, 6.77, 129.23, 17.13]\n",
      "Detected Mask with confidence 0.019 at location [107.32, 15.89, 120.86, 26.18]\n",
      "Detected Mask with confidence 0.019 at location [107.66, 2.5, 120.5, 12.6]\n",
      "Detected Mask with confidence 0.019 at location [104.95, 8.18, 118.77, 18.63]\n",
      "Detected Goggles with confidence 0.019 at location [133.49, 111.48, 142.12, 117.69]\n",
      "Detected Gloves with confidence 0.019 at location [96.71, 46.6, 109.18, 54.11]\n",
      "Detected Mask with confidence 0.019 at location [113.74, 13.16, 127.62, 23.56]\n",
      "Detected Mask with confidence 0.019 at location [37.3, 4.52, 50.25, 14.65]\n",
      "Detected Gloves with confidence 0.019 at location [11.05, 34.58, 24.8, 43.34]\n",
      "Detected Mask with confidence 0.019 at location [84.58, 9.55, 100.08, 21.22]\n",
      "Detected Mask with confidence 0.019 at location [119.66, 9.86, 134.24, 20.01]\n",
      "Detected Mask with confidence 0.019 at location [150.04, 2.96, 162.87, 13.43]\n",
      "Detected Mask with confidence 0.019 at location [155.69, 8.2, 167.26, 19.19]\n",
      "Detected Gloves with confidence 0.019 at location [17.47, 36.82, 31.06, 45.46]\n",
      "Detected Mask with confidence 0.019 at location [27.35, 4.41, 39.47, 14.74]\n",
      "Detected Gloves with confidence 0.019 at location [96.28, 49.6, 107.76, 56.57]\n",
      "Detected Goggles with confidence 0.019 at location [6.91, 46.65, 19.87, 54.95]\n",
      "Detected Mask with confidence 0.019 at location [121.61, 15.49, 137.28, 26.2]\n",
      "Detected Mask with confidence 0.019 at location [134.82, 12.93, 149.69, 22.8]\n",
      "Detected Mask with confidence 0.019 at location [156.81, 17.59, 167.77, 27.84]\n",
      "Detected Mask with confidence 0.019 at location [131.83, 16.87, 147.47, 27.42]\n",
      "Detected Mask with confidence 0.019 at location [19.76, 2.71, 31.58, 13.98]\n",
      "Detected Mask with confidence 0.019 at location [125.99, 9.72, 141.68, 20.3]\n",
      "Detected Mask with confidence 0.019 at location [10.01, 30.91, 23.67, 41.03]\n",
      "Detected Mask with confidence 0.019 at location [7.97, 3.05, 18.41, 12.96]\n",
      "Detected Gloves with confidence 0.019 at location [5.9, 50.62, 17.75, 58.33]\n",
      "Detected Mask with confidence 0.019 at location [33.41, 12.43, 45.69, 22.5]\n",
      "Detected Gloves with confidence 0.019 at location [145.7, 89.52, 155.51, 97.25]\n",
      "Detected Mask with confidence 0.019 at location [96.28, 49.6, 107.76, 56.57]\n",
      "Detected Gloves with confidence 0.019 at location [59.74, 91.89, 70.3, 99.46]\n",
      "Detected Gloves with confidence 0.019 at location [164.92, 59.23, 172.99, 70.32]\n",
      "Detected Goggles with confidence 0.019 at location [139.54, 102.9, 150.05, 111.32]\n",
      "Detected Mask with confidence 0.019 at location [135.56, 7.62, 150.36, 17.75]\n",
      "Detected Gloves with confidence 0.019 at location [59.81, 91.75, 69.82, 99.04]\n",
      "Detected Mask with confidence 0.019 at location [147.6, 16.93, 161.21, 27.29]\n",
      "Detected Mask with confidence 0.019 at location [141.61, 2.99, 155.63, 13.09]\n",
      "Detected Mask with confidence 0.019 at location [5.01, 10.52, 14.74, 20.59]\n",
      "Detected Mask with confidence 0.019 at location [137.11, 104.72, 145.33, 110.87]\n",
      "Detected Coverall with confidence 0.019 at location [39.42, 65.47, 56.25, 95.28]\n"
     ]
    }
   ],
   "source": [
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
