training: 
  base_model: "llama3"
  qlora: True
  lora_r: 64
  lora_alpha: 32
  lora_dropout: 0.1
  seed: 42
  batch_size: 1
  max_steps: 80
  max_seq_length: 8142
  gradient_accumulation_steps: 4
  learning_rate: 1e-5
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  num_train_epochs: 3
  num_workers: 12
plancraft:
  model: meta-llama/Meta-Llama-3-70B-Instruct
  num_generations: 1
  mode: "act"
  max_steps: 80
  quantize: False
  environment:
    symbolic: True
    symbolic_observation_space: True
    symbolic_action_space: True
  split: val.small
  batch_size: 1
  max_message_window: 30
  resume: True
  output_dir: "outputs"
wandb:
  project: "plancraft-train"
  entity: "itl"
  mode: "offline"
launch:
  command: "python main.py"
  job_name: "plancraft-s2234411"
  gpu_limit: 1
  gpu_product: NVIDIA-A100-SXM4-80GB
  cpu_request: 12
  ram_request: 80Gi
  interactive: False
  namespace: informatics
  env_vars:
    HF_TOKEN:
      secret_name: s2234411-hf
      key: HF_TOKEN
    OPENAI_API_KEY:
      secret_name: s2234411-openai
      key: OPENAI_API_KEY
    WANDB_API_KEY:
      secret_name: s2234411-wandb
      key: WANDB_API_KEY
    SLACK_WEBHOOK:
      secret_name: s2234411-slack-webhook
      key: SLACK_WEBHOOK