{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel, AutoProcessor\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py:163\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision::nms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should have 4 elements in dimension 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PlancraftEnvironmentDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: str = \"data/oracle\", split=\"train\"):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.transform = transforms.ToTensor()\n",
    "        data = []\n",
    "        for example_path in sorted(glob.glob(f\"{dataset_dir}/{split}/oa/*.json\")):\n",
    "            with open(example_path) as f:\n",
    "                messages = json.load(f)\n",
    "                environments = []\n",
    "                for message in messages:\n",
    "                    if \"inventory=\" in message[\"content\"] and message[\"role\"] == \"user\":\n",
    "                        environments.append(self.clean((message[\"content\"].split(\"\\ninventory=\")[-1])))\n",
    "                example = {\n",
    "                    \"environments\": environments,\n",
    "                    \"example_id\": example_path.split(\"/\")[-1].split(\".json\")[0],\n",
    "                }\n",
    "                data.append(example)\n",
    "\n",
    "        print(\"Loading images\")\n",
    "        for example in data:\n",
    "            example[\"images\"] = []\n",
    "            for message_idx, _ in enumerate(example[\"environments\"]):\n",
    "                img_path = f\"{dataset_dir}/{split}/imgs/{example['example_id']}_{message_idx}.png\"\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = self.transform(img)\n",
    "                example[\"images\"].append(img)\n",
    "\n",
    "        self.dataset = []\n",
    "        for example in data:\n",
    "            for i, (env, img) in enumerate(zip(example[\"environments\"], example[\"images\"])):\n",
    "                self.dataset.append((env, img))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean(s: str):\n",
    "        return s.replace('\"type\": \"', \"\").replace('\"quantity\": ', \"\").replace('\"index\": ', \"\").replace('\"', \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\",\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return self.dataset[idx]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = PlancraftEnvironmentDataset(split=\"val\")\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        texts, images = zip(*batch)\n",
    "        inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        pixel_values = inputs.pixel_values.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        text_embeddings = outputs.text_embeds\n",
    "        image_embeddings = outputs.image_embeds\n",
    "        \n",
    "        # Assuming you want to calculate contrastive loss\n",
    "        logits = torch.mm(text_embeddings, image_embeddings.T)\n",
    "        labels = torch.arange(len(text_embeddings)).to(device)\n",
    "        loss = CrossEntropyLoss()(logits, labels) + CrossEntropyLoss()(logits.T, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": total_loss / len(progress_bar)})\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} - Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PlancraftEnvironmentDataset at 0x7f36ea950eb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "# inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# logits_per_image = outputs.logits_per_image\n",
    "# probs = torch.sigmoid(logits_per_image) # these are the probabilities\n",
    "# print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.tokenizer(dataset[0][0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inventory = [\n",
    "#     {\"slot\": 13, \"type\": \"stick\", \"quantity\": 2},\n",
    "#     {\"slot\": 20, \"type\": \"acacia_log\", \"quantity\": 1},\n",
    "#     {\"slot\": 43, \"type\": \"dead_fire_coral\", \"quantity\": 55},\n",
    "#     {\"slot\": 27, \"type\": \"acacia_leaves\", \"quantity\": 11},\n",
    "#     {\"slot\": 28, \"type\": \"brown_mushroom\", \"quantity\": 23},\n",
    "#     {\"slot\": 14, \"type\": \"llama_spawn_egg\", \"quantity\": 22},\n",
    "#     {\"slot\": 45, \"type\": \"bat_spawn_egg\", \"quantity\": 6},\n",
    "#     {\"slot\": 23, \"type\": \"oak_leaves\", \"quantity\": 8},\n",
    "#     {\"slot\": 34, \"type\": \"diorite_slab\", \"quantity\": 38},\n",
    "#     {\"slot\": 22, \"type\": \"dark_prismarine_slab\", \"quantity\": 54},\n",
    "# ]\n",
    "\n",
    "class TypeEmbedding(nn.Module):\n",
    "    def __init__(self, model=AutoModel, tokenizer=AutoTokenizer):\n",
    "        super(TypeEmbedding, self).__init__()\n",
    "        self.embedding_dim = model.config.hidden_size\n",
    "        self.learnable_params = nn.Parameter(torch.randn(self.embedding_dim))\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, object_types: list[str]):\n",
    "        batch, new_types = ([], [])\n",
    "        for object_type in object_types:\n",
    "            if object_type not in self.cache:\n",
    "                batch.append(object_type)\n",
    "                new_types.append(object_type)\n",
    "        if len(new_types) > 0:\n",
    "            inputs = self.tokenizer(new_types, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            for i, object_type in enumerate(new_types):\n",
    "                type_embedding = outputs.last_hidden_state[i].mean(dim=0)\n",
    "                self.cache[object_type] = type_embedding\n",
    "        embeddings = [\n",
    "            self.cache[object_type] + self.learnable_params\n",
    "            for object_type in object_types\n",
    "        ]\n",
    "        return torch.stack(embeddings)\n",
    "\n",
    "class InventoryEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryEncoder, self).__init__()\n",
    "        hidden_size = model.config.hidden_size\n",
    "        self.type_embedding = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_embedding = nn.Embedding(max_quantity, hidden_size)\n",
    "        self.slot_embedding = nn.Embedding(max_slot, hidden_size)\n",
    "        self.combine = nn.Linear(\n",
    "            hidden_size * 3,\n",
    "            hidden_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        type_embeddings = self.type_embedding([item[\"type\"] for item in inventory])\n",
    "        quantities = torch.tensor(\n",
    "            [item[\"quantity\"] for item in inventory], dtype=torch.long\n",
    "        )\n",
    "        slots = torch.tensor([item[\"slot\"] for item in inventory], dtype=torch.long)\n",
    "\n",
    "        quantities = quantities.cuda()\n",
    "        slots = slots.cuda()\n",
    "\n",
    "        quantity_embeddings = self.quantity_embedding(quantities)\n",
    "        slot_embeddings = self.slot_embedding(slots)\n",
    "        x_concat = torch.cat(\n",
    "            [type_embeddings, quantity_embeddings, slot_embeddings], dim=-1\n",
    "        )\n",
    "        embed = self.combine(x_concat).mean(dim=0)\n",
    "        return embed\n",
    "\n",
    "\n",
    "encoder = InventoryEncoder(model, tokenizer)\n",
    "encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InventoryGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryGenerator, self).__init__()\n",
    "        hidden_size = model.config.hidden_size\n",
    "        self.max_quantity = max_quantity\n",
    "        self.max_slot = max_slot\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.type_decoder = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_decoder = nn.Linear(hidden_size, max_quantity)\n",
    "        self.slot_decoder = nn.Linear(hidden_size, max_slot)\n",
    "\n",
    "    def forward(self, inventory_embedding):\n",
    "        x = self.fc(inventory_embedding)\n",
    "\n",
    "        type_embeds, quantity_embeds, slot_embeds = torch.split(\n",
    "            x, self.hidden_size, dim=-1\n",
    "        )\n",
    "\n",
    "        # Decode type embeddings\n",
    "        decoded_types = self.type_decoder.decode(type_embeds)\n",
    "\n",
    "        # Decode quantity and slot embeddings\n",
    "        quantities = self.quantity_decoder(quantity_embeds)\n",
    "        slots = self.slot_decoder(slot_embeds)\n",
    "\n",
    "        # Convert logits to indices\n",
    "        quantities = torch.argmax(quantities, dim=-1)\n",
    "        slots = torch.argmax(slots, dim=-1)\n",
    "\n",
    "        # Create the decoded inventory list\n",
    "        decoded_inventory = []\n",
    "        for obj_type, quantity, slot in zip(decoded_types, quantities, slots):\n",
    "            decoded_inventory.append(\n",
    "                {\n",
    "                    \"type\": obj_type,\n",
    "                    \"quantity\": quantity.item(),\n",
    "                    \"slot\": slot.item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return decoded_inventory\n",
    "\n",
    "\n",
    "# Example of how to use the InventoryEmbedding and InventoryGenerator\n",
    "class InventoryAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryAutoencoder, self).__init__()\n",
    "        self.encoder = InventoryEncoder(model, tokenizer, max_quantity, max_slot)\n",
    "        self.decoder = InventoryGenerator(model, tokenizer, max_quantity, max_slot)\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        encoded = self.encoder(inventory)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Example usage\n",
    "inventory = [\n",
    "    {\"slot\": 13, \"type\": \"stick\", \"quantity\": 2},\n",
    "    {\"slot\": 20, \"type\": \"acacia_log\", \"quantity\": 1},\n",
    "    {\"slot\": 43, \"type\": \"dead_fire_coral\", \"quantity\": 55},\n",
    "    {\"slot\": 27, \"type\": \"acacia_leaves\", \"quantity\": 11},\n",
    "    {\"slot\": 28, \"type\": \"brown_mushroom\", \"quantity\": 23},\n",
    "    {\"slot\": 14, \"type\": \"llama_spawn_egg\", \"quantity\": 22},\n",
    "    {\"slot\": 45, \"type\": \"bat_spawn_egg\", \"quantity\": 6},\n",
    "    {\"slot\": 23, \"type\": \"oak_leaves\", \"quantity\": 8},\n",
    "    {\"slot\": 34, \"type\": \"diorite_slab\", \"quantity\": 38},\n",
    "    {\"slot\": 22, \"type\": \"dark_prismarine_slab\", \"quantity\": 54},\n",
    "]\n",
    "\n",
    "autoencoder = InventoryAutoencoder(model, tokenizer)\n",
    "autoencoder = autoencoder.to(\"cuda\")\n",
    "encoded_inventory = autoencoder.encoder(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.decoder(encoded_inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5024,  0.7074,  1.2459,  ...,  0.2150, -0.3527, -0.0361],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(data[0][\"slotted_inventory\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data/train.json\n",
    "\n",
    "# with open('data/train.json') as f:\n",
    "#     data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
