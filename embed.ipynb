{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.36s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModel.from_pretrained(\"google/gemma-2b\")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'slot': 13, 'type': 'stick', 'quantity': 2},\n",
       " {'slot': 20, 'type': 'acacia_log', 'quantity': 1},\n",
       " {'slot': 43, 'type': 'dead_fire_coral', 'quantity': 55},\n",
       " {'slot': 27, 'type': 'acacia_leaves', 'quantity': 11},\n",
       " {'slot': 28, 'type': 'brown_mushroom', 'quantity': 23},\n",
       " {'slot': 14, 'type': 'llama_spawn_egg', 'quantity': 22},\n",
       " {'slot': 45, 'type': 'bat_spawn_egg', 'quantity': 6},\n",
       " {'slot': 23, 'type': 'oak_leaves', 'quantity': 8},\n",
       " {'slot': 34, 'type': 'diorite_slab', 'quantity': 38},\n",
       " {'slot': 22, 'type': 'dark_prismarine_slab', 'quantity': 54}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"slotted_inventory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inventory = [\n",
    "#     {\"slot\": 13, \"type\": \"stick\", \"quantity\": 2},\n",
    "#     {\"slot\": 20, \"type\": \"acacia_log\", \"quantity\": 1},\n",
    "#     {\"slot\": 43, \"type\": \"dead_fire_coral\", \"quantity\": 55},\n",
    "#     {\"slot\": 27, \"type\": \"acacia_leaves\", \"quantity\": 11},\n",
    "#     {\"slot\": 28, \"type\": \"brown_mushroom\", \"quantity\": 23},\n",
    "#     {\"slot\": 14, \"type\": \"llama_spawn_egg\", \"quantity\": 22},\n",
    "#     {\"slot\": 45, \"type\": \"bat_spawn_egg\", \"quantity\": 6},\n",
    "#     {\"slot\": 23, \"type\": \"oak_leaves\", \"quantity\": 8},\n",
    "#     {\"slot\": 34, \"type\": \"diorite_slab\", \"quantity\": 38},\n",
    "#     {\"slot\": 22, \"type\": \"dark_prismarine_slab\", \"quantity\": 54},\n",
    "# ]\n",
    "\n",
    "class TypeEmbedding(nn.Module):\n",
    "    def __init__(self, model=AutoModel, tokenizer=AutoTokenizer):\n",
    "        super(TypeEmbedding, self).__init__()\n",
    "        self.embedding_dim = model.config.hidden_size\n",
    "        self.learnable_params = nn.Parameter(torch.randn(self.embedding_dim))\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, object_types: list[str]):\n",
    "        batch, new_types = ([], [])\n",
    "        for object_type in object_types:\n",
    "            if object_type not in self.cache:\n",
    "                batch.append(object_type)\n",
    "                new_types.append(object_type)\n",
    "        if len(new_types) > 0:\n",
    "            inputs = self.tokenizer(new_types, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            for i, object_type in enumerate(new_types):\n",
    "                type_embedding = outputs.last_hidden_state[i].mean(dim=0)\n",
    "                self.cache[object_type] = type_embedding\n",
    "        embeddings = [\n",
    "            self.cache[object_type] + self.learnable_params\n",
    "            for object_type in object_types\n",
    "        ]\n",
    "        return torch.stack(embeddings)\n",
    "\n",
    "class InventoryEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryEncoder, self).__init__()\n",
    "        hidden_size = model.config.hidden_size\n",
    "        self.type_embedding = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_embedding = nn.Embedding(max_quantity, hidden_size)\n",
    "        self.slot_embedding = nn.Embedding(max_slot, hidden_size)\n",
    "        self.combine = nn.Linear(\n",
    "            hidden_size * 3,\n",
    "            hidden_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        type_embeddings = self.type_embedding([item[\"type\"] for item in inventory])\n",
    "        quantities = torch.tensor(\n",
    "            [item[\"quantity\"] for item in inventory], dtype=torch.long\n",
    "        )\n",
    "        slots = torch.tensor([item[\"slot\"] for item in inventory], dtype=torch.long)\n",
    "\n",
    "        quantities = quantities.cuda()\n",
    "        slots = slots.cuda()\n",
    "\n",
    "        quantity_embeddings = self.quantity_embedding(quantities)\n",
    "        slot_embeddings = self.slot_embedding(slots)\n",
    "        x_concat = torch.cat(\n",
    "            [type_embeddings, quantity_embeddings, slot_embeddings], dim=-1\n",
    "        )\n",
    "        embed = self.combine(x_concat).mean(dim=0)\n",
    "        return embed\n",
    "\n",
    "\n",
    "encoder = InventoryEncoder(model, tokenizer)\n",
    "encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InventoryGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryGenerator, self).__init__()\n",
    "        hidden_size = model.config.hidden_size\n",
    "        self.max_quantity = max_quantity\n",
    "        self.max_slot = max_slot\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.type_decoder = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_decoder = nn.Linear(hidden_size, max_quantity)\n",
    "        self.slot_decoder = nn.Linear(hidden_size, max_slot)\n",
    "\n",
    "    def forward(self, inventory_embedding):\n",
    "        x = self.fc(inventory_embedding)\n",
    "\n",
    "        type_embeds, quantity_embeds, slot_embeds = torch.split(\n",
    "            x, self.hidden_size, dim=-1\n",
    "        )\n",
    "\n",
    "        # Decode type embeddings\n",
    "        decoded_types = self.type_decoder.decode(type_embeds)\n",
    "\n",
    "        # Decode quantity and slot embeddings\n",
    "        quantities = self.quantity_decoder(quantity_embeds)\n",
    "        slots = self.slot_decoder(slot_embeds)\n",
    "\n",
    "        # Convert logits to indices\n",
    "        quantities = torch.argmax(quantities, dim=-1)\n",
    "        slots = torch.argmax(slots, dim=-1)\n",
    "\n",
    "        # Create the decoded inventory list\n",
    "        decoded_inventory = []\n",
    "        for obj_type, quantity, slot in zip(decoded_types, quantities, slots):\n",
    "            decoded_inventory.append(\n",
    "                {\n",
    "                    \"type\": obj_type,\n",
    "                    \"quantity\": quantity.item(),\n",
    "                    \"slot\": slot.item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return decoded_inventory\n",
    "\n",
    "\n",
    "# Example of how to use the InventoryEmbedding and InventoryGenerator\n",
    "class InventoryAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryAutoencoder, self).__init__()\n",
    "        self.encoder = InventoryEncoder(model, tokenizer, max_quantity, max_slot)\n",
    "        self.decoder = InventoryGenerator(model, tokenizer, max_quantity, max_slot)\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        encoded = self.encoder(inventory)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Example usage\n",
    "inventory = [\n",
    "    {\"slot\": 13, \"type\": \"stick\", \"quantity\": 2},\n",
    "    {\"slot\": 20, \"type\": \"acacia_log\", \"quantity\": 1},\n",
    "    {\"slot\": 43, \"type\": \"dead_fire_coral\", \"quantity\": 55},\n",
    "    {\"slot\": 27, \"type\": \"acacia_leaves\", \"quantity\": 11},\n",
    "    {\"slot\": 28, \"type\": \"brown_mushroom\", \"quantity\": 23},\n",
    "    {\"slot\": 14, \"type\": \"llama_spawn_egg\", \"quantity\": 22},\n",
    "    {\"slot\": 45, \"type\": \"bat_spawn_egg\", \"quantity\": 6},\n",
    "    {\"slot\": 23, \"type\": \"oak_leaves\", \"quantity\": 8},\n",
    "    {\"slot\": 34, \"type\": \"diorite_slab\", \"quantity\": 38},\n",
    "    {\"slot\": 22, \"type\": \"dark_prismarine_slab\", \"quantity\": 54},\n",
    "]\n",
    "\n",
    "autoencoder = InventoryAutoencoder(model, tokenizer)\n",
    "autoencoder = autoencoder.to(\"cuda\")\n",
    "encoded_inventory = autoencoder.encoder(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.decoder(encoded_inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5024,  0.7074,  1.2459,  ...,  0.2150, -0.3527, -0.0361],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(data[0][\"slotted_inventory\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data/train.json\n",
    "\n",
    "# with open('data/train.json') as f:\n",
    "#     data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
