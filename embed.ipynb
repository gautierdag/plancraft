{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModel.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeEmbedding(nn.Module):\n",
    "    def __init__(self, model=AutoModel, tokenizer=AutoTokenizer):\n",
    "        super(TypeEmbedding, self).__init__()\n",
    "        self.embedding_dim = model.config.hidden_size\n",
    "        self.learnable_params = nn.Parameter(torch.randn(self.embedding_dim))\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, object_types: list[str]):\n",
    "        batch, new_types = ([], [])\n",
    "        for object_type in object_types:\n",
    "            if object_type not in self.cache:\n",
    "                batch.append(object_type)\n",
    "                new_types.append(object_type)\n",
    "        if len(new_types) > 0:\n",
    "            inputs = self.tokenizer(\n",
    "                new_types, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            for i, object_type in enumerate(new_types):\n",
    "                type_embedding = outputs.last_hidden_state[i].mean(dim=0)\n",
    "                self.cache[object_type] = type_embedding\n",
    "        embeddings = [\n",
    "            self.cache[object_type] + self.learnable_params\n",
    "            for object_type in object_types\n",
    "        ]\n",
    "        return torch.stack(embeddings)\n",
    "\n",
    "\n",
    "class InventoryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        hid_dim=128,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryEmbedding, self).__init__()\n",
    "        self.type_embedding = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_embedding = nn.Embedding(max_quantity, hid_dim)\n",
    "        self.slot_embedding = nn.Embedding(max_slot, hid_dim)\n",
    "        self.combine = nn.Linear(\n",
    "            self.type_embedding.embedding_dim + hid_dim + hid_dim,\n",
    "            hid_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        type_embeddings = self.type_embedding([item[\"type\"] for item in inventory])\n",
    "        quantities = torch.tensor([item[\"quantity\"] for item in inventory], dtype=torch.long)\n",
    "        slots = torch.tensor([item[\"slot\"] for item in inventory], dtype=torch.long)\n",
    "        quantity_embeddings = self.quantity_embedding(quantities)\n",
    "        slot_embeddings = self.slot_embedding(slots)\n",
    "        x_concat = torch.cat(\n",
    "            [type_embeddings, quantity_embeddings, slot_embeddings], dim=-1\n",
    "        )\n",
    "        embed = self.combine(x_concat)\n",
    "        \n",
    "        return embed\n",
    "\n",
    "\n",
    "inv_embed = InventoryEmbedding(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2238, -1.8951, -1.0089,  ...,  0.3079, -0.0340, -1.9083],\n",
       "        [-2.3568, -0.7631,  0.4788,  ..., -0.3396, -0.4099, -2.5646],\n",
       "        [-2.6034, -0.2149,  1.0169,  ..., -0.7544, -0.3539, -2.3843],\n",
       "        ...,\n",
       "        [-2.2182, -0.1333, -0.3205,  ..., -0.8458,  0.0336, -2.8775],\n",
       "        [-2.5166, -0.1252,  0.1649,  ..., -0.5924, -0.0082, -2.9452],\n",
       "        [-2.2456, -0.0594,  0.3108,  ...,  0.1623,  0.0161, -1.8329]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_embed(data[0][\"slotted_inventory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data/train.json\n",
    "\n",
    "# with open('data/train.json') as f:\n",
    "#     data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
