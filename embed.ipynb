{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModel.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeEmbedding(nn.Module):\n",
    "    def __init__(self, model=AutoModel, tokenizer=AutoTokenizer):\n",
    "        super(TypeEmbedding, self).__init__()\n",
    "        embedding_dim = model.config.hidden_size\n",
    "        self.learnable_params = nn.Parameter(torch.randn(embedding_dim))\n",
    "\n",
    "    def forward(self, object_type_str: str):\n",
    "        inputs = self.tokenizer(\n",
    "            object_type_str, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        type_embedding = outputs.last_hidden_state.mean(\n",
    "            dim=1\n",
    "        )  # [batch_size, embedding_dim]\n",
    "        type_embedding = type_embedding.squeeze(0)  # Remove batch dimension\n",
    "        return type_embedding + self.learnable_params\n",
    "\n",
    "\n",
    "\n",
    "import torchtune\n",
    "\n",
    "class InventoryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        rope_embedding_dim=128,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryEmbedding, self).__init__()\n",
    "        self.type_embedding = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_embedding = torchtune.modules.RotaryPositionalEmbeddings(\n",
    "            rope_embedding_dim, max_seq_len=max_quantity\n",
    "        )\n",
    "        self.slot_embedding = torchtune.modules.RotaryPositionalEmbeddings(\n",
    "            rope_embedding_dim, max_seq_len=max_slot\n",
    "        )\n",
    "        self.fc = nn.Linear(3 * rope_embedding_dim, rope_embedding_dim)\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "\n",
    "        for item in inventory:\n",
    "            object_type_str = item[\"type\"]\n",
    "            quantity = item[\"quantity\"]\n",
    "            slot = item[\"slot\"]\n",
    "            type_embedding = self.type_embedding(object_type_str)\n",
    "            quantity_embedding = self.quantity_embedding(quantity)\n",
    "            slot_embedding = self.slot_embedding(slot)\n",
    "        return type_embedding, slot_embedding, quantity_embedding\n",
    "\n",
    "\n",
    "inv_embed = InventoryEmbedding(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[-0.3012,  1.3818,  0.3128,  1.3792,  0.6394,  1.2614,  0.8073,\n",
       "            1.1611,  0.8952,  1.0948,  0.9422,  1.0546,  0.9679,  1.0311,\n",
       "            0.9821,  1.0176,  0.9900,  1.0099,  0.9944,  1.0056,  0.9968,\n",
       "            1.0032,  0.9982,  1.0018,  0.9990,  1.0010,  0.9994,  1.0006,\n",
       "            0.9997,  1.0003,  0.9998,  1.0002]],\n",
       "\n",
       "         [[-1.3254,  0.4932, -0.4707,  1.3336,  0.2155,  1.3977,  0.5892,\n",
       "            1.2856,  0.7814,  1.1787,  0.8815,  1.1059,  0.9348,  1.0612,\n",
       "            0.9638,  1.0349,  0.9798,  1.0198,  0.9887,  1.0112,  0.9937,\n",
       "            1.0063,  0.9964,  1.0036,  0.9980,  1.0020,  0.9989,  1.0011,\n",
       "            0.9994,  1.0006,  0.9996,  1.0004]],\n",
       "\n",
       "         [[-1.1311, -0.8489, -1.1092,  0.8773, -0.2299,  1.3954,  0.3525,\n",
       "            1.3696,  0.6598,  1.2509,  0.8179,  1.1537,  0.9008,  1.0902,\n",
       "            0.9453,  1.0519,  0.9696,  1.0295,  0.9830,  1.0167,  0.9905,\n",
       "            1.0094,  0.9947,  1.0053,  0.9970,  1.0030,  0.9983,  1.0017,\n",
       "            0.9991,  1.0009,  0.9995,  1.0005]],\n",
       "\n",
       "         [[ 0.1032, -1.4104, -1.4062,  0.1508, -0.6524,  1.2547,  0.1047,\n",
       "            1.4103,  0.5316,  1.3105,  0.7518,  1.1979,  0.8659,  1.1182,\n",
       "            0.9264,  1.0685,  0.9592,  1.0392,  0.9773,  1.0222,  0.9873,\n",
       "            1.0126,  0.9929,  1.0071,  0.9960,  1.0040,  0.9977,  1.0022,\n",
       "            0.9987,  1.0013,  0.9993,  1.0007]],\n",
       "\n",
       "         [[ 1.2426, -0.6753, -1.2700, -0.6221, -1.0103,  0.9896, -0.1464,\n",
       "            1.4066,  0.3982,  1.3570,  0.6833,  1.2382,  0.8301,  1.1450,\n",
       "            0.9073,  1.0848,  0.9488,  1.0487,  0.9715,  1.0277,  0.9841,\n",
       "            1.0157,  0.9911,  1.0089,  0.9950,  1.0050,  0.9972,  1.0028,\n",
       "            0.9984,  1.0016,  0.9991,  1.0009]],\n",
       "\n",
       "         [[ 1.2396,  0.6808, -0.7427, -1.2035, -1.2679,  0.6264, -0.3930,\n",
       "            1.3585,  0.2607,  1.3900,  0.6126,  1.2747,  0.7935,  1.1707,\n",
       "            0.8878,  1.1008,  0.9382,  1.0582,  0.9657,  1.0332,  0.9808,\n",
       "            1.0188,  0.9893,  1.0106,  0.9940,  1.0060,  0.9966,  1.0034,\n",
       "            0.9981,  1.0019,  0.9989,  1.0011]],\n",
       "\n",
       "         [[ 0.0969,  1.4109,  0.0133, -1.4142, -1.3999,  0.2010, -0.6271,\n",
       "            1.2676,  0.1206,  1.4091,  0.5400,  1.3071,  0.7560,  1.1952,\n",
       "            0.8681,  1.1164,  0.9276,  1.0675,  0.9599,  1.0386,  0.9776,\n",
       "            1.0219,  0.9875,  1.0124,  0.9930,  1.0070,  0.9961,  1.0039,\n",
       "            0.9978,  1.0022,  0.9988,  1.0012]],\n",
       "\n",
       "         [[-1.1349,  0.8439,  0.7652, -1.1893, -1.3930, -0.2443, -0.8414,\n",
       "            1.1367, -0.0206,  1.4141,  0.4657,  1.3354,  0.7179,  1.2185,\n",
       "            0.8481,  1.1317,  0.9169,  1.0767,  0.9540,  1.0440,  0.9744,\n",
       "            1.0250,  0.9857,  1.0141,  0.9920,  1.0080,  0.9955,  1.0045,\n",
       "            0.9975,  1.0025,  0.9986,  1.0014]],\n",
       "\n",
       "         [[-1.3232, -0.4990,  1.2815, -0.5982, -1.2479, -0.6654, -1.0292,\n",
       "            0.9699, -0.1617,  1.4049,  0.3899,  1.3594,  0.6790,  1.2406,\n",
       "            0.8279,  1.1466,  0.9061,  1.0858,  0.9481,  1.0493,  0.9711,\n",
       "            1.0281,  0.9839,  1.0159,  0.9910,  1.0090,  0.9949,  1.0050,\n",
       "            0.9971,  1.0028,  0.9984,  1.0016]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data/train.json\n",
    "\n",
    "# with open('data/train.json') as f:\n",
    "#     data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
