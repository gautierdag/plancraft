{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Loading images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "class PlancraftEnvironmentDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: str = \"data/oracle\", split=\"train\"):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.transform = transforms.ToTensor()\n",
    "        data = []\n",
    "        for example_path in sorted(glob.glob(f\"{dataset_dir}/{split}/oa/*.json\")):\n",
    "            with open(example_path) as f:\n",
    "                messages = json.load(f)\n",
    "                environments = []\n",
    "                for message in messages:\n",
    "                    if \"inventory=\" in message[\"content\"] and message[\"role\"] == \"user\":\n",
    "                        environments.append(self.clean((message[\"content\"].split(\"\\ninventory=\")[-1])))\n",
    "                example = {\n",
    "                    \"environments\": environments,\n",
    "                    \"example_id\": example_path.split(\"/\")[-1].split(\".json\")[0],\n",
    "                }\n",
    "                data.append(example)\n",
    "\n",
    "        print(\"Loading images\")\n",
    "        for example in data:\n",
    "            example[\"images\"] = []\n",
    "            for message_idx, _ in enumerate(example[\"environments\"]):\n",
    "                img_path = f\"{dataset_dir}/{split}/imgs/{example['example_id']}_{message_idx}.png\"\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                example[\"images\"].append(img)\n",
    "\n",
    "        self.dataset = []\n",
    "        for example in data:\n",
    "            for i, (env, img) in enumerate(zip(example[\"environments\"], example[\"images\"])):\n",
    "                self.dataset.append((env, img))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean(s: str):\n",
    "        return s.replace('\"type\": \"', \"\").replace('\"quantity\": ', \"\").replace('\"index\": ', \"\").replace('\"', \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\",\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "# Load the dataset\n",
    "dataset = PlancraftEnvironmentDataset(split=\"train\")\n",
    "val_dataset = PlancraftEnvironmentDataset(split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img_tensors = []\n",
    "    texts = []\n",
    "    for text, img in batch:\n",
    "        img_tensors.append(img)\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"images\": img_tensors,\n",
    "        \"texts\": texts,\n",
    "    }\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      " 12%|█▏        | 41/331 [02:45<19:31,  4.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     text_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(text_preds, eye)\n\u001b[1;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m image_loss \u001b[38;5;241m+\u001b[39m text_loss\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# val_loss = 0\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# for batch in val_dataloader:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     texts = batch[\"texts\"]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     val_loss += loss.item()\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(f\"Validation loss for epoch {epoch}: {val_loss}\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\", \n",
    "                                  attn_implementation=\"sdpa\",\n",
    "                                #   torch_dtype=torch.bfloat16,\n",
    "                                  device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    for batch in tqdm(dataloader, total=len(dataset)//32):\n",
    "        texts = batch[\"texts\"]\n",
    "        images = batch[\"images\"]\n",
    "        inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        image_preds = torch.sigmoid(outputs.logits_per_image)\n",
    "        text_preds = torch.sigmoid(outputs.logits_per_text)\n",
    "        eye = torch.eye(len(texts), len(images), device=model.device)\n",
    "        image_loss = F.binary_cross_entropy(image_preds, eye)\n",
    "        text_loss = F.binary_cross_entropy(text_preds, eye)\n",
    "        loss = image_loss + text_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # val_loss = 0\n",
    "    # for batch in val_dataloader:\n",
    "    #     texts = batch[\"texts\"]\n",
    "    #     images = batch[\"images\"]\n",
    "    #     inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    #     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    #     outputs = model(**inputs)\n",
    "    #     image_preds = torch.sigmoid(outputs.logits_per_image)\n",
    "    #     text_preds = torch.sigmoid(outputs.logits_per_text)\n",
    "    #     # logits = torch.mm(text_embeddings, image_embeddings.T)\n",
    "    #     # labels = torch.arange(len(text_embeddings), device=model.device)\n",
    "    #     # labels\n",
    "    #     loss = F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)\n",
    "    #     val_loss += loss.item()\n",
    "    # print(f\"Validation loss for epoch {epoch}: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.7906e-18, 4.5203e-13,  ..., 9.9979e-01, 3.3730e-04,\n",
       "         1.5441e-21],\n",
       "        [1.9761e-09, 1.0000e+00, 1.7174e-14,  ..., 9.1930e-08, 8.2393e-08,\n",
       "         3.1202e-02],\n",
       "        [5.1788e-12, 3.6618e-11, 1.0000e+00,  ..., 5.0566e-15, 7.0461e-01,\n",
       "         2.3677e-15],\n",
       "        ...,\n",
       "        [3.7981e-01, 2.7335e-12, 1.9660e-20,  ..., 4.8340e-04, 4.3409e-05,\n",
       "         2.9892e-14],\n",
       "        [1.5553e-01, 3.3465e-18, 1.0885e-05,  ..., 9.9999e-01, 1.0000e+00,\n",
       "         6.4046e-22],\n",
       "        [3.3447e-07, 1.1841e-08, 3.6278e-08,  ..., 1.3880e-12, 7.3904e-13,\n",
       "         1.0000e+00]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[43mdataloader\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     12\u001b[0m     texts, images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        texts, images = zip(*batch)\n",
    "        inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        pixel_values = inputs.pixel_values.to(device)\n",
    "         \n",
    "#         outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "#         text_embeddings = outputs.text_embeds\n",
    "#         image_embeddings = outputs.image_embeds\n",
    "        \n",
    "#         # Assuming you want to calculate contrastive loss\n",
    "#         logits = torch.mm(text_embeddings, image_embeddings.T)\n",
    "#         labels = torch.arange(len(text_embeddings)).to(device)\n",
    "#         loss = CrossEntropyLoss()(logits, labels) + CrossEntropyLoss()(logits.T, labels)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         progress_bar.set_postfix({\"loss\": total_loss / len(progress_bar)})\n",
    "        \n",
    "#     print(f\"Epoch {epoch + 1} - Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inventory = [\n",
    "#     {\"slot\": 13, \"type\": \"stick\", \"quantity\": 2},\n",
    "#     {\"slot\": 20, \"type\": \"acacia_log\", \"quantity\": 1},\n",
    "#     {\"slot\": 43, \"type\": \"dead_fire_coral\", \"quantity\": 55},\n",
    "#     {\"slot\": 27, \"type\": \"acacia_leaves\", \"quantity\": 11},\n",
    "#     {\"slot\": 28, \"type\": \"brown_mushroom\", \"quantity\": 23},\n",
    "#     {\"slot\": 14, \"type\": \"llama_spawn_egg\", \"quantity\": 22},\n",
    "#     {\"slot\": 45, \"type\": \"bat_spawn_egg\", \"quantity\": 6},\n",
    "#     {\"slot\": 23, \"type\": \"oak_leaves\", \"quantity\": 8},\n",
    "#     {\"slot\": 34, \"type\": \"diorite_slab\", \"quantity\": 38},\n",
    "#     {\"slot\": 22, \"type\": \"dark_prismarine_slab\", \"quantity\": 54},\n",
    "# ]\n",
    "\n",
    "class TypeEmbedding(nn.Module):\n",
    "    def __init__(self, model=AutoModel, tokenizer=AutoTokenizer):\n",
    "        super(TypeEmbedding, self).__init__()\n",
    "        self.embedding_dim = model.config.hidden_size\n",
    "        self.learnable_params = nn.Parameter(torch.randn(self.embedding_dim))\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, object_types: list[str]):\n",
    "        batch, new_types = ([], [])\n",
    "        for object_type in object_types:\n",
    "            if object_type not in self.cache:\n",
    "                batch.append(object_type)\n",
    "                new_types.append(object_type)\n",
    "        if len(new_types) > 0:\n",
    "            inputs = self.tokenizer(new_types, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            for i, object_type in enumerate(new_types):\n",
    "                type_embedding = outputs.last_hidden_state[i].mean(dim=0)\n",
    "                self.cache[object_type] = type_embedding\n",
    "        embeddings = [\n",
    "            self.cache[object_type] + self.learnable_params\n",
    "            for object_type in object_types\n",
    "        ]\n",
    "        return torch.stack(embeddings)\n",
    "\n",
    "class InventoryEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryEncoder, self).__init__()\n",
    "        hidden_size = model.config.hidden_size\n",
    "        self.type_embedding = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_embedding = nn.Embedding(max_quantity, hidden_size)\n",
    "        self.slot_embedding = nn.Embedding(max_slot, hidden_size)\n",
    "        self.combine = nn.Linear(\n",
    "            hidden_size * 3,\n",
    "            hidden_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        type_embeddings = self.type_embedding([item[\"type\"] for item in inventory])\n",
    "        quantities = torch.tensor(\n",
    "            [item[\"quantity\"] for item in inventory], dtype=torch.long\n",
    "        )\n",
    "        slots = torch.tensor([item[\"slot\"] for item in inventory], dtype=torch.long)\n",
    "\n",
    "        quantities = quantities.cuda()\n",
    "        slots = slots.cuda()\n",
    "\n",
    "        quantity_embeddings = self.quantity_embedding(quantities)\n",
    "        slot_embeddings = self.slot_embedding(slots)\n",
    "        x_concat = torch.cat(\n",
    "            [type_embeddings, quantity_embeddings, slot_embeddings], dim=-1\n",
    "        )\n",
    "        embed = self.combine(x_concat).mean(dim=0)\n",
    "        return embed\n",
    "\n",
    "\n",
    "encoder = InventoryEncoder(model, tokenizer)\n",
    "encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InventoryGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryGenerator, self).__init__()\n",
    "        hidden_size = model.config.hidden_size\n",
    "        self.max_quantity = max_quantity\n",
    "        self.max_slot = max_slot\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.type_decoder = TypeEmbedding(model, tokenizer)\n",
    "        self.quantity_decoder = nn.Linear(hidden_size, max_quantity)\n",
    "        self.slot_decoder = nn.Linear(hidden_size, max_slot)\n",
    "\n",
    "    def forward(self, inventory_embedding):\n",
    "        x = self.fc(inventory_embedding)\n",
    "\n",
    "        type_embeds, quantity_embeds, slot_embeds = torch.split(\n",
    "            x, self.hidden_size, dim=-1\n",
    "        )\n",
    "\n",
    "        # Decode type embeddings\n",
    "        decoded_types = self.type_decoder.decode(type_embeds)\n",
    "\n",
    "        # Decode quantity and slot embeddings\n",
    "        quantities = self.quantity_decoder(quantity_embeds)\n",
    "        slots = self.slot_decoder(slot_embeds)\n",
    "\n",
    "        # Convert logits to indices\n",
    "        quantities = torch.argmax(quantities, dim=-1)\n",
    "        slots = torch.argmax(slots, dim=-1)\n",
    "\n",
    "        # Create the decoded inventory list\n",
    "        decoded_inventory = []\n",
    "        for obj_type, quantity, slot in zip(decoded_types, quantities, slots):\n",
    "            decoded_inventory.append(\n",
    "                {\n",
    "                    \"type\": obj_type,\n",
    "                    \"quantity\": quantity.item(),\n",
    "                    \"slot\": slot.item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return decoded_inventory\n",
    "\n",
    "\n",
    "# Example of how to use the InventoryEmbedding and InventoryGenerator\n",
    "class InventoryAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=AutoModel,\n",
    "        tokenizer=AutoTokenizer,\n",
    "        max_quantity=64,\n",
    "        max_slot=46,\n",
    "    ):\n",
    "        super(InventoryAutoencoder, self).__init__()\n",
    "        self.encoder = InventoryEncoder(model, tokenizer, max_quantity, max_slot)\n",
    "        self.decoder = InventoryGenerator(model, tokenizer, max_quantity, max_slot)\n",
    "\n",
    "    def forward(self, inventory: list[dict]):\n",
    "        encoded = self.encoder(inventory)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Example usage\n",
    "inventory = [\n",
    "    {\"slot\": 13, \"type\": \"stick\", \"quantity\": 2},\n",
    "    {\"slot\": 20, \"type\": \"acacia_log\", \"quantity\": 1},\n",
    "    {\"slot\": 43, \"type\": \"dead_fire_coral\", \"quantity\": 55},\n",
    "    {\"slot\": 27, \"type\": \"acacia_leaves\", \"quantity\": 11},\n",
    "    {\"slot\": 28, \"type\": \"brown_mushroom\", \"quantity\": 23},\n",
    "    {\"slot\": 14, \"type\": \"llama_spawn_egg\", \"quantity\": 22},\n",
    "    {\"slot\": 45, \"type\": \"bat_spawn_egg\", \"quantity\": 6},\n",
    "    {\"slot\": 23, \"type\": \"oak_leaves\", \"quantity\": 8},\n",
    "    {\"slot\": 34, \"type\": \"diorite_slab\", \"quantity\": 38},\n",
    "    {\"slot\": 22, \"type\": \"dark_prismarine_slab\", \"quantity\": 54},\n",
    "]\n",
    "\n",
    "autoencoder = InventoryAutoencoder(model, tokenizer)\n",
    "autoencoder = autoencoder.to(\"cuda\")\n",
    "encoded_inventory = autoencoder.encoder(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.decoder(encoded_inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/train.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5024,  0.7074,  1.2459,  ...,  0.2150, -0.3527, -0.0361],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(data[0][\"slotted_inventory\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data/train.json\n",
    "\n",
    "# with open('data/train.json') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'SymbolicMoveAction',\n",
       "  'strict': True,\n",
       "  'parameters': {'description': '\"Moves an item from one slot to another',\n",
       "   'properties': {'slot_from': {'exclusiveMaximum': 46,\n",
       "     'minimum': 0,\n",
       "     'title': 'Slot From',\n",
       "     'type': 'integer'},\n",
       "    'slot_to': {'exclusiveMaximum': 46,\n",
       "     'minimum': 0,\n",
       "     'title': 'Slot To',\n",
       "     'type': 'integer'},\n",
       "    'quantity': {'default': 1,\n",
       "     'exclusiveMinimum': 0,\n",
       "     'maximum': 64,\n",
       "     'title': 'Quantity',\n",
       "     'type': 'integer'}},\n",
       "   'required': ['slot_from', 'slot_to', 'quantity'],\n",
       "   'title': 'SymbolicMoveAction',\n",
       "   'type': 'object',\n",
       "   'additionalProperties': False},\n",
       "  'description': ' \"Moves an item from one slot to another'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from plancraft.environments.actions import (\n",
    "    SymbolicMoveAction,\n",
    "    SymbolicSmeltAction,\n",
    ")\n",
    "\n",
    "openai.pydantic_function_tool(SymbolicMoveAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': '\\nYou are crafting in Minecraft. Actions are tools.\\n\\nThe first 10 slots in the inventory are reserved for crafting and correspond to the minecraft crafting table. \\n\\n[1, 2, 3] \\n[4, 5, 6] -> [0]\\n[7, 8, 9]\\n\\nThe crafting matrix is a 3x3 grid, and the output is sent to slot 0.\\nYou cannot move or smelt items into output slot 0.\\nThe remaining slots (10-45) are for storing items.\\n'}\n",
      "{'role': 'user', 'content': 'Craft an item of type: andesite\\ninventory=\\'[{\"type\": \"diorite\", \"slot\": 27, \"quantity\": 1},{\"type\": \"cobblestone\", \"slot\": 39, \"quantity\": 1}]\\''}\n",
      "{'role': 'assistant', 'content': 'SymbolicMoveAction(slot_from=27, slot_to=4, quantity=1)'}\n",
      "{'role': 'user', 'content': 'Craft an item of type: andesite\\ninventory=[{\"type\": \"diorite\", \"slot\": 4,  \"quantity\": 1},{\"type\": \"cobblestone\", \"slot\": 39, \"quantity\": 1}]'}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from plancraft.models.act_tools import OpenAIToolsGenerator, ACT_TOOLS_SYSTEM_PROMPT, ACT_TOOLS_EXAMPLE\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": ACT_TOOLS_SYSTEM_PROMPT}] + ACT_TOOLS_EXAMPLE[:3]\n",
    "\n",
    "model = OpenAIToolsGenerator()\n",
    "client = OpenAI()\n",
    "\n",
    "for m in messages:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SymbolicMoveAction(slot_from=4, slot_to=1, quantity=1)],\n",
       " ['SymbolicMoveAction(slot_from=4, slot_to=1, quantity=1)'],\n",
       " 465)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_next(batch_messages=[messages])\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=messages,\n",
    "#         temperature=1.0,\n",
    "#         max_tokens=256,\n",
    "#         top_p=1,\n",
    "#         frequency_penalty=0,\n",
    "#         presence_penalty=0,\n",
    "#         # stop=[\"\\n\"],\n",
    "#         tools=[MOVE, SMELT],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python main.py --config-name configs/text-evals/act_eval_gpt4o_mini_few_shot.yaml\n",
      "python main.py --config-name configs/text-evals/dummy.yaml\n",
      "python main.py --config-name configs/text-evals/react_eval_llama70b_few_shot.yaml\n",
      "python main.py --config-name configs/text-evals/act_eval_llama8b_lora.yaml\n",
      "python main.py --config-name configs/text-evals/act_eval_llama8b_zero_shot.yaml\n",
      "python main.py --config-name configs/text-evals/act_eval_llama8b_few_shot.yaml\n",
      "python main.py --config-name configs/text-evals/react_eval_gpt4o_mini_few_shot.yaml\n",
      "python main.py --config-name configs/text-evals/react_eval_llama8b_lora.yaml\n",
      "python main.py --config-name configs/text-evals/react_eval_llama8b_few_shot.yaml\n",
      "python main.py --config-name configs/text-evals/oracle.yaml\n",
      "python main.py --config-name configs/text-evals/react_eval_llama8b_zero_shot.yaml\n",
      "python main.py --config-name configs/text-evals/act_eval_llama70B_few_shot.yaml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for x in glob.glob(\"configs/text-evals/*\"):\n",
    "    print(f\"python main.py --config-name {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_9KPxJLWCnuKIAEPZO0ymE4GO', function=Function(arguments='{\"from_slot\": 4, \"to_slot\": 1, \"quantity\": 1}', name='move'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_cTzmGRkiGne1gTd6SZFgr4Ho', function=Function(arguments='{\"from_slot\": 39, \"to_slot\": 2, \"quantity\": 1}', name='move'), type='function')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "def format_function_call(func_obj):\n",
    "    # convert the OpenAI function object to a string\n",
    "    # Function(arguments='{\"from_slot\": 4, \"to_slot\": 1, \"quantity\": 1}', name='move')\n",
    "    # -> move(from_slot=4, to_slot=1, quantity=1)\n",
    "    args = json.loads(func_obj.arguments).items()\n",
    "    return f\"{func_obj.name}({', '.join([f'{k}={v}' for k, v in args])})\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# format_function_call(response.choices[0].message.function[0])\n",
    "# format_function_call(response.choices[0].message.tool_calls[0].function)\n",
    "response.choices[0].message.tool_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
