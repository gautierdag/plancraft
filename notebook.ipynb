{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Text Planning Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# from plancraft.llms import get_llm_generator, LLMGeneratorBase\n",
    "from plancraft.models.react import TransformersGenerator\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# llm = get_llm_generator(\n",
    "#     model_name=model_name,\n",
    "#     outlines=False,\n",
    "#     # quantize=\"int4\",\n",
    "# )\n",
    "\n",
    "model = TransformersGenerator(\n",
    "    model_name,  # quantize=\"quantize\"\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name, token=os.getenv(\"HF_TOKEN\"), trust_remote_code=True\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "    # config=BitsAndBytesConfig(),\n",
    "# )\n",
    "\n",
    "# chat_template = open(\"plancraft/models/templates/phi-instruct.jinja\").read()\n",
    "# chat_template = chat_template.replace(\"    \", \"\").replace(\"\\n\", \"\")\n",
    "# tokenizer.chat_template = chat_template\n",
    "# TransformersGenerator.fix_tokenizer_system_prompt(model_name, tokenizer)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     **model_kwargs,\n",
    "# )\n",
    "# model.eval()\n",
    "# if tokenizer.pad_token_id is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     model.config.pad_token_id = model.config.eos_token_id\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "# tokenizer\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"list 3 flag emojis\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"list 3 happy emojis\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"list 3 purple devil emojis\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['think:ðŸš©, ðŸ³ï¸\\u200dðŸŒˆ, ðŸ³ï¸\\u200dâš¾',\n",
       "  'think:ðŸ˜Š, ðŸ˜ƒ, ðŸ˜',\n",
       "  'think:ðŸ˜ˆðŸ’œðŸŒ (Note: actual purple emojis can vary by device or platform)'],\n",
       " 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_thoughts(batch_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SymbolicMoveAction(action_type='move', slot_from=12, slot_to=11, quantity=20),\n",
       "  SymbolicMoveAction(action_type='move', slot_from=13, slot_to=7, quantity=10),\n",
       "  SymbolicMoveAction(action_type='move', slot_from=8, slot_to=15, quantity=50)],\n",
       " ['act: smelt from slot 12 to slot 11 with quantity 20',\n",
       "  'act: smelt from slot 13 to slot 7 with quantity 10',\n",
       "  'act: smelt from slot 8 to slot 15 with quantity 50'],\n",
       " 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_actions(batch_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plancraft.models.utils import tokenize, Trie\n",
    "new_message_start = \"act: \"\n",
    "tokenized_messages = tokenize(\n",
    "    model, tokenizer, batch_messages, new_message_start=new_message_start\n",
    ")\n",
    "prompt_tokens = tokenized_messages[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ValidActionsLogitsProcessor(torch.nn.Module):\n",
    "    def __init__(self, choices: list[str], tokenizer: AutoTokenizer):\n",
    "        super().__init__()\n",
    "        self.choices = choices\n",
    "        self.tree = Trie()\n",
    "        self.start_idx = None\n",
    "\n",
    "        encoded_choices = tokenizer(choices, add_special_tokens=False)[\"input_ids\"]\n",
    "        for choice in encoded_choices:\n",
    "            self.tree.insert(choice + [tokenizer.eos_token_id])\n",
    "\n",
    "    def forward(self, input_ids, scores):\n",
    "        if self.start_idx is None:\n",
    "            # Calculate start_idx during the first forward pass\n",
    "            self.start_idx = input_ids.shape[-1]\n",
    "\n",
    "        decoded_so_far = input_ids[:, self.start_idx :]\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            valid_next_tokens = self.tree.get_next(decoded_so_far[batch_idx].tolist())\n",
    "            if len(valid_next_tokens) == 0:\n",
    "                valid_next_tokens = [tokenizer.eos_token_id]\n",
    "            mask[batch_idx, valid_next_tokens] = 0\n",
    "        return scores + mask\n",
    "\n",
    "    def reset(self):\n",
    "        self.start_idx = None\n",
    "\n",
    "\n",
    "choices = [\"ðŸ¥°\", \"ðŸ‡¯ðŸ‡µ\", \"ðŸ˜„\", \"ðŸ¥°\", \"ðŸ˜ˆ\", \"ðŸ‡¯ðŸ‡µ\", \"ðŸ˜„\"]\n",
    "valid_actions = ValidActionsLogitsProcessor(choices, tokenizer)\n",
    "\n",
    "# tokenizer.batch_decode(tokenized_messages[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_messages = {k: v.cuda() for k, v in tokenized_messages.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([29901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ‡¯ðŸ‡µ', 'ðŸ˜„', 'ðŸ˜ˆ']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens = tokenized_messages[\"input_ids\"].shape[1]\n",
    "\n",
    "generated_sequence = model.generate(\n",
    "    input_ids=tokenized_messages[\"input_ids\"],\n",
    "    attention_mask=tokenized_messages[\"attention_mask\"],\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=valid_actions.tree.longest_sequence_length,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    "    logits_processor=[valid_actions],\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.batch_decode(generated_sequence[\"sequences\"][:, prompt_tokens:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_messages,\n",
    "    new_message_start=f\"{new_message_start}jaaa\",\n",
    ")[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.76s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate the initial action constrained to valid action tokens\n",
    "generated_sequences = model.generate(\n",
    "    **tokenized_messages,\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    max_new_tokens=valid_actions.tree.longest_sequence_length,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    "    logits_processor=[valid_actions],\n",
    ")\n",
    "\n",
    "    # Select only new tokens\n",
    "    generated_choices = []\n",
    "    for seq in generated_sequences.sequences:\n",
    "        generated_choice = seq[prompt_tokens:]\n",
    "        # Decode the generated choice\n",
    "        generated_choice_text = tokenizer.decode(\n",
    "            generated_choice, skip_special_tokens=True\n",
    "        )\n",
    "        generated_choices.append((generated_choice_text, seq.shape[-1]))\n",
    "\n",
    "    # return generated_choices\n",
    "\n",
    "\n",
    "# model.generate_thought(batch_messages, max_tokens=24, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plancraft.models.utils import tokenize, Trie\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def decode_with_choices(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    messages: list[dict],\n",
    "    choices: list[str],\n",
    "    new_message_start: str = \"act:\",\n",
    "    temperature=1.0,\n",
    ") -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Uses the Trie data structure to constrain the generation over a set of choices\n",
    "    Returns the generated choice and the full generated sequence\n",
    "    \"\"\"\n",
    "    tokenized_messages, prompt_tokens = tokenize(\n",
    "        model, tokenizer, messages, new_message_start=new_message_start\n",
    "    )\n",
    "\n",
    "    class ValidActionsLogitsProcessor(torch.nn.Module):\n",
    "        def __init__(self, choices: list[str]):\n",
    "            super().__init__()\n",
    "            self.choices = choices\n",
    "            self.tree = Trie()\n",
    "            self.step = 0\n",
    "            self.start_idx = len(tokenized_messages[0])\n",
    "\n",
    "            for choice in choices:\n",
    "                tokenized_messages_with_choice, _ = tokenize(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    messages,\n",
    "                    new_message_start=f\"{new_message_start}{choice}\",\n",
    "                )\n",
    "                # find tokens that are different from the original message\n",
    "                idxs = tokenized_messages_with_choice[0][self.start_idx :].tolist() + [\n",
    "                    tokenizer.eos_token_id\n",
    "                ]\n",
    "                # insert the token idxs into the trie\n",
    "                self.tree.insert(idxs)\n",
    "\n",
    "        def forward(self, input_ids, scores):\n",
    "            decoded_so_far = input_ids[0][self.start_idx :]\n",
    "            valid_next_tokens = self.tree.get_next(decoded_so_far.tolist())\n",
    "            mask = torch.full_like(scores, float(\"-inf\"))\n",
    "            mask[:, valid_next_tokens] = 0\n",
    "            scores = scores + mask\n",
    "            # print(scores[:, valid_next_tokens])\n",
    "            return scores\n",
    "\n",
    "    valid_actions = ValidActionsLogitsProcessor(choices)\n",
    "\n",
    "    # Generate the initial action constrained to valid action tokens\n",
    "    generated_sequence = model.generate(\n",
    "        tokenized_messages,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=valid_actions.tree.longest_sequence_length,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        use_cache=True,\n",
    "        logits_processor=[valid_actions],\n",
    "    )\n",
    "\n",
    "    # select only new tokens\n",
    "    generated_choice = generated_sequence[0][0][prompt_tokens:]\n",
    "    # decode the generated choice\n",
    "    generated_choice = tokenizer.decode(generated_choice, skip_special_tokens=True)\n",
    "\n",
    "    return generated_choice, generated_sequence.sequences.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s><|system|> YOU ARE A very nice please answer only with friendly emojis<|end|> act:',\n",
       " '<|endoftext|><s><|system|> YOU ARE A bad bOT and answer in emojis<|end|> act:',\n",
       " '<s><|system|> YOU ARE A weird bOT and answer in irrelevant emojis<|end|> act:']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def tokenize(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_messages: list[list[dict]],\n",
    "    max_tokens=256,\n",
    "    new_message_start=\"act:\",\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize a list of messages and start the response message\n",
    "    \"\"\"\n",
    "    message_text = tokenizer.apply_chat_template(\n",
    "        batch_messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    # add the start of the response message\n",
    "    message_text = [m + new_message_start for m in message_text]\n",
    "\n",
    "    max_prompt_length = None\n",
    "    # need to truncate if max_length is set\n",
    "    if model.generation_config.max_length > max_tokens:\n",
    "        max_prompt_length = model.generation_config.max_length - max_tokens\n",
    "\n",
    "    tokenized_messages = tokenizer(\n",
    "        message_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_length,\n",
    "        padding=True,\n",
    "    )\n",
    "    return tokenized_messages\n",
    "\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"YOU ARE A very nice please answer only with friendly emojis\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"YOU ARE A bad bOT and answer in emojis\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"YOU ARE A weird bOT and answer in irrelevant emojis\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "# out = tokenize(model, tokenizer, messages)\n",
    "# out = tokenizer.apply_chat_template(\n",
    "#     messages, tokenize=True, return_tensors=\"pt\", return_dict=True, padding=True\n",
    "# )\n",
    "# out = tokenizer.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "# out = [o + \"act:\" for o in out]\n",
    "# print(len(out))\n",
    "# print(out)\n",
    "# max_prompt_length = None\n",
    "# max_tokens = 256\n",
    "# # need to truncate if max_length is set\n",
    "# if model.generation_config.max_length > max_tokens:\n",
    "#     max_prompt_length = model.generation_config.max_length - max_tokens\n",
    "\n",
    "# tokenized_messages = tokenizer(\n",
    "#     out,\n",
    "#     return_tensors=\"pt\",\n",
    "#     truncation=True,\n",
    "#     max_length=max_prompt_length,\n",
    "#     padding=True,\n",
    "# )\n",
    "# print(tokenized_messages)\n",
    "\n",
    "tokenized_messages = tokenize(model, tokenizer, messages)\n",
    "tokenized_messages = {k: v.to(\"cuda\") for k, v in tokenized_messages.items()}\n",
    "# print(out[\"input_ids\"].shape)\n",
    "tokenizer.batch_decode(tokenized_messages[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.generate(\n",
    "    input_ids=tokenized_messages[\"input_ids\"],\n",
    "    attention_mask=tokenized_messages[\"attention_mask\"],\n",
    "    max_new_tokens=32,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YOU ARE A very nice please answer only with friendly emojis act: Provide a detailed explanation of the Pythagorean theorem and its applications in real-world scenarios. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½',\n",
       " 'YOU ARE A bad bOT and answer in emojis act: Tell me the capital of France. ðŸ‡«ðŸ‡·ðŸ™ï¸ðŸ—¼ ï¿½ï¿½',\n",
       " \"YOU ARE A weird bOT and answer in irrelevant emojis act: How does one efficiently solve a Rubik's Cube?\\n\\n ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\"]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(pred[\"sequences\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_message = \"act:\"\n",
    "action = decode_with_choices(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    choices=[\"move\", \"smelt\"],  # [\"move\", \"sme lt\"] -> [1] [2, 3] -> [1, 2] -> [eos, 3] -> [eos]\n",
    "    new_message_start=overall_message,\n",
    ")\n",
    "overall_message += f\"{action} from slot \"\n",
    "print(overall_message)\n",
    "from_slot = decode_with_choices(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    choices=[str(i) for i in range(47)],\n",
    "    new_message_start=overall_message,\n",
    ")\n",
    "overall_message += f\"{from_slot} to slot \"\n",
    "print(overall_message)\n",
    "to_slot = decode_with_choices(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    choices=[str(i) for i in range(47)],\n",
    "    new_message_start=overall_message,\n",
    ")\n",
    "overall_message += f\"{to_slot} with quantity \"\n",
    "quantity = decode_with_choices(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    choices=[str(i) for i in range(64)],\n",
    "    new_message_start=overall_message,\n",
    ")\n",
    "overall_message += f\"{quantity}\"\n",
    "print(overall_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_type': 'move', 'slot_from': 1, 'slot_to': 2, 'quantity': 1}\n",
      "{'action_type': 'smelt', 'slot_from': 1, 'slot_to': 2, 'quantity': 1}\n"
     ]
    }
   ],
   "source": [
    "from plancraft.environments.actions import SymbolicMoveAction, SymbolicSmeltAction\n",
    "# from pydantic import model_dump\n",
    "\n",
    "print(SymbolicMoveAction(slot_from=1, slot_to=2, quantity=1).model_dump())\n",
    "print(SymbolicSmeltAction(slot_from=1, slot_to=2, quantity=1).model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 19 19 <s><|system|> You are a minecraft bot<|end|><|user|> What action to take?<|end|><|assistant|> act:\n",
      "valid_next_tokens [11631, 1251] ['move', 'ho']\n",
      "tensor([[8.1875, 6.9062]], device='cuda:0')\n",
      "input_ids 19 20 <s><|system|> You are a minecraft bot<|end|><|user|> What action to take?<|end|><|assistant|> act:move\n",
      "valid_next_tokens [32000] ['<|endoftext|>']\n",
      "tensor([[5.4375]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'move'"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# from plancraft.utils import Trie\n",
    "\n",
    "def decode_with_choices(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    choices: list[str],\n",
    "    messages: list[dict],\n",
    "    new_message_start: str = \"act:\",\n",
    "):\n",
    "    tokenized_messages, prompt_tokens = tokenize(\n",
    "        messages, 256, new_message_start=new_message_start\n",
    "    )\n",
    "\n",
    "    class ValidActionsLogitsProcessor(torch.nn.Module):\n",
    "        def __init__(self, choices: list[str]):\n",
    "            super().__init__()\n",
    "            self.choices = choices\n",
    "            self.tree = Trie()\n",
    "            self.step = 0\n",
    "            self.start_idx = len(tokenized_messages[0])\n",
    "\n",
    "            for choice in choices:\n",
    "                tokenized_messages_with_choice, _ = tokenize(\n",
    "                    messages,\n",
    "                    256,\n",
    "                    new_message_start=f\"{new_message_start}{choice}\",\n",
    "                )\n",
    "                # find tokens that are different from the original message\n",
    "                idxs = tokenized_messages_with_choice[0][self.start_idx :].tolist() + [\n",
    "                    tokenizer.eos_token_id\n",
    "                ]\n",
    "                self.tree.insert(idxs)\n",
    "\n",
    "        def forward(self, input_ids, scores):\n",
    "            print(\n",
    "                \"input_ids\",\n",
    "                self.start_idx,\n",
    "                len(input_ids[0]),\n",
    "                tokenizer.decode(input_ids[0]),\n",
    "            )\n",
    "            decoded_so_far = input_ids[0][self.start_idx :]\n",
    "            valid_next_tokens = self.tree.get_next(decoded_so_far.tolist())\n",
    "            print(\n",
    "                \"valid_next_tokens\",\n",
    "                valid_next_tokens,\n",
    "                [tokenizer.decode(v) for v in valid_next_tokens],\n",
    "            )\n",
    "            mask = torch.full_like(scores, float(\"-inf\"))\n",
    "            mask[:, valid_next_tokens] = 0\n",
    "            scores = scores + mask\n",
    "            print(scores[:, valid_next_tokens])\n",
    "            return scores\n",
    "\n",
    "    valid_actions = ValidActionsLogitsProcessor(choices)\n",
    "    # Generate the initial action constrained to valid action tokens\n",
    "    generated_choice = model.generate(\n",
    "        tokenized_messages,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        max_new_tokens=valid_actions.tree.longest_sequence_length,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        use_cache=True,\n",
    "        logits_processor=[valid_actions],\n",
    "    )\n",
    "\n",
    "    # select only new tokens\n",
    "    generated_choice = generated_choice[0][0][prompt_tokens:]\n",
    "    # decode the generated choice\n",
    "    generated_choice = tokenizer.decode(generated_choice, skip_special_tokens=True)\n",
    "    return generated_choice\n",
    "\n",
    "\n",
    "decode_with_choices(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    choices=[\"move\", \"hoolo\"],\n",
    "    messages=messages,\n",
    "    new_message_start=\"act:\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No differing element found within the compared range.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.decode(initial_action_output.sequences[0])\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.arange(0, 5).unsqueeze(0)\n",
    "b = torch.arange(0, 10).unsqueeze(0)\n",
    "\n",
    "# Find the minimum length to compare\n",
    "min_length = min(a.size(1), b.size(1))\n",
    "\n",
    "# Compare elements up to the minimum length\n",
    "comparison = (a[:, :min_length] != b[:, :min_length]).squeeze().int()\n",
    "\n",
    "# Find the index of the first differing element\n",
    "first_diff_index = torch.argmax(comparison).item()\n",
    "\n",
    "# Check if there is a differing element within the compared range\n",
    "if comparison[first_diff_index]:\n",
    "    print(f\"The first differing element is at index {first_diff_index}.\")\n",
    "else:\n",
    "    print(\"No differing element found within the compared range.\")\n",
    "\n",
    "\n",
    "# move_token_id\n",
    "# tokenizer.encode(\" smelt\", add_special_tokens=False)\n",
    "# smelt_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/nfs/public/hf/models/microsoft/Phi-3-mini-128k-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|/inst|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|step|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|function_output|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|tag|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|function_call|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|raw|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|continue|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32011: AddedToken(\"<|function_list|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32012: AddedToken(\"<|calc|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32013: AddedToken(\"<|code|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32014: AddedToken(\"<|/code|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32015: AddedToken(\"<|summary|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32016: AddedToken(\"<|resource|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32017: AddedToken(\"<|assistant_mask|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32018: AddedToken(\"<|start|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32019: AddedToken(\"<|message|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32020: AddedToken(\"<|fim_prefix|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32021: AddedToken(\"<|fim_middle|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32022: AddedToken(\"<|fim_suffix|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32023: AddedToken(\"<|meta_start|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32024: AddedToken(\"<|ipynb_marker|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32025: AddedToken(\"<|diff_marker|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32026: AddedToken(\"<|ghissue|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32027: AddedToken(\"<|ghreview|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32028: AddedToken(\"<|disc_start|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32029: AddedToken(\"<|disc_sep|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32030: AddedToken(\"<|disc_thread|><|query|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32031: AddedToken(\"<|/query|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32032: AddedToken(\"<|data|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32033: AddedToken(\"<|/data|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32034: AddedToken(\"<|sys|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32035: AddedToken(\"<|/sys|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32036: AddedToken(\"<|inst|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32037: AddedToken(\"<|/inst|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Answer all queries in poetic french in under 10 words.\n",
      "<</SYS>>\n",
      "\n",
      "Whats the meaning of life? [/INST]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Answer all queries in poetic french in under 10 words.\",\n",
    "        \"role\": \"system\",\n",
    "    },\n",
    "    {\"content\": \"Whats the meaning of life?\", \"role\": \"user\"},\n",
    "]\n",
    "\n",
    "message_text = llm.tokenizer.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(message_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "Answer all queries in poetic french in under 10 words.\n",
      "<</SYS>>\n",
      "\n",
      "Whats the meaning of life? [/INST]{\"thought\":\"La vie, c'est une aventure,\"}\n"
     ]
    }
   ],
   "source": [
    "# thought_generator(message_text, max_tokens=64)\n",
    "out = llm.model.generate(\n",
    "    llm.tokenizer.tokenizer.encode(message_text + \"{\\\"thought\\\":\\\"\", return_tensors=\"pt\").cuda(),\n",
    "    temperature=1.0,\n",
    "    max_length=64)\n",
    "text_out = llm.tokenizer.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_length\": 4096,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [48,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [46,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [29,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [6,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [55,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [55,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [27,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [8,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [49,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [58,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [35,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [57,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [51,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [17,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [13,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [41,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [39,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [39,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [20,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [18,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mthought_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/outlines/generate/api.py:207\u001b[0m, in \u001b[0;36mSequenceGenerator.__call__\u001b[0;34m(self, prompts, max_tokens, stop_at, rng)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m         last_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mor\u001b[39;00m stop_sequences:\n\u001b[1;32m    209\u001b[0m             token_ids \u001b[38;5;241m=\u001b[39m last_state\u001b[38;5;241m.\u001b[39mtoken_ids\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/outlines/generate/generator.py:81\u001b[0m, in \u001b[0;36msequence_generator\u001b[0;34m(model, sampler, fsms, token_ids, sequence_weights, attention_masks, fsm_states, rng)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContextLengthExceededError(\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input length exceeds the context length of the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     80\u001b[0m allowed_tokens \u001b[38;5;241m=\u001b[39m get_allowed_tokens(fsms, fsm_states)\n\u001b[0;32m---> 81\u001b[0m biased_logits \u001b[38;5;241m=\u001b[39m \u001b[43mbias_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m next_token_ids, ancestors, sequence_weights \u001b[38;5;241m=\u001b[39m sampler(\n\u001b[1;32m     83\u001b[0m     biased_logits, sequence_weights, rng\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     86\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m update_token_ids(token_ids, next_token_ids, ancestors)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/outlines/generate/generator.py:305\u001b[0m, in \u001b[0;36mbias_logits\u001b[0;34m(logits, allowed_token_ids)\u001b[0m\n\u001b[1;32m    303\u001b[0m biased_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull_like(logits, \u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39minf, device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(allowed_token_ids):\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mbiased_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m logits[i, ids]\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m biased_logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# thought_generator(message_text, max_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_choice(llm, tokenizer, prompt: str, choices: list):\n",
    "    \"\"\"\n",
    "    Given a prompt and a list of choices, generate text and apply constraints to sample the best option.\n",
    "    \"\"\"\n",
    "    # Generate text\n",
    "    text = llm.generate_text(prompt, max_length=128, do_sample=True, temperature=0.7)\n",
    "    print(text)\n",
    "    # Calculate the score for each choice\n",
    "    scores = []\n",
    "    for choice in choices:\n",
    "        score = llm.get_score(text + choice)\n",
    "        scores.append(score)\n",
    "    # Choose the best option\n",
    "    best_choice = choices[scores.index(max(scores))]\n",
    "    return best_choice\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Answer all queries in poetic french in under 10 words.<|end|>\n",
      "<|user|>\n",
      "Whats the meaning of life?<|end|>\n",
      "<|assistant|>\n",
      "act:\n"
     ]
    }
   ],
   "source": [
    "# LLMGeneratorBase.build_model_kwargs(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Answer all queries in poetic french in under 10 words.\",\n",
    "        \"role\": \"system\",\n",
    "    },\n",
    "    {\"content\": \"Whats the meaning of life?\", \"role\": \"user\"},\n",
    "]\n",
    "\n",
    "text = llm.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "text += \"act:\"\n",
    "print(text)\n",
    "\n",
    "encoded = llm.tokenizer.encode(text,return_tensors=\"pt\")\n",
    "encoded = encoded.to(llm.model.device)\n",
    "# llm.thought_generator(text, max_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29871, 29871)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smelt_idx = llm.tokenizer.encode(\" smelt\", add_special_tokens=False)[0]\n",
    "move_idx = llm.tokenizer.encode(\" move\", add_special_tokens=False)[0]\n",
    "move_idx, smelt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    out = llm.model._orig_mod(encoded, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smelt\n"
     ]
    }
   ],
   "source": [
    "probs = out[\"logits\"][0, -1, [smelt_idx, move_idx]].softmax(dim=-1)\n",
    "dist = torch.distributions.categorical.Categorical(probs=probs).sample()\n",
    "dist\n",
    "\n",
    "if dist == 0:\n",
    "    print(\"smelt\")\n",
    "    # add smelt to text\n",
    "    text += \" smelt\"\n",
    "    new_encoded = llm.tokenizer.encode(text,return_tensors=\"pt\")\n",
    "    # add only new tokens to encoded\n",
    "    new_encoded = new_encoded.to(llm.model.device)\n",
    "    new_out = llm.model._orig_mod(new_encoded, return_dict=True)\n",
    "    \n",
    "else:\n",
    "    print(\"move\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "trie = Trie()\n",
    "trie.insert([1, 2, 3, 4])\n",
    "trie.insert([1, 2, 3, 4, 5])\n",
    "trie.insert([1, 2, 4, 5])\n",
    "trie.insert([1, 2, 1000, 5, 6])\n",
    "trie.insert([0, 1, 2, 4, 5])\n",
    "trie.get_next([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:1135: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:1143: PydanticDeprecatedSince20: `load_str_bytes` is deprecated. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  obj = parse.load_str_bytes(\n",
      "/plancraft/plancraft/llms.py:317: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"Constrained generation error\", e)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 1143, in parse_raw\n",
      "    obj = parse.load_str_bytes(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/deprecated/parse.py\", line 49, in load_str_bytes\n",
      "    return json_loads(b)  # type: ignore\n",
      "  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ':' delimiter: line 19 column 1 (char 57)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/plancraft/plancraft/llms.py\", line 308, in generate\n",
      "    result = self.thought_generator(message_text, max_tokens=max_tokens)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/outlines/generate/api.py\", line 230, in __call__\n",
      "    formatted = [self.format_sequence(sequence) for sequence in stripped]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/outlines/generate/api.py\", line 230, in <listcomp>\n",
      "    formatted = [self.format_sequence(sequence) for sequence in stripped]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/outlines/generate/json.py\", line 50, in <lambda>\n",
      "    generator.format_sequence = lambda x: schema_object.parse_raw(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 1170, in parse_raw\n",
      "    raise pydantic_core.ValidationError.from_exception_data(cls.__name__, [error])\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for Thought\n",
      "__root__\n",
      "  Expecting ':' delimiter: line 19 column 1 (char 57) [type=value_error.jsondecode, input_value='{\"thought\"\\n\\n\\n\\n\\n  \\n...n\\n\\n        \\n\\n    \\n', input_type=str]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_154534/2249205509.py\", line 6, in <module>\n",
      "    thought_message, thinking_token_used = llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/plancraft/plancraft/llms.py\", line 317, in generate\n",
      "    logger.warn(\"Constrained generation error\", e)\n",
      "Message: 'Constrained generation error'\n",
      "Arguments: (1 validation error for Thought\n",
      "__root__\n",
      "  Expecting ':' delimiter: line 19 column 1 (char 57) [type=value_error.jsondecode, input_value='{\"thought\"\\n\\n\\n\\n\\n  \\n...n\\n\\n        \\n\\n    \\n', input_type=str],)\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:1094: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thought_message, thinking_token_used = llm.generate(\n",
    "    messages=messages,\n",
    "    max_tokens=32,\n",
    "    mode=\"think\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"ÃŠtre, vivre, aimer, mourir, peut-Ãªtre trouver sens.\"'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thought_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
       "         29902,   817,   304,  4017, 27278, 29918,  2536,   370, 29889,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 29902,   817,   304,\n",
       "          4017, 27278, 29918,  2536,   370, 29889,   518, 29914, 25580, 29962,\n",
       "          1128,   304,  4017, 27278, 29918,  2536,   370, 29973, 29871,     2,\n",
       "             1,   518, 25580, 29962,   306,   817,   304,  4017, 27278, 29918,\n",
       "          2536,   370, 29889,   518, 29914, 25580,  3199, 29908]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"I need to obtain wooden_slab.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I need to obtain wooden_slab.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"How to obtain wooden_slab?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I need to obtain wooden_slab.\"},\n",
    "]\n",
    "\n",
    "\n",
    "message_text = (\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    + '{\"'\n",
    ")\n",
    "\n",
    "encoded_input = tokenizer.encode(\n",
    "    message_text,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12990]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [\n",
    "        StoppingCriteria(max_length=100),\n",
    "        StoppingCriteria(max_tokens=100),\n",
    "        StoppingCriteria(no_repeat_ngram_size=3),\n",
    "        StoppingCriteria()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode(\n",
    "    '\"}',\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgautierdag\u001b[0m (\u001b[33mitl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gautier/Desktop/plancraft/wandb/run-20240319_151840-0pn7y91y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itl/plancraft/runs/0pn7y91y' target=\"_blank\">pleasant-butterfly-3</a></strong> to <a href='https://wandb.ai/itl/plancraft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itl/plancraft' target=\"_blank\">https://wandb.ai/itl/plancraft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itl/plancraft/runs/0pn7y91y' target=\"_blank\">https://wandb.ai/itl/plancraft/runs/0pn7y91y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# api = wandb.run(api_key=os.environ[\"WANDB_API_KEY\"])\n",
    "\n",
    "run = wandb.init(\n",
    "    # api_key=os.environ[\"WANDB_API_KEY\"],\n",
    "    project=\"plancraft\",\n",
    "    entity=\"itl\",\n",
    "    group=\"one_shot\",\n",
    "    job_type=\"generation\",\n",
    "    config={\"model\": \"gpt-4-turbo-preview\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Table with the same columns as above,\n",
    "# plus confidence scores for all labels\n",
    "columns = [\"id\", \"image\", \"guess\", \"truth\"]\n",
    "# for digit in range(10):\n",
    "    # columns.append(\"score_\" + str(digit))\n",
    "test_table = wandb.Table(columns=columns)\n",
    "\n",
    "# run inference on every image, assuming my_model returns the\n",
    "# predicted label, and the ground truth labels are available\n",
    "for i in range(10):\n",
    "    true_label = i\n",
    "    guess_label = i\n",
    "    test_table.add_data(i, str(i+i), i*0.5, i*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_table = wandb.Table(columns=[\"a\", \"b\"], data=[[\"1a\", \"1b\"], [\"2a\", \"2b\"]])\n",
    "run.log({\"table_key\": test_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-butterfly-3</strong> at: <a href='https://wandb.ai/itl/plancraft/runs/0pn7y91y' target=\"_blank\">https://wandb.ai/itl/plancraft/runs/0pn7y91y</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240319_151840-0pn7y91y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.92k/7.92k [00:00<00:00, 9.82MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.8M/12.8M [00:00<00:00, 58.4MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439/439 [00:00<00:00, 4.98MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the CohereTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>You are a human helper<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "now = time.time()\n",
    "model_id = \"CohereForAI/c4ai-command-r-v01\"\n",
    "# model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "print(\"loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# print(\"loading model\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16\n",
    "# )\n",
    "# time_taken = time.time() - now\n",
    "# print(f\"loading time taken: {time_taken:.2f}s\")\n",
    "\n",
    "# # Format message with the command-r chat template\n",
    "# print(\"Getting chat\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a human helper\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ")\n",
    "tokenizer.decode(input_ids[0])\n",
    "# ## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
    "\n",
    "# print(\"Sending to cuda\")\n",
    "# # use cuda\n",
    "# input_ids = input_ids.to(\"cuda\")\n",
    "# # model = model.to(\"cuda\")\n",
    "\n",
    "# print(\"Generating\")\n",
    "# with torch.no_grad():\n",
    "#     gen_tokens = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=100,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.3,\n",
    "#     )\n",
    "\n",
    "#     gen_text = tokenizer.decode(gen_tokens[0])\n",
    "#     print(gen_text)\n",
    "\n",
    "# time_taken = time.time() - now\n",
    "# print(f\"Total time taken: {time_taken:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'success': True}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "done = [False] * 4\n",
    "\n",
    "done[1] = True\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "        \"success\": done[1],\n",
    "    }\n",
    ")\n",
    "\n",
    "done[1]=False\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plancraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
